import mimetypes import os import re import rfc822 import StringIO import base64 import math import urllib import boto . utils from boto . exception import BotoClientError from boto . provider import Provider from boto . s3 . user import User from boto import UserAgent from boto . utils import compute_md5 try : from hashlib import md5 except ImportError : from md5 import md5 class Key ( object ) : DefaultContentType = 'application/octet-stream' BufferSize = 8192 def __init__ ( self , bucket = None , name = None ) : self . bucket = bucket self . name = name self . metadata = { } self . cache_control = None self . content_type = self . DefaultContentType self . content_encoding = None self . filename = None self . etag = None self . is_latest = False self . last_modified = None self . owner = None self . storage_class = 'STANDARD' self . md5 = None self . base64md5 = None self . path = None self . resp = None self . mode = None self . size = None self . version_id = None self . source_version_id = None self . delete_marker = False self . encrypted = None def __repr__ ( self ) : if self . bucket : return '<Key: %s,%s>' % ( self . bucket . name , self . name ) else : return '<Key: None,%s>' % self . name def __getattr__ ( self , name ) : if name == 'key' : return self . name else : raise AttributeError def __setattr__ ( self , name , value ) : if name == 'key' : self . __dict__ [ 'name' ] = value else : self . __dict__ [ name ] = value def __iter__ ( self ) : return self @ property def provider ( self ) : provider = None if self . bucket : if self . bucket . connection : provider = self . bucket . connection . provider return provider def get_md5_from_hexdigest ( self , md5_hexdigest ) : import binascii digest = binascii . unhexlify ( md5_hexdigest ) base64md5 = base64 . encodestring ( digest ) if base64md5 [ - 1 ] == '\n' : base64md5 = base64md5 [ 0 : - 1 ] return ( md5_hexdigest , base64md5 ) def handle_encryption_headers ( self , resp ) : provider = self . bucket . connection . provider if provider . server_side_encryption_header : self . encrypted = resp . getheader ( provider . server_side_encryption_header , None ) else : self . encrypted = None def handle_version_headers ( self , resp , force = False ) : provider = self . bucket . connection . provider if self . version_id is None or force : self . version_id = resp . getheader ( provider . version_id , None ) self . source_version_id = resp . getheader ( provider . copy_source_version_id , None ) if resp . getheader ( provider . delete_marker , 'false' ) == 'true' : self . delete_marker = True else : self . delete_marker = False def open_read ( self , headers = None , query_args = '' , override_num_retries = None , response_headers = None ) : if self . resp == None : self . mode = 'r' provider = self . bucket . connection . provider self . resp = self . bucket . connection . make_request ( , self . bucket . name , self . name , headers , query_args = query_args , override_num_retries = override_num_retries ) if self . resp . status < 199 or self . resp . status > 299 : body = self . resp . read ( ) raise provider . storage_response_error ( self . resp . status , self . resp . reason , body ) response_headers = self . resp . msg self . metadata = boto . utils . get_aws_metadata ( response_headers , provider ) for name , value in response_headers . items ( ) : if ( name . lower ( ) == 'content-length' and not in response_headers ) : self . size = int ( value ) elif name . lower ( ) == 'content-range' : end_range = re . sub ( '.*/(.*)' , '\\1' , value ) self . size = int ( end_range ) elif name . lower ( ) == 'etag' : self . etag = value elif name . lower ( ) == 'content-type' : self . content_type = value elif name . lower ( ) == 'content-encoding' : self . content_encoding = value elif name . lower ( ) == 'last-modified' : self . last_modified = value elif name . lower ( ) == 'cache-control' : self . cache_control = value self . handle_version_headers ( self . resp ) self . handle_encryption_headers ( self . resp ) def open_write ( self , headers = None , override_num_retries = None ) : raise BotoClientError ( 'Not Implemented' ) def open ( self , mode = 'r' , headers = None , query_args = None , override_num_retries = None ) : if mode == 'r' : self . mode = 'r' self . open_read ( headers = headers , query_args = query_args , override_num_retries = override_num_retries ) elif mode == 'w' : self . mode = 'w' self . open_write ( headers = headers , override_num_retries = override_num_retries ) else : raise BotoClientError ( 'Invalid mode: %s' % mode ) closed = False def close ( self ) : if self . resp : self . resp . read ( ) self . resp = None self . mode = None self . closed = True def next ( self ) : self . open_read ( ) data = self . resp . read ( self . BufferSize ) if not data : self . close ( ) raise StopIteration return data def read ( self , size = 0 ) : self . open_read ( ) if size == 0 : data = self . resp . read ( ) else : data = self . resp . read ( size ) if not data : self . close ( ) return data def change_storage_class ( self , new_storage_class , dst_bucket = None ) : if new_storage_class == 'STANDARD' : return self . copy ( self . bucket . name , self . name , reduced_redundancy = False , preserve_acl = True ) elif new_storage_class == 'REDUCED_REDUNDANCY' : return self . copy ( self . bucket . name , self . name , reduced_redundancy = True , preserve_acl = True ) else : raise BotoClientError ( 'Invalid storage class: %s' % new_storage_class ) def copy ( self , dst_bucket , dst_key , metadata = None , reduced_redundancy = False , preserve_acl = False , encrypt_key = False ) : dst_bucket = self . bucket . connection . lookup ( dst_bucket ) if reduced_redundancy : storage_class = 'REDUCED_REDUNDANCY' else : storage_class = self . storage_class return dst_bucket . copy_key ( dst_key , self . bucket . name , self . name , metadata , storage_class = storage_class , preserve_acl = preserve_acl , encrypt_key = encrypt_key ) def startElement ( self , name , attrs , connection ) : if name == 'Owner' : self . owner = User ( self ) return self . owner else : return None def endElement ( self , name , value , connection ) : if name == 'Key' : self . name = value elif name == 'ETag' : self . etag = value elif name == 'IsLatest' : if value == 'true' : self . is_latest = True else : self . is_latest = False elif name == 'LastModified' : self . last_modified = value elif name == 'Size' : self . size = int ( value ) elif name == 'StorageClass' : self . storage_class = value elif name == 'Owner' : pass elif name == 'VersionId' : self . version_id = value else : setattr ( self , name , value ) def exists ( self ) : return bool ( self . bucket . lookup ( self . name ) ) def delete ( self ) : return self . bucket . delete_key ( self . name , version_id = self . version_id ) def get_metadata ( self , name ) : return self . metadata . get ( name ) def set_metadata ( self , name , value ) : self . metadata [ name ] = value def update_metadata ( self , d ) : self . metadata . update ( d ) def set_acl ( self , acl_str , headers = None ) : if self . bucket != None : self . bucket . set_acl ( acl_str , self . name , headers = headers ) def get_acl ( self , headers = None ) : if self . bucket != None : return self . bucket . get_acl ( self . name , headers = headers ) def get_xml_acl ( self , headers = None ) : if self . bucket != None : return self . bucket . get_xml_acl ( self . name , headers = headers ) def set_xml_acl ( self , acl_str , headers = None ) : if self . bucket != None : return self . bucket . set_xml_acl ( acl_str , self . name , headers = headers ) def set_canned_acl ( self , acl_str , headers = None ) : return self . bucket . set_canned_acl ( acl_str , self . name , headers ) def make_public ( self , headers = None ) : return self . bucket . set_canned_acl ( 'public-read' , self . name , headers ) def generate_url ( self , expires_in , method = 'GET' , headers = None , query_auth = True , force_http = False , response_headers = None , expires_in_absolute = False ) : return self . bucket . connection . generate_url ( expires_in , method , self . bucket . name , self . name , headers , query_auth , force_http , response_headers , expires_in_absolute ) def send_file ( self , fp , headers = None , cb = None , num_cb = 10 , query_args = None , chunked_transfer = False , size = None ) : provider = self . bucket . connection . provider try : spos = fp . tell ( ) except IOError : spos = None self . read_from_stream = False def sender ( http_conn , method , path , data , headers ) : if spos is not None and spos != fp . tell ( ) : fp . seek ( spos ) elif spos is None and self . read_from_stream : raise provider . storage_data_error ( ) http_conn . putrequest ( method , path ) for key in headers : http_conn . putheader ( key , headers [ key ] ) http_conn . endheaders ( ) if chunked_transfer and not self . base64md5 : m = md5 ( ) else : m = None save_debug = self . bucket . connection . debug self . bucket . connection . debug = 0 if getattr ( http_conn , 'debuglevel' , 0 ) < 3 : http_conn . set_debuglevel ( 0 ) data_len = 0 if cb : if size : cb_size = size elif self . size : cb_size = self . size else : cb_size = 0 if chunked_transfer and cb_size == 0 : cb_count = ( 1024 * 1024 ) / self . BufferSize elif num_cb > 1 : cb_count = int ( math . ceil ( cb_size / self . BufferSize / ( num_cb - 1.0 ) ) ) elif num_cb < 0 : cb_count = - 1 else : cb_count = 0 i = 0 cb ( data_len , cb_size ) bytes_togo = size if bytes_togo and bytes_togo < self . BufferSize : chunk = fp . read ( bytes_togo ) else : chunk = fp . read ( self . BufferSize ) if spos is None : self . read_from_stream = True while chunk : chunk_len = len ( chunk ) data_len += chunk_len if chunked_transfer : http_conn . send ( '%x;\r\n' % chunk_len ) http_conn . send ( chunk ) http_conn . send ( '\r\n' ) else : http_conn . send ( chunk ) if m : m . update ( chunk ) if bytes_togo : bytes_togo -= chunk_len if bytes_togo <= 0 : break if cb : i += 1 if i == cb_count or cb_count == - 1 : cb ( data_len , cb_size ) i = 0 if bytes_togo and bytes_togo < self . BufferSize : chunk = fp . read ( bytes_togo ) else : chunk = fp . read ( self . BufferSize ) self . size = data_len if chunked_transfer : http_conn . send ( '0\r\n' ) if m : hd = m . hexdigest ( ) self . md5 , self . base64md5 = self . get_md5_from_hexdigest ( hd ) http_conn . send ( '\r\n' ) if cb and ( cb_count <= 1 or i > 0 ) and data_len > 0 : cb ( data_len , cb_size ) response = http_conn . getresponse ( ) body = response . read ( ) http_conn . set_debuglevel ( save_debug ) self . bucket . connection . debug = save_debug if ( ( response . status == 500 or response . status == 503 or response . getheader ( 'location' ) ) and not chunked_transfer ) : return response elif response . status >= 200 and response . status <= 299 : self . etag = response . getheader ( 'etag' ) if self . etag != '"%s"' % self . md5 : raise provider . storage_data_error ( ) return response else : raise provider . storage_response_error ( response . status , response . reason , body ) if not headers : headers = { } else : headers = headers . copy ( ) headers [ 'User-Agent' ] = UserAgent if self . storage_class != 'STANDARD' : headers [ provider . storage_class_header ] = self . storage_class if headers . has_key ( 'Content-Encoding' ) : self . content_encoding = headers [ 'Content-Encoding' ] if headers . has_key ( 'Content-Type' ) : if headers [ 'Content-Type' ] : self . content_type = headers [ 'Content-Type' ] else : del headers [ 'Content-Type' ] elif self . path : self . content_type = mimetypes . guess_type ( self . path ) [ 0 ] if self . content_type == None : self . content_type = self . DefaultContentType headers [ 'Content-Type' ] = self . content_type else : headers [ 'Content-Type' ] = self . content_type if self . base64md5 : headers [ 'Content-MD5' ] = self . base64md5 if chunked_transfer : headers [ 'Transfer-Encoding' ] = 'chunked' else : headers [ 'Content-Length' ] = str ( self . size ) headers [ 'Expect' ] = '100-Continue' headers = boto . utils . merge_meta ( headers , self . metadata , provider ) resp = self . bucket . connection . make_request ( 'PUT' , self . bucket . name , self . name , headers , sender = sender , query_args = query_args ) self . handle_version_headers ( resp , force = True ) def compute_md5 ( self , fp , size = None ) : tup = compute_md5 ( fp , size = size ) self . size = tup [ 2 ] return tup [ 0 : 2 ] def set_contents_from_stream ( self , fp , headers = None , replace = True , cb = None , num_cb = 10 , policy = None , reduced_redundancy = False , query_args = None , size = None ) : provider = self . bucket . connection . provider if not provider . supports_chunked_transfer ( ) : raise BotoClientError ( '%s does not support chunked transfer' % provider . get_provider_name ( ) ) if not self . name or self . name == '' : raise BotoClientError ( 'Cannot determine the destination ' ) if headers is None : headers = { } if policy : headers [ provider . acl_header ] = policy if reduced_redundancy : self . storage_class = 'REDUCED_REDUNDANCY' if provider . storage_class_header : headers [ provider . storage_class_header ] = self . storage_class if self . bucket != None : if not replace : if self . bucket . lookup ( self . name ) : return self . send_file ( fp , headers , cb , num_cb , query_args , chunked_transfer = True , size = size ) def set_contents_from_file ( self , fp , headers = None , replace = True , cb = None , num_cb = 10 , policy = None , md5 = None , reduced_redundancy = False , query_args = None , encrypt_key = False , size = None ) : provider = self . bucket . connection . provider headers = headers or { } if policy : headers [ provider . acl_header ] = policy if encrypt_key : headers [ provider . server_side_encryption_header ] = 'AES256' if reduced_redundancy : self . storage_class = 'REDUCED_REDUNDANCY' if provider . storage_class_header : headers [ provider . storage_class_header ] = self . storage_class if hasattr ( fp , 'name' ) : self . path = fp . name if self . bucket != None : if not md5 and provider . supports_chunked_transfer ( ) : chunked_transfer = True self . size = None else : chunked_transfer = False if not md5 : md5 = self . compute_md5 ( fp , size ) size = self . size elif size : self . size = size else : spos = fp . tell ( ) fp . seek ( 0 , os . SEEK_END ) self . size = fp . tell ( ) - spos fp . seek ( spos ) size = self . size self . md5 = md5 [ 0 ] self . base64md5 = md5 [ 1 ] if self . name == None : self . name = self . md5 if not replace : if self . bucket . lookup ( self . name ) : return self . send_file ( fp , headers = headers , cb = cb , num_cb = num_cb , query_args = query_args , chunked_transfer = chunked_transfer , size = size ) def set_contents_from_filename ( self , filename , headers = None , replace = True , cb = None , num_cb = 10 , policy = None , md5 = None , reduced_redundancy = False , encrypt_key = False ) : fp = open ( filename , 'rb' ) self . set_contents_from_file ( fp , headers , replace , cb , num_cb , policy , md5 , reduced_redundancy , encrypt_key = encrypt_key ) fp . close ( ) def set_contents_from_string ( self , s , headers = None , replace = True , cb = None , num_cb = 10 , policy = None , md5 = None , reduced_redundancy = False , encrypt_key = False ) : if isinstance ( s , unicode ) : s = s . encode ( "utf-8" ) fp = StringIO . StringIO ( s ) r = self . set_contents_from_file ( fp , headers , replace , cb , num_cb , policy , md5 , reduced_redundancy , encrypt_key = encrypt_key ) fp . close ( ) return r def get_file ( self , fp , headers = None , cb = None , num_cb = 10 , torrent = False , version_id = None , override_num_retries = None , response_headers = None ) : save_debug = self . bucket . connection . debug if self . bucket . connection . debug == 1 : self . bucket . connection . debug = 0 query_args = [ ] if torrent : query_args . append ( 'torrent' ) m = None else : m = md5 ( ) if version_id is None : version_id = self . version_id if version_id : query_args . append ( 'versionId=%s' % version_id ) if response_headers : for key in response_headers : query_args . append ( '%s=%s' % ( key , urllib . quote ( response_headers [ key ] ) ) ) query_args = '&' . join ( query_args ) self . open ( 'r' , headers , query_args = query_args , override_num_retries = override_num_retries ) data_len = 0 if cb : if self . size is None : cb_size = 0 else : cb_size = self . size if self . size is None and num_cb != - 1 : cb_count = ( 1024 * 1024 ) / self . BufferSize elif num_cb > 1 : cb_count = int ( math . ceil ( cb_size / self . BufferSize / ( num_cb - 1.0 ) ) ) elif num_cb < 0 : cb_count = - 1 else : cb_count = 0 i = 0 cb ( data_len , cb_size ) for bytes in self : fp . write ( bytes ) data_len += len ( bytes ) if m : m . update ( bytes ) if cb : if cb_size > 0 and data_len >= cb_size : break i += 1 if i == cb_count or cb_count == - 1 : cb ( data_len , cb_size ) i = 0 if cb and ( cb_count <= 1 or i > 0 ) and data_len > 0 : cb ( data_len , cb_size ) if m : self . md5 = m . hexdigest ( ) if self . size is None and not torrent and not headers . has_key ( "Range" ) : self . size = data_len self . close ( ) self . bucket . connection . debug = save_debug def get_torrent_file ( self , fp , headers = None , cb = None , num_cb = 10 ) : return self . get_file ( fp , headers , cb , num_cb , torrent = True ) def get_contents_to_file ( self , fp , headers = None , cb = None , num_cb = 10 , torrent = False , version_id = None , res_download_handler = None , response_headers = None ) : if self . bucket != None : if res_download_handler : res_download_handler . get_file ( self , fp , headers , cb , num_cb , torrent = torrent , version_id = version_id ) else : self . get_file ( fp , headers , cb , num_cb , torrent = torrent , version_id = version_id , response_headers = response_headers ) def get_contents_to_filename ( self , filename , headers = None , cb = None , num_cb = 10 , torrent = False , version_id = None , res_download_handler = None , response_headers = None ) : fp = open ( filename , 'wb' ) self . get_contents_to_file ( fp , headers , cb , num_cb , torrent = torrent , version_id = version_id , res_download_handler = res_download_handler , response_headers = response_headers ) fp . close ( ) if self . last_modified != None : try : modified_tuple = rfc822 . parsedate_tz ( self . last_modified ) modified_stamp = int ( rfc822 . mktime_tz ( modified_tuple ) ) os . utime ( fp . name , ( modified_stamp , modified_stamp ) ) except Exception : pass def get_contents_as_string ( self , headers = None , cb = None , num_cb = 10 , torrent = False , version_id = None , response_headers = None ) : fp = StringIO . StringIO ( ) self . get_contents_to_file ( fp , headers , cb , num_cb , torrent = torrent , version_id = version_id , response_headers = response_headers ) return fp . getvalue ( ) def add_email_grant ( self , permission , email_address , headers = None ) : policy = self . get_acl ( headers = headers ) policy . acl . add_email_grant ( permission , email_address ) self . set_acl ( policy , headers = headers ) def add_user_grant ( self , permission , user_id , headers = None , display_name = None ) : policy = self . get_acl ( ) policy . acl . add_user_grant ( permission , user_id , display_name = display_name ) self . set_acl ( policy , headers = headers )
