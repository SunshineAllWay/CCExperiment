"""A simple "pull API" for HTML parsing, after Perl's HTML::TokeParser. Examples This program extracts all links from a document. It will print one line for each link, containing the URL and the textual description between the <A>...</A> tags: import pullparser, sys f = file(sys.argv[1]) p = pullparser.PullParser(f) for token in p.tags("a"): if token.type == "endtag": continue url = dict(token.attrs).get("href", "-") text = p.get_compressed_text(endat=("endtag", "a")) print "%s\t%s" % (url, text) This program extracts the <TITLE> from the document: import pullparser, sys f = file(sys.argv[1]) p = pullparser.PullParser(f) if p.get_tag("title"): title = p.get_compressed_text() print "Title: %s" % title Copyright 2003-2006 John J. Lee <jjl@pobox.com> Copyright 1998-2001 Gisle Aas (original libwww-perl code) This code is free software; you can redistribute it and/or modify it under the terms of the BSD or ZPL 2.1 licenses. """ import re , htmlentitydefs import sgmllib , HTMLParser from xml . sax import saxutils from _html import unescape , unescape_charref class NoMoreTokensError ( Exception ) : pass class Token : def __init__ ( self , type , data , attrs = None ) : self . type = type self . data = data self . attrs = attrs def __iter__ ( self ) : return iter ( ( self . type , self . data , self . attrs ) ) def __eq__ ( self , other ) : type , data , attrs = other if ( self . type == type and self . data == data and self . attrs == attrs ) : return True else : return False def __ne__ ( self , other ) : return not self . __eq__ ( other ) def __repr__ ( self ) : args = ", " . join ( map ( repr , [ self . type , self . data , self . attrs ] ) ) return self . __class__ . __name__ + "(%s)" % args def __str__ ( self ) : if self . attrs is not None : attrs = "" . join ( [ " %s=%s" % ( k , saxutils . quoteattr ( v ) ) for k , v in self . attrs ] ) else : attrs = "" if self . type == "starttag" : return "<%s%s>" % ( self . data , attrs ) elif self . type == "startendtag" : return "<%s%s />" % ( self . data , attrs ) elif self . type == "endtag" : return "</%s>" % self . data elif self . type == "charref" : return "&#%s;" % self . data elif self . type == "entityref" : return "&%s;" % self . data elif self . type == "data" : return self . data elif self . type == "comment" : return "<!--%s-->" % self . data elif self . type == "decl" : return "<!%s>" % self . data elif self . type == "pi" : return "<?%s>" % self . data assert False def iter_until_exception ( fn , exception , * args , ** kwds ) : while 1 : try : yield fn ( * args , ** kwds ) except exception : raise StopIteration class _AbstractParser : chunk = 1024 compress_re = re . compile ( r"\s+" ) def __init__ ( self , fh , textify = { "img" : "alt" , "applet" : "alt" } , encoding = "ascii" , entitydefs = None ) : self . _fh = fh self . _tokenstack = [ ] self . textify = textify self . encoding = encoding if entitydefs is None : entitydefs = htmlentitydefs . name2codepoint self . _entitydefs = entitydefs def __iter__ ( self ) : return self def tags ( self , * names ) : return iter_until_exception ( self . get_tag , NoMoreTokensError , * names ) def tokens ( self , * tokentypes ) : return iter_until_exception ( self . get_token , NoMoreTokensError , * tokentypes ) def next ( self ) : try : return self . get_token ( ) except NoMoreTokensError : raise StopIteration ( ) def get_token ( self , * tokentypes ) : while 1 : while self . _tokenstack : token = self . _tokenstack . pop ( 0 ) if tokentypes : if token . type in tokentypes : return token else : return token data = self . _fh . read ( self . chunk ) if not data : raise NoMoreTokensError ( ) self . feed ( data ) def unget_token ( self , token ) : self . _tokenstack . insert ( 0 , token ) def get_tag ( self , * names ) : while 1 : tok = self . get_token ( ) if tok . type not in [ "starttag" , "endtag" , "startendtag" ] : continue if names : if tok . data in names : return tok else : return tok def get_text ( self , endat = None ) : text = [ ] tok = None while 1 : try : tok = self . get_token ( ) except NoMoreTokensError : if tok : self . unget_token ( tok ) break if tok . type == "data" : text . append ( tok . data ) elif tok . type == "entityref" : t = unescape ( "&%s;" % tok . data , self . _entitydefs , self . encoding ) text . append ( t ) elif tok . type == "charref" : t = unescape_charref ( tok . data , self . encoding ) text . append ( t ) elif tok . type in [ "starttag" , "endtag" , "startendtag" ] : tag_name = tok . data if tok . type in [ "starttag" , "startendtag" ] : alt = self . textify . get ( tag_name ) if alt is not None : if callable ( alt ) : text . append ( alt ( tok ) ) elif tok . attrs is not None : for k , v in tok . attrs : if k == alt : text . append ( v ) text . append ( "[%s]" % tag_name . upper ( ) ) if endat is None or endat == ( tok . type , tag_name ) : self . unget_token ( tok ) break return "" . join ( text ) def get_compressed_text ( self , * args , ** kwds ) : text = self . get_text ( * args , ** kwds ) text = text . strip ( ) return self . compress_re . sub ( " " , text ) def handle_startendtag ( self , tag , attrs ) : self . _tokenstack . append ( Token ( "startendtag" , tag , attrs ) ) def handle_starttag ( self , tag , attrs ) : self . _tokenstack . append ( Token ( "starttag" , tag , attrs ) ) def handle_endtag ( self , tag ) : self . _tokenstack . append ( Token ( "endtag" , tag ) ) def handle_charref ( self , name ) : self . _tokenstack . append ( Token ( "charref" , name ) ) def handle_entityref ( self , name ) : self . _tokenstack . append ( Token ( "entityref" , name ) ) def handle_data ( self , data ) : self . _tokenstack . append ( Token ( "data" , data ) ) def handle_comment ( self , data ) : self . _tokenstack . append ( Token ( "comment" , data ) ) def handle_decl ( self , decl ) : self . _tokenstack . append ( Token ( "decl" , decl ) ) def unknown_decl ( self , data ) : self . _tokenstack . append ( Token ( "decl" , data ) ) def handle_pi ( self , data ) : self . _tokenstack . append ( Token ( "pi" , data ) ) def unescape_attr ( self , name ) : return unescape ( name , self . _entitydefs , self . encoding ) def unescape_attrs ( self , attrs ) : escaped_attrs = [ ] for key , val in attrs : escaped_attrs . append ( ( key , self . unescape_attr ( val ) ) ) return escaped_attrs class PullParser ( _AbstractParser , HTMLParser . HTMLParser ) : def __init__ ( self , * args , ** kwds ) : HTMLParser . HTMLParser . __init__ ( self ) _AbstractParser . __init__ ( self , * args , ** kwds ) def unescape ( self , name ) : return self . unescape_attr ( name ) class TolerantPullParser ( _AbstractParser , sgmllib . SGMLParser ) : def __init__ ( self , * args , ** kwds ) : sgmllib . SGMLParser . __init__ ( self ) _AbstractParser . __init__ ( self , * args , ** kwds ) def unknown_starttag ( self , tag , attrs ) : attrs = self . unescape_attrs ( attrs ) self . _tokenstack . append ( Token ( "starttag" , tag , attrs ) ) def unknown_endtag ( self , tag ) : self . _tokenstack . append ( Token ( "endtag" , tag ) ) def _test ( ) : import doctest , _pullparser return doctest . testmod ( _pullparser ) if __name__ == "__main__" : _test ( )
