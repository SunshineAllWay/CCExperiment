from __future__ import with_statement from pylons import config import pytz , os , logging , sys , socket , re , subprocess , random import signal from datetime import timedelta , datetime from urlparse import urlparse import json from sqlalchemy import engine from sqlalchemy import event from r2 . lib . cache import LocalCache , SelfEmptyingCache from r2 . lib . cache import CMemcache , StaleCacheChain from r2 . lib . cache import HardCache , MemcacheChain , MemcacheChain , HardcacheChain from r2 . lib . cache import CassandraCache , CassandraCacheChain , CacheChain , CL_ONE , CL_QUORUM from r2 . lib . utils import thread_dump from r2 . lib . db . stats import QueryStats from r2 . lib . translation import get_active_langs from r2 . lib . lock import make_lock_factory from r2 . lib . manager import db_manager from r2 . lib . stats import Stats , CacheStats , StatsCollectingConnectionPool class Globals ( object ) : int_props = [ 'db_pool_size' , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ] float_props = [ 'min_promote_bid' , , , , , ] bool_props = [ 'debug' , 'translator' , , , , , , , , , , , , , , , , , , , , , , , , , ] tuple_props = [ 'stalecaches' , , , , , , , , , , , , , , ] choice_props = { 'cassandra_rcl' : { 'ONE' : CL_ONE , : CL_QUORUM } , : { 'ONE' : CL_ONE , : CL_QUORUM } , } def __init__ ( self , global_conf , app_conf , paths , ** extra ) : global_conf . setdefault ( "debug" , False ) for k , v in global_conf . iteritems ( ) : if not k . startswith ( "_" ) and not hasattr ( self , k ) : if k in self . int_props : v = int ( v ) elif k in self . float_props : v = float ( v ) elif k in self . bool_props : v = self . to_bool ( v ) elif k in self . tuple_props : v = tuple ( self . to_iter ( v ) ) elif k in self . choice_props : if v not in self . choice_props [ k ] : raise ValueError ( "Unknown option for %r: %r not in %r" % ( k , v , self . choice_props [ k ] ) ) v = self . choice_props [ k ] [ v ] setattr ( self , k , v ) self . paths = paths self . running_as_script = global_conf . get ( 'running_as_script' , False ) if not hasattr ( self , 'lang' ) : self . lang = 'en' self . languages , self . lang_name = get_active_langs ( default_lang = self . lang ) all_languages = self . lang_name . keys ( ) all_languages . sort ( ) self . all_languages = all_languages tz = global_conf . get ( 'timezone' , 'UTC' ) self . tz = pytz . timezone ( tz ) dtz = global_conf . get ( 'display_timezone' , tz ) self . display_tz = pytz . timezone ( dtz ) def setup ( self , global_conf ) : if self . heavy_load_mode : self . read_only_mode = True if hasattr ( signal , 'SIGUSR1' ) : signal . signal ( signal . SIGUSR1 , thread_dump ) localcache_cls = ( SelfEmptyingCache if self . running_as_script else LocalCache ) num_mc_clients = self . num_mc_clients self . cache_chains = { } self . memcache = CMemcache ( self . memcaches , num_clients = num_mc_clients ) self . make_lock = make_lock_factory ( self . memcache ) self . stats = Stats ( global_conf . get ( 'statsd_addr' ) , global_conf . get ( 'statsd_sample_rate' ) ) event . listens_for ( engine . Engine , 'before_cursor_execute' ) ( self . stats . pg_before_cursor_execute ) event . listens_for ( engine . Engine , 'after_cursor_execute' ) ( self . stats . pg_after_cursor_execute ) if not self . cassandra_seeds : raise ValueError ( "cassandra_seeds not set in the .ini" ) keyspace = "reddit" self . cassandra_pools = { : StatsCollectingConnectionPool ( keyspace , stats = self . stats , logging_name = "main" , server_list = self . cassandra_seeds , pool_size = self . cassandra_pool_size , timeout = 2 , max_retries = 3 , prefill = False ) , : StatsCollectingConnectionPool ( keyspace , stats = self . stats , logging_name = "noretries" , server_list = self . cassandra_seeds , pool_size = len ( self . cassandra_seeds ) , timeout = 2 , max_retries = 0 , prefill = False ) , } perma_memcache = ( CMemcache ( self . permacache_memcaches , num_clients = num_mc_clients ) if self . permacache_memcaches else None ) self . permacache = CassandraCacheChain ( localcache_cls ( ) , CassandraCache ( 'permacache' , self . cassandra_pools [ self . cassandra_default_pool ] , read_consistency_level = self . cassandra_rcl , write_consistency_level = self . cassandra_wcl ) , memcache = perma_memcache , lock_factory = self . make_lock ) self . cache_chains . update ( permacache = self . permacache ) if self . stalecaches : self . cache = StaleCacheChain ( localcache_cls ( ) , CMemcache ( self . stalecaches , num_clients = num_mc_clients ) , self . memcache ) else : self . cache = MemcacheChain ( ( localcache_cls ( ) , self . memcache ) ) self . cache_chains . update ( cache = self . cache ) self . rendercache = MemcacheChain ( ( localcache_cls ( ) , CMemcache ( self . rendercaches , noreply = True , no_block = True , num_clients = num_mc_clients ) ) ) self . cache_chains . update ( rendercache = self . rendercache ) self . thing_cache = CacheChain ( ( localcache_cls ( ) , ) ) self . cache_chains . update ( thing_cache = self . thing_cache ) self . dbm = self . load_db_params ( global_conf ) self . hardcache = HardcacheChain ( ( localcache_cls ( ) , self . memcache , HardCache ( self ) ) , cache_negative_results = True ) self . cache_chains . update ( hardcache = self . hardcache ) cache_chains = self . cache_chains . copy ( ) def reset_caches ( ) : for name , chain in cache_chains . iteritems ( ) : chain . reset ( ) chain . stats = CacheStats ( self . stats , name ) self . reset_caches = reset_caches self . reset_caches ( ) self . stats_collector = QueryStats ( ) self . MODWINDOW = timedelta ( self . MODWINDOW ) self . REDDIT_MAIN = bool ( os . environ . get ( 'REDDIT_MAIN' ) ) origin_prefix = self . domain_prefix + "." if self . domain_prefix else "" self . origin = "http://" + origin_prefix + self . domain self . secure_domains = set ( [ urlparse ( self . payment_domain ) . netloc ] ) self . trusted_domains = set ( [ self . domain ] ) self . trusted_domains . update ( self . authorized_cnames ) if self . https_endpoint : https_url = urlparse ( self . https_endpoint ) self . secure_domains . add ( https_url . netloc ) self . trusted_domains . add ( https_url . hostname ) static_files = os . path . join ( self . paths . get ( 'static_files' ) , 'static' ) names_file_path = os . path . join ( static_files , 'names.json' ) if os . path . exists ( names_file_path ) : with open ( names_file_path ) as handle : self . static_names = json . load ( handle ) else : self . static_names = { } self . log = logging . getLogger ( 'reddit' ) self . log . addHandler ( logging . StreamHandler ( ) ) if self . debug : self . log . setLevel ( logging . DEBUG ) else : self . log . setLevel ( logging . INFO ) logging . getLogger ( 'pycountry.db' ) . setLevel ( logging . CRITICAL ) if not self . media_domain : self . media_domain = self . domain if self . media_domain == self . domain : print ( "Warning: g.media_domain == g.domain. " + ) self . reddit_host = socket . gethostname ( ) self . reddit_pid = os . getpid ( ) for arg in sys . argv : tokens = arg . split ( "=" ) if len ( tokens ) == 2 : k , v = tokens self . log . debug ( "Overriding g.%s to %s" % ( k , v ) ) setattr ( self , k , v ) if self . write_query_queue and not self . amqp_host : raise Exception ( "amqp_host must be defined to use the query queue" ) if self . write_query_queue and not self . use_query_cache : raise Exception ( "write_query_queue requires use_query_cache" ) try : self . version = subprocess . check_output ( [ "git" , "rev-parse" , "HEAD" ] ) except subprocess . CalledProcessError , e : self . log . info ( "Couldn't read source revision (%r)" % e ) self . version = self . short_version = '(unknown)' else : self . short_version = self . version [ : 7 ] if self . log_start : self . log . error ( "reddit app %s:%s started %s at %s" % ( self . reddit_host , self . reddit_pid , self . short_version , datetime . now ( ) ) ) @ staticmethod def to_bool ( x ) : return ( x . lower ( ) == 'true' ) if x else None @ staticmethod def to_iter ( v , delim = ',' ) : return ( x . strip ( ) for x in v . split ( delim ) if x ) def load_db_params ( self , gc ) : self . databases = tuple ( self . to_iter ( gc [ 'databases' ] ) ) self . db_params = { } if not self . databases : return dbm = db_manager . db_manager ( ) db_param_names = ( 'name' , 'db_host' , 'db_user' , 'db_pass' , 'db_port' , , 'max_overflow' ) for db_name in self . databases : conf_params = self . to_iter ( gc [ db_name + '_db' ] ) params = dict ( zip ( db_param_names , conf_params ) ) if params [ 'db_user' ] == "*" : params [ 'db_user' ] = self . db_user if params [ 'db_pass' ] == "*" : params [ 'db_pass' ] = self . db_pass if params [ 'db_port' ] == "*" : params [ 'db_port' ] = self . db_port if params [ 'pool_size' ] == "*" : params [ 'pool_size' ] = self . db_pool_size if params [ 'max_overflow' ] == "*" : params [ 'max_overflow' ] = self . db_pool_overflow_size dbm . setup_db ( db_name , g_override = self , ** params ) self . db_params [ db_name ] = params dbm . type_db = dbm . get_engine ( gc [ 'type_db' ] ) dbm . relation_type_db = dbm . get_engine ( gc [ 'rel_type_db' ] ) def split_flags ( p ) : return ( [ n for n in p if not n . startswith ( "!" ) ] , dict ( ( n . strip ( '!' ) , True ) for n in p if n . startswith ( "!" ) ) ) prefix = 'db_table_' for k , v in gc . iteritems ( ) : if k . startswith ( prefix ) : params = list ( self . to_iter ( v ) ) name = k [ len ( prefix ) : ] kind = params [ 0 ] if kind == 'thing' : engines , flags = split_flags ( params [ 1 : ] ) dbm . add_thing ( name , dbm . get_engines ( engines ) , ** flags ) elif kind == 'relation' : engines , flags = split_flags ( params [ 3 : ] ) dbm . add_relation ( name , params [ 1 ] , params [ 2 ] , dbm . get_engines ( engines ) , ** flags ) return dbm def __del__ ( self ) : pass
