""" The main QuerySet implementation. This provides the public API for the ORM. """ import copy import itertools import sys from django . db import connections , router , transaction , IntegrityError from django . db . models . fields import AutoField from django . db . models . query_utils import ( Q , select_related_descend , deferred_class_factory , InvalidQuery ) from django . db . models . deletion import Collector from django . db . models import sql from django . utils . functional import partition CHUNK_SIZE = 100 ITER_CHUNK_SIZE = CHUNK_SIZE REPR_OUTPUT_SIZE = 20 EmptyResultSet = sql . EmptyResultSet class QuerySet ( object ) : def __init__ ( self , model = None , query = None , using = None ) : self . model = model self . _db = using self . query = query or sql . Query ( self . model ) self . _result_cache = None self . _iter = None self . _sticky_filter = False self . _for_write = False self . _prefetch_related_lookups = [ ] self . _prefetch_done = False def __deepcopy__ ( self , memo ) : obj = self . __class__ ( ) for k , v in self . __dict__ . items ( ) : if k in ( '_iter' , '_result_cache' ) : obj . __dict__ [ k ] = None else : obj . __dict__ [ k ] = copy . deepcopy ( v , memo ) return obj def __getstate__ ( self ) : len ( self ) obj_dict = self . __dict__ . copy ( ) obj_dict [ '_iter' ] = None return obj_dict def __repr__ ( self ) : data = list ( self [ : REPR_OUTPUT_SIZE + 1 ] ) if len ( data ) > REPR_OUTPUT_SIZE : data [ - 1 ] = "...(remaining elements truncated)..." return repr ( data ) def __len__ ( self ) : if self . _result_cache is None : if self . _iter : self . _result_cache = list ( self . _iter ) else : self . _result_cache = list ( self . iterator ( ) ) elif self . _iter : self . _result_cache . extend ( self . _iter ) if self . _prefetch_related_lookups and not self . _prefetch_done : self . _prefetch_related_objects ( ) return len ( self . _result_cache ) def __iter__ ( self ) : if self . _prefetch_related_lookups and not self . _prefetch_done : len ( self ) if self . _result_cache is None : self . _iter = self . iterator ( ) self . _result_cache = [ ] if self . _iter : return self . _result_iter ( ) return iter ( self . _result_cache ) def _result_iter ( self ) : pos = 0 while 1 : upper = len ( self . _result_cache ) while pos < upper : yield self . _result_cache [ pos ] pos = pos + 1 if not self . _iter : raise StopIteration if len ( self . _result_cache ) <= pos : self . _fill_cache ( ) def __nonzero__ ( self ) : if self . _prefetch_related_lookups and not self . _prefetch_done : len ( self ) if self . _result_cache is not None : return bool ( self . _result_cache ) try : iter ( self ) . next ( ) except StopIteration : return False return True def __contains__ ( self , val ) : pos = 0 if self . _result_cache is not None : if val in self . _result_cache : return True elif self . _iter is None : return False pos = len ( self . _result_cache ) else : it = iter ( self ) while True : if len ( self . _result_cache ) <= pos : self . _fill_cache ( num = 1 ) if self . _iter is None : return False if self . _result_cache [ pos ] == val : return True pos += 1 def __getitem__ ( self , k ) : if not isinstance ( k , ( slice , int , long ) ) : raise TypeError assert ( ( not isinstance ( k , slice ) and ( k >= 0 ) ) or ( isinstance ( k , slice ) and ( k . start is None or k . start >= 0 ) and ( k . stop is None or k . stop >= 0 ) ) ) , "Negative indexing is not supported." if self . _result_cache is not None : if self . _iter is not None : if isinstance ( k , slice ) : if k . stop is not None : bound = int ( k . stop ) else : bound = None else : bound = k + 1 if len ( self . _result_cache ) < bound : self . _fill_cache ( bound - len ( self . _result_cache ) ) return self . _result_cache [ k ] if isinstance ( k , slice ) : qs = self . _clone ( ) if k . start is not None : start = int ( k . start ) else : start = None if k . stop is not None : stop = int ( k . stop ) else : stop = None qs . query . set_limits ( start , stop ) return k . step and list ( qs ) [ : : k . step ] or qs try : qs = self . _clone ( ) qs . query . set_limits ( k , k + 1 ) return list ( qs ) [ 0 ] except self . model . DoesNotExist , e : raise IndexError ( e . args ) def __and__ ( self , other ) : self . _merge_sanity_check ( other ) if isinstance ( other , EmptyQuerySet ) : return other . _clone ( ) combined = self . _clone ( ) combined . query . combine ( other . query , sql . AND ) return combined def __or__ ( self , other ) : self . _merge_sanity_check ( other ) combined = self . _clone ( ) if isinstance ( other , EmptyQuerySet ) : return combined combined . query . combine ( other . query , sql . OR ) return combined def iterator ( self ) : fill_cache = False if connections [ self . db ] . features . supports_select_related : fill_cache = self . query . select_related if isinstance ( fill_cache , dict ) : requested = fill_cache else : requested = None max_depth = self . query . max_depth extra_select = self . query . extra_select . keys ( ) aggregate_select = self . query . aggregate_select . keys ( ) only_load = self . query . get_loaded_field_names ( ) if not fill_cache : fields = self . model . _meta . fields load_fields = [ ] if only_load : for field , model in self . model . _meta . get_fields_with_model ( ) : if model is None : model = self . model try : if field . name in only_load [ model ] : load_fields . append ( field . name ) except KeyError : load_fields . append ( field . name ) index_start = len ( extra_select ) aggregate_start = index_start + len ( load_fields or self . model . _meta . fields ) skip = None if load_fields and not fill_cache : skip = set ( ) init_list = [ ] for field in fields : if field . name not in load_fields : skip . add ( field . attname ) else : init_list . append ( field . attname ) model_cls = deferred_class_factory ( self . model , skip ) db = self . db model = self . model compiler = self . query . get_compiler ( using = db ) if fill_cache : klass_info = get_klass_info ( model , max_depth = max_depth , requested = requested , only_load = only_load ) for row in compiler . results_iter ( ) : if fill_cache : obj , _ = get_cached_row ( row , index_start , db , klass_info , offset = len ( aggregate_select ) ) else : if skip : row_data = row [ index_start : aggregate_start ] obj = model_cls ( ** dict ( zip ( init_list , row_data ) ) ) else : obj = model ( * row [ index_start : aggregate_start ] ) obj . _state . db = db obj . _state . adding = False if extra_select : for i , k in enumerate ( extra_select ) : setattr ( obj , k , row [ i ] ) if aggregate_select : for i , aggregate in enumerate ( aggregate_select ) : setattr ( obj , aggregate , row [ i + aggregate_start ] ) yield obj def aggregate ( self , * args , ** kwargs ) : if self . query . distinct_fields : raise NotImplementedError ( "aggregate() + distinct(fields) not implemented." ) for arg in args : kwargs [ arg . default_alias ] = arg query = self . query . clone ( ) for ( alias , aggregate_expr ) in kwargs . items ( ) : query . add_aggregate ( aggregate_expr , self . model , alias , is_summary = True ) return query . get_aggregation ( using = self . db ) def count ( self ) : if self . _result_cache is not None and not self . _iter : return len ( self . _result_cache ) return self . query . get_count ( using = self . db ) def get ( self , * args , ** kwargs ) : clone = self . filter ( * args , ** kwargs ) if self . query . can_filter ( ) : clone = clone . order_by ( ) num = len ( clone ) if num == 1 : return clone . _result_cache [ 0 ] if not num : raise self . model . DoesNotExist ( "%s matching query does not exist." % self . model . _meta . object_name ) raise self . model . MultipleObjectsReturned ( "get() returned more than one %s -- it returned %s! Lookup parameters were %s" % ( self . model . _meta . object_name , num , kwargs ) ) def create ( self , ** kwargs ) : obj = self . model ( ** kwargs ) self . _for_write = True obj . save ( force_insert = True , using = self . db ) return obj def bulk_create ( self , objs ) : if self . model . _meta . parents : raise ValueError ( "Can't bulk create an inherited model" ) if not objs : return objs self . _for_write = True connection = connections [ self . db ] fields = self . model . _meta . local_fields if not transaction . is_managed ( using = self . db ) : transaction . enter_transaction_management ( using = self . db ) forced_managed = True else : forced_managed = False try : if ( connection . features . can_combine_inserts_with_and_without_auto_increment_pk and self . model . _meta . has_auto_field ) : self . model . _base_manager . _insert ( objs , fields = fields , using = self . db ) else : objs_with_pk , objs_without_pk = partition ( lambda o : o . pk is None , objs ) if objs_with_pk : self . model . _base_manager . _insert ( objs_with_pk , fields = fields , using = self . db ) if objs_without_pk : self . model . _base_manager . _insert ( objs_without_pk , fields = [ f for f in fields if not isinstance ( f , AutoField ) ] , using = self . db ) if forced_managed : transaction . commit ( using = self . db ) else : transaction . commit_unless_managed ( using = self . db ) finally : if forced_managed : transaction . leave_transaction_management ( using = self . db ) return objs def get_or_create ( self , ** kwargs ) : assert kwargs , 'get_or_create() must be passed at least one keyword argument' defaults = kwargs . pop ( 'defaults' , { } ) lookup = kwargs . copy ( ) for f in self . model . _meta . fields : if f . attname in lookup : lookup [ f . name ] = lookup . pop ( f . attname ) try : self . _for_write = True return self . get ( ** lookup ) , False except self . model . DoesNotExist : try : params = dict ( [ ( k , v ) for k , v in kwargs . items ( ) if '__' not in k ] ) params . update ( defaults ) obj = self . model ( ** params ) sid = transaction . savepoint ( using = self . db ) obj . save ( force_insert = True , using = self . db ) transaction . savepoint_commit ( sid , using = self . db ) return obj , True except IntegrityError , e : transaction . savepoint_rollback ( sid , using = self . db ) exc_info = sys . exc_info ( ) try : return self . get ( ** lookup ) , False except self . model . DoesNotExist : raise exc_info [ 1 ] , None , exc_info [ 2 ] def latest ( self , field_name = None ) : latest_by = field_name or self . model . _meta . get_latest_by assert bool ( latest_by ) , "latest() requires either a field_name parameter or 'get_latest_by' in the model" assert self . query . can_filter ( ) , "Cannot change a query once a slice has been taken." obj = self . _clone ( ) obj . query . set_limits ( high = 1 ) obj . query . clear_ordering ( ) obj . query . add_ordering ( '-%s' % latest_by ) return obj . get ( ) def in_bulk ( self , id_list ) : assert self . query . can_filter ( ) , "Cannot use 'limit' or 'offset' with in_bulk" if not id_list : return { } qs = self . _clone ( ) qs . query . add_filter ( ( 'pk__in' , id_list ) ) qs . query . clear_ordering ( force_empty = True ) return dict ( [ ( obj . _get_pk_val ( ) , obj ) for obj in qs . iterator ( ) ] ) def delete ( self ) : assert self . query . can_filter ( ) , "Cannot use 'limit' or 'offset' with delete." del_query = self . _clone ( ) del_query . _for_write = True del_query . query . select_for_update = False del_query . query . select_related = False del_query . query . clear_ordering ( ) collector = Collector ( using = del_query . db ) collector . collect ( del_query ) collector . delete ( ) self . _result_cache = None delete . alters_data = True def update ( self , ** kwargs ) : assert self . query . can_filter ( ) , "Cannot update a query once a slice has been taken." self . _for_write = True query = self . query . clone ( sql . UpdateQuery ) query . add_update_values ( kwargs ) if not transaction . is_managed ( using = self . db ) : transaction . enter_transaction_management ( using = self . db ) forced_managed = True else : forced_managed = False try : rows = query . get_compiler ( self . db ) . execute_sql ( None ) if forced_managed : transaction . commit ( using = self . db ) else : transaction . commit_unless_managed ( using = self . db ) finally : if forced_managed : transaction . leave_transaction_management ( using = self . db ) self . _result_cache = None return rows update . alters_data = True def _update ( self , values ) : assert self . query . can_filter ( ) , "Cannot update a query once a slice has been taken." query = self . query . clone ( sql . UpdateQuery ) query . add_update_fields ( values ) self . _result_cache = None return query . get_compiler ( self . db ) . execute_sql ( None ) _update . alters_data = True def exists ( self ) : if self . _result_cache is None : return self . query . has_results ( using = self . db ) return bool ( self . _result_cache ) def _prefetch_related_objects ( self ) : prefetch_related_objects ( self . _result_cache , self . _prefetch_related_lookups ) self . _prefetch_done = True def values ( self , * fields ) : return self . _clone ( klass = ValuesQuerySet , setup = True , _fields = fields ) def values_list ( self , * fields , ** kwargs ) : flat = kwargs . pop ( 'flat' , False ) if kwargs : raise TypeError ( 'Unexpected keyword arguments to values_list: %s' % ( kwargs . keys ( ) , ) ) if flat and len ( fields ) > 1 : raise TypeError ( "'flat' is not valid when values_list is called with more than one field." ) return self . _clone ( klass = ValuesListQuerySet , setup = True , flat = flat , _fields = fields ) def dates ( self , field_name , kind , order = 'ASC' ) : assert kind in ( "month" , "year" , "day" ) , "'kind' must be one of 'year', 'month' or 'day'." assert order in ( 'ASC' , 'DESC' ) , "'order' must be either 'ASC' or 'DESC'." return self . _clone ( klass = DateQuerySet , setup = True , _field_name = field_name , _kind = kind , _order = order ) def none ( self ) : return self . _clone ( klass = EmptyQuerySet ) def all ( self ) : return self . _clone ( ) def filter ( self , * args , ** kwargs ) : return self . _filter_or_exclude ( False , * args , ** kwargs ) def exclude ( self , * args , ** kwargs ) : return self . _filter_or_exclude ( True , * args , ** kwargs ) def _filter_or_exclude ( self , negate , * args , ** kwargs ) : if args or kwargs : assert self . query . can_filter ( ) , "Cannot filter a query once a slice has been taken." clone = self . _clone ( ) if negate : clone . query . add_q ( ~ Q ( * args , ** kwargs ) ) else : clone . query . add_q ( Q ( * args , ** kwargs ) ) return clone def complex_filter ( self , filter_obj ) : if isinstance ( filter_obj , Q ) or hasattr ( filter_obj , 'add_to_query' ) : clone = self . _clone ( ) clone . query . add_q ( filter_obj ) return clone else : return self . _filter_or_exclude ( None , ** filter_obj ) def select_for_update ( self , ** kwargs ) : nowait = kwargs . pop ( 'nowait' , False ) obj = self . _clone ( ) obj . query . select_for_update = True obj . query . select_for_update_nowait = nowait return obj def select_related ( self , * fields , ** kwargs ) : depth = kwargs . pop ( 'depth' , 0 ) if kwargs : raise TypeError ( 'Unexpected keyword arguments to select_related: %s' % ( kwargs . keys ( ) , ) ) obj = self . _clone ( ) if fields : if depth : raise TypeError ( 'Cannot pass both "depth" and fields to select_related()' ) obj . query . add_select_related ( fields ) else : obj . query . select_related = True if depth : obj . query . max_depth = depth return obj def prefetch_related ( self , * lookups ) : clone = self . _clone ( ) if lookups == ( None , ) : clone . _prefetch_related_lookups = [ ] else : clone . _prefetch_related_lookups . extend ( lookups ) return clone def dup_select_related ( self , other ) : self . query . select_related = other . query . select_related def annotate ( self , * args , ** kwargs ) : for arg in args : if arg . default_alias in kwargs : raise ValueError ( "The named annotation '%s' conflicts with the " % arg . default_alias ) kwargs [ arg . default_alias ] = arg names = getattr ( self , '_fields' , None ) if names is None : names = set ( self . model . _meta . get_all_field_names ( ) ) for aggregate in kwargs : if aggregate in names : raise ValueError ( "The annotation '%s' conflicts with a field on " % aggregate ) obj = self . _clone ( ) obj . _setup_aggregate_query ( kwargs . keys ( ) ) for ( alias , aggregate_expr ) in kwargs . items ( ) : obj . query . add_aggregate ( aggregate_expr , self . model , alias , is_summary = False ) return obj def order_by ( self , * field_names ) : assert self . query . can_filter ( ) , "Cannot reorder a query once a slice has been taken." obj = self . _clone ( ) obj . query . clear_ordering ( ) obj . query . add_ordering ( * field_names ) return obj def distinct ( self , * field_names ) : assert self . query . can_filter ( ) , "Cannot create distinct fields once a slice has been taken." obj = self . _clone ( ) obj . query . add_distinct_fields ( * field_names ) return obj def extra ( self , select = None , where = None , params = None , tables = None , order_by = None , select_params = None ) : assert self . query . can_filter ( ) , "Cannot change a query once a slice has been taken" clone = self . _clone ( ) clone . query . add_extra ( select , select_params , where , params , tables , order_by ) return clone def reverse ( self ) : clone = self . _clone ( ) clone . query . standard_ordering = not clone . query . standard_ordering return clone def defer ( self , * fields ) : clone = self . _clone ( ) if fields == ( None , ) : clone . query . clear_deferred_loading ( ) else : clone . query . add_deferred_loading ( fields ) return clone def only ( self , * fields ) : if fields == ( None , ) : raise TypeError ( "Cannot pass None as an argument to only()." ) clone = self . _clone ( ) clone . query . add_immediate_loading ( fields ) return clone def using ( self , alias ) : clone = self . _clone ( ) clone . _db = alias return clone def ordered ( self ) : if self . query . extra_order_by or self . query . order_by : return True elif self . query . default_ordering and self . query . model . _meta . ordering : return True else : return False ordered = property ( ordered ) @ property def db ( self ) : if self . _for_write : return self . _db or router . db_for_write ( self . model ) return self . _db or router . db_for_read ( self . model ) def _clone ( self , klass = None , setup = False , ** kwargs ) : if klass is None : klass = self . __class__ query = self . query . clone ( ) if self . _sticky_filter : query . filter_is_sticky = True c = klass ( model = self . model , query = query , using = self . _db ) c . _for_write = self . _for_write c . _prefetch_related_lookups = self . _prefetch_related_lookups [ : ] c . __dict__ . update ( kwargs ) if setup and hasattr ( c , '_setup_query' ) : c . _setup_query ( ) return c def _fill_cache ( self , num = None ) : if self . _iter : try : for i in range ( num or ITER_CHUNK_SIZE ) : self . _result_cache . append ( self . _iter . next ( ) ) except StopIteration : self . _iter = None def _next_is_sticky ( self ) : self . _sticky_filter = True return self def _merge_sanity_check ( self , other ) : pass def _setup_aggregate_query ( self , aggregates ) : opts = self . model . _meta if self . query . group_by is None : field_names = [ f . attname for f in opts . fields ] self . query . add_fields ( field_names , False ) self . query . set_group_by ( ) def _prepare ( self ) : return self def _as_sql ( self , connection ) : obj = self . values ( "pk" ) if obj . _db is None or connection == connections [ obj . _db ] : return obj . query . get_compiler ( connection = connection ) . as_nested_sql ( ) raise ValueError ( "Can't do subqueries with queries on different DBs." ) value_annotation = True class ValuesQuerySet ( QuerySet ) : def __init__ ( self , * args , ** kwargs ) : super ( ValuesQuerySet , self ) . __init__ ( * args , ** kwargs ) self . query . select_related = False def iterator ( self ) : extra_names = self . query . extra_select . keys ( ) field_names = self . field_names aggregate_names = self . query . aggregate_select . keys ( ) names = extra_names + field_names + aggregate_names for row in self . query . get_compiler ( self . db ) . results_iter ( ) : yield dict ( zip ( names , row ) ) def _setup_query ( self ) : self . query . clear_deferred_loading ( ) self . query . clear_select_fields ( ) if self . _fields : self . extra_names = [ ] self . aggregate_names = [ ] if not self . query . extra and not self . query . aggregates : self . field_names = list ( self . _fields ) else : self . query . default_cols = False self . field_names = [ ] for f in self . _fields : if f in self . query . extra : self . extra_names . append ( f ) elif f in self . query . aggregate_select : self . aggregate_names . append ( f ) else : self . field_names . append ( f ) else : self . extra_names = None self . field_names = [ f . attname for f in self . model . _meta . fields ] self . aggregate_names = None self . query . select = [ ] if self . extra_names is not None : self . query . set_extra_mask ( self . extra_names ) self . query . add_fields ( self . field_names , True ) if self . aggregate_names is not None : self . query . set_aggregate_mask ( self . aggregate_names ) def _clone ( self , klass = None , setup = False , ** kwargs ) : c = super ( ValuesQuerySet , self ) . _clone ( klass , ** kwargs ) if not hasattr ( c , '_fields' ) : c . _fields = self . _fields [ : ] c . field_names = self . field_names c . extra_names = self . extra_names c . aggregate_names = self . aggregate_names if setup and hasattr ( c , '_setup_query' ) : c . _setup_query ( ) return c def _merge_sanity_check ( self , other ) : super ( ValuesQuerySet , self ) . _merge_sanity_check ( other ) if ( set ( self . extra_names ) != set ( other . extra_names ) or set ( self . field_names ) != set ( other . field_names ) or self . aggregate_names != other . aggregate_names ) : raise TypeError ( "Merging '%s' classes must involve the same values in each case." % self . __class__ . __name__ ) def _setup_aggregate_query ( self , aggregates ) : self . query . set_group_by ( ) if self . aggregate_names is not None : self . aggregate_names . extend ( aggregates ) self . query . set_aggregate_mask ( self . aggregate_names ) super ( ValuesQuerySet , self ) . _setup_aggregate_query ( aggregates ) def _as_sql ( self , connection ) : if ( ( self . _fields and len ( self . _fields ) > 1 ) or ( not self . _fields and len ( self . model . _meta . fields ) > 1 ) ) : raise TypeError ( 'Cannot use a multi-field %s as a filter value.' % self . __class__ . __name__ ) obj = self . _clone ( ) if obj . _db is None or connection == connections [ obj . _db ] : return obj . query . get_compiler ( connection = connection ) . as_nested_sql ( ) raise ValueError ( "Can't do subqueries with queries on different DBs." ) def _prepare ( self ) : if ( ( self . _fields and len ( self . _fields ) > 1 ) or ( not self . _fields and len ( self . model . _meta . fields ) > 1 ) ) : raise TypeError ( 'Cannot use a multi-field %s as a filter value.' % self . __class__ . __name__ ) return self class ValuesListQuerySet ( ValuesQuerySet ) : def iterator ( self ) : if self . flat and len ( self . _fields ) == 1 : for row in self . query . get_compiler ( self . db ) . results_iter ( ) : yield row [ 0 ] elif not self . query . extra_select and not self . query . aggregate_select : for row in self . query . get_compiler ( self . db ) . results_iter ( ) : yield tuple ( row ) else : extra_names = self . query . extra_select . keys ( ) field_names = self . field_names aggregate_names = self . query . aggregate_select . keys ( ) names = extra_names + field_names + aggregate_names if self . _fields : fields = list ( self . _fields ) + filter ( lambda f : f not in self . _fields , aggregate_names ) else : fields = names for row in self . query . get_compiler ( self . db ) . results_iter ( ) : data = dict ( zip ( names , row ) ) yield tuple ( [ data [ f ] for f in fields ] ) def _clone ( self , * args , ** kwargs ) : clone = super ( ValuesListQuerySet , self ) . _clone ( * args , ** kwargs ) if not hasattr ( clone , "flat" ) : clone . flat = self . flat return clone class DateQuerySet ( QuerySet ) : def iterator ( self ) : return self . query . get_compiler ( self . db ) . results_iter ( ) def _setup_query ( self ) : self . query . clear_deferred_loading ( ) self . query = self . query . clone ( klass = sql . DateQuery , setup = True ) self . query . select = [ ] self . query . add_date_select ( self . _field_name , self . _kind , self . _order ) def _clone ( self , klass = None , setup = False , ** kwargs ) : c = super ( DateQuerySet , self ) . _clone ( klass , False , ** kwargs ) c . _field_name = self . _field_name c . _kind = self . _kind if setup and hasattr ( c , '_setup_query' ) : c . _setup_query ( ) return c class EmptyQuerySet ( QuerySet ) : def __init__ ( self , model = None , query = None , using = None ) : super ( EmptyQuerySet , self ) . __init__ ( model , query , using ) self . _result_cache = [ ] def __and__ ( self , other ) : return self . _clone ( ) def __or__ ( self , other ) : return other . _clone ( ) def count ( self ) : return 0 def delete ( self ) : pass def _clone ( self , klass = None , setup = False , ** kwargs ) : c = super ( EmptyQuerySet , self ) . _clone ( klass , setup = setup , ** kwargs ) c . _result_cache = [ ] return c def iterator ( self ) : yield iter ( [ ] ) . next ( ) def all ( self ) : return self def filter ( self , * args , ** kwargs ) : return self def exclude ( self , * args , ** kwargs ) : return self def complex_filter ( self , filter_obj ) : return self def select_related ( self , * fields , ** kwargs ) : return self def annotate ( self , * args , ** kwargs ) : return self def order_by ( self , * field_names ) : return self def distinct ( self , fields = None ) : return self def extra ( self , select = None , where = None , params = None , tables = None , order_by = None , select_params = None ) : assert self . query . can_filter ( ) , "Cannot change a query once a slice has been taken" return self def reverse ( self ) : return self def defer ( self , * fields ) : return self def only ( self , * fields ) : return self def update ( self , ** kwargs ) : return 0 def aggregate ( self , * args , ** kwargs ) : for arg in args : kwargs [ arg . default_alias ] = arg return dict ( [ ( key , None ) for key in kwargs ] ) value_annotation = False def get_klass_info ( klass , max_depth = 0 , cur_depth = 0 , requested = None , only_load = None , local_only = False ) : if max_depth and requested is None and cur_depth > max_depth : return None if only_load : load_fields = only_load . get ( klass ) for parent in klass . _meta . get_parent_list ( ) : fields = only_load . get ( parent ) if fields : load_fields . update ( fields ) else : load_fields = None if load_fields : skip = set ( ) init_list = [ ] for field , model in klass . _meta . get_fields_with_model ( ) : if field . name not in load_fields : skip . add ( field . name ) elif local_only and model is not None : continue else : init_list . append ( field . attname ) field_count = len ( init_list ) if skip : klass = deferred_class_factory ( klass , skip ) field_names = init_list else : field_names = ( ) else : if local_only and len ( klass . _meta . local_fields ) != len ( klass . _meta . fields ) : field_count = len ( klass . _meta . local_fields ) field_names = [ f . attname for f in klass . _meta . local_fields ] else : field_count = len ( klass . _meta . fields ) field_names = ( ) restricted = requested is not None related_fields = [ ] for f in klass . _meta . fields : if select_related_descend ( f , restricted , requested ) : if restricted : next = requested [ f . name ] else : next = None klass_info = get_klass_info ( f . rel . to , max_depth = max_depth , cur_depth = cur_depth + 1 , requested = next , only_load = only_load ) related_fields . append ( ( f , klass_info ) ) reverse_related_fields = [ ] if restricted : for o in klass . _meta . get_all_related_objects ( ) : if o . field . unique and select_related_descend ( o . field , restricted , requested , reverse = True ) : next = requested [ o . field . related_query_name ( ) ] klass_info = get_klass_info ( o . model , max_depth = max_depth , cur_depth = cur_depth + 1 , requested = next , only_load = only_load , local_only = True ) reverse_related_fields . append ( ( o . field , klass_info ) ) return klass , field_names , field_count , related_fields , reverse_related_fields def get_cached_row ( row , index_start , using , klass_info , offset = 0 ) : if klass_info is None : return None klass , field_names , field_count , related_fields , reverse_related_fields = klass_info fields = row [ index_start : index_start + field_count ] if fields == ( None , ) * field_count : obj = None else : if field_names : obj = klass ( ** dict ( zip ( field_names , fields ) ) ) else : obj = klass ( * fields ) if obj : obj . _state . db = using obj . _state . adding = False index_end = index_start + field_count + offset for f , klass_info in related_fields : cached_row = get_cached_row ( row , index_end , using , klass_info ) if cached_row : rel_obj , index_end = cached_row if obj is not None : setattr ( obj , f . get_cache_name ( ) , rel_obj ) if f . unique and rel_obj is not None : setattr ( rel_obj , f . related . get_cache_name ( ) , obj ) for f , klass_info in reverse_related_fields : cached_row = get_cached_row ( row , index_end , using , klass_info ) if cached_row : rel_obj , index_end = cached_row if obj is not None : setattr ( obj , f . related . get_cache_name ( ) , rel_obj ) if rel_obj is not None : setattr ( rel_obj , f . get_cache_name ( ) , obj ) for rel_field , rel_model in rel_obj . _meta . get_fields_with_model ( ) : if rel_model is not None : setattr ( rel_obj , rel_field . attname , getattr ( obj , rel_field . attname ) ) if rel_field . rel : try : cached_obj = getattr ( obj , rel_field . get_cache_name ( ) ) setattr ( rel_obj , rel_field . get_cache_name ( ) , cached_obj ) except AttributeError : pass return obj , index_end class RawQuerySet ( object ) : def __init__ ( self , raw_query , model = None , query = None , params = None , translations = None , using = None ) : self . raw_query = raw_query self . model = model self . _db = using self . query = query or sql . RawQuery ( sql = raw_query , using = self . db , params = params ) self . params = params or ( ) self . translations = translations or { } def __iter__ ( self ) : model_init_field_names = { } annotation_fields = [ ] db = self . db compiler = connections [ db ] . ops . compiler ( 'SQLCompiler' ) ( self . query , connections [ db ] , db ) need_resolv_columns = hasattr ( compiler , 'resolve_columns' ) query = iter ( self . query ) for pos , column in enumerate ( self . columns ) : if column in self . model_fields : model_init_field_names [ self . model_fields [ column ] . attname ] = pos else : annotation_fields . append ( ( column , pos ) ) skip = set ( ) for field in self . model . _meta . fields : if field . attname not in model_init_field_names : skip . add ( field . attname ) if skip : if self . model . _meta . pk . attname in skip : raise InvalidQuery ( 'Raw query must include the primary key' ) model_cls = deferred_class_factory ( self . model , skip ) else : model_cls = self . model model_init_field_pos = [ ] for field in self . model . _meta . fields : model_init_field_pos . append ( model_init_field_names [ field . attname ] ) if need_resolv_columns : fields = [ self . model_fields . get ( c , None ) for c in self . columns ] for values in query : if need_resolv_columns : values = compiler . resolve_columns ( values , fields ) if skip : model_init_kwargs = { } for attname , pos in model_init_field_names . iteritems ( ) : model_init_kwargs [ attname ] = values [ pos ] instance = model_cls ( ** model_init_kwargs ) else : model_init_args = [ values [ pos ] for pos in model_init_field_pos ] instance = model_cls ( * model_init_args ) if annotation_fields : for column , pos in annotation_fields : setattr ( instance , column , values [ pos ] ) instance . _state . db = db instance . _state . adding = False yield instance def __repr__ ( self ) : return "<RawQuerySet: %r>" % ( self . raw_query % tuple ( self . params ) ) def __getitem__ ( self , k ) : return list ( self ) [ k ] @ property def db ( self ) : return self . _db or router . db_for_read ( self . model ) def using ( self , alias ) : return RawQuerySet ( self . raw_query , model = self . model , query = self . query . clone ( using = alias ) , params = self . params , translations = self . translations , using = alias ) @ property def columns ( self ) : if not hasattr ( self , '_columns' ) : self . _columns = self . query . get_columns ( ) for ( query_name , model_name ) in self . translations . items ( ) : try : index = self . _columns . index ( query_name ) self . _columns [ index ] = model_name except ValueError : pass return self . _columns @ property def model_fields ( self ) : if not hasattr ( self , '_model_fields' ) : converter = connections [ self . db ] . introspection . table_name_converter self . _model_fields = { } for field in self . model . _meta . fields : name , column = field . get_attname_column ( ) self . _model_fields [ converter ( column ) ] = field return self . _model_fields def insert_query ( model , objs , fields , return_id = False , raw = False , using = None ) : query = sql . InsertQuery ( model ) query . insert_values ( fields , objs , raw = raw ) return query . get_compiler ( using = using ) . execute_sql ( return_id ) def prefetch_related_objects ( result_cache , related_lookups ) : from django . db . models . sql . constants import LOOKUP_SEP if len ( result_cache ) == 0 : return model = result_cache [ 0 ] . __class__ done_lookups = set ( ) done_queries = { } auto_lookups = [ ] followed_descriptors = set ( ) all_lookups = itertools . chain ( related_lookups , auto_lookups ) for lookup in all_lookups : if lookup in done_lookups : continue done_lookups . add ( lookup ) obj_list = result_cache attrs = lookup . split ( LOOKUP_SEP ) for level , attr in enumerate ( attrs ) : if len ( obj_list ) == 0 : break good_objects = True for obj in obj_list : if not hasattr ( obj , '_prefetched_objects_cache' ) : try : obj . _prefetched_objects_cache = { } except AttributeError : good_objects = False break else : break if not good_objects : break first_obj = obj_list [ 0 ] prefetcher , descriptor , attr_found , is_fetched = get_prefetcher ( first_obj , attr ) if not attr_found : raise AttributeError ( "Cannot find '%s' on %s object, '%s' is an invalid " % ( attr , first_obj . __class__ . __name__ , lookup ) ) if level == len ( attrs ) - 1 and prefetcher is None : raise ValueError ( "'%s' does not resolve to a item that supports " % lookup ) if prefetcher is not None and not is_fetched : current_lookup = LOOKUP_SEP . join ( attrs [ 0 : level + 1 ] ) if current_lookup in done_queries : obj_list = done_queries [ current_lookup ] else : obj_list , additional_prl = prefetch_one_level ( obj_list , prefetcher , attr ) if not ( lookup in auto_lookups and descriptor in followed_descriptors ) : for f in additional_prl : new_prl = LOOKUP_SEP . join ( [ current_lookup , f ] ) auto_lookups . append ( new_prl ) done_queries [ current_lookup ] = obj_list followed_descriptors . add ( descriptor ) else : obj_list = [ getattr ( obj , attr ) for obj in obj_list ] obj_list = [ obj for obj in obj_list if obj is not None ] def get_prefetcher ( instance , attr ) : prefetcher = None attr_found = False is_fetched = False rel_obj_descriptor = getattr ( instance . __class__ , attr , None ) if rel_obj_descriptor is None : try : rel_obj = getattr ( instance , attr ) attr_found = True except AttributeError : pass else : attr_found = True if rel_obj_descriptor : if hasattr ( rel_obj_descriptor , 'get_prefetch_query_set' ) : prefetcher = rel_obj_descriptor if rel_obj_descriptor . is_cached ( instance ) : is_fetched = True else : rel_obj = getattr ( instance , attr ) if hasattr ( rel_obj , 'get_prefetch_query_set' ) : prefetcher = rel_obj return prefetcher , rel_obj_descriptor , attr_found , is_fetched def prefetch_one_level ( instances , prefetcher , attname ) : rel_qs , rel_obj_attr , instance_attr , single , cache_name = prefetcher . get_prefetch_query_set ( instances ) additional_prl = getattr ( rel_qs , '_prefetch_related_lookups' , [ ] ) if additional_prl : rel_qs . _prefetch_related_lookups = [ ] all_related_objects = list ( rel_qs ) rel_obj_cache = { } for rel_obj in all_related_objects : rel_attr_val = rel_obj_attr ( rel_obj ) if rel_attr_val not in rel_obj_cache : rel_obj_cache [ rel_attr_val ] = [ ] rel_obj_cache [ rel_attr_val ] . append ( rel_obj ) for obj in instances : instance_attr_val = instance_attr ( obj ) vals = rel_obj_cache . get ( instance_attr_val , [ ] ) if single : if vals : setattr ( obj , cache_name , vals [ 0 ] ) else : qs = getattr ( obj , attname ) . all ( ) qs . _result_cache = vals qs . _prefetch_done = True obj . _prefetched_objects_cache [ cache_name ] = qs return all_related_objects , additional_prl
