""" Multi-part parsing for file uploads. Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to file upload handlers for processing. """ import cgi from django . conf import settings from django . core . exceptions import SuspiciousOperation from django . utils . datastructures import MultiValueDict from django . utils . encoding import force_unicode from django . utils . text import unescape_entities from django . core . files . uploadhandler import StopUpload , SkipFile , StopFutureHandlers __all__ = ( 'MultiPartParser' , 'MultiPartParserError' , 'InputStreamExhausted' ) class MultiPartParserError ( Exception ) : pass class InputStreamExhausted ( Exception ) : pass RAW = "raw" FILE = "file" FIELD = "field" class MultiPartParser ( object ) : def __init__ ( self , META , input_data , upload_handlers , encoding = None ) : content_type = META . get ( 'HTTP_CONTENT_TYPE' , META . get ( 'CONTENT_TYPE' , '' ) ) if not content_type . startswith ( 'multipart/' ) : raise MultiPartParserError ( 'Invalid Content-Type: %s' % content_type ) ctypes , opts = parse_header ( content_type ) boundary = opts . get ( 'boundary' ) if not boundary or not cgi . valid_boundary ( boundary ) : raise MultiPartParserError ( 'Invalid boundary in multipart: %s' % boundary ) try : content_length = int ( META . get ( 'HTTP_CONTENT_LENGTH' , META . get ( 'CONTENT_LENGTH' , 0 ) ) ) except ( ValueError , TypeError ) : content_length = 0 if content_length < 0 : raise MultiPartParserError ( "Invalid content length: %r" % content_length ) self . _boundary = boundary self . _input_data = input_data possible_sizes = [ x . chunk_size for x in upload_handlers if x . chunk_size ] self . _chunk_size = min ( [ 2 ** 31 - 4 ] + possible_sizes ) self . _meta = META self . _encoding = encoding or settings . DEFAULT_CHARSET self . _content_length = content_length self . _upload_handlers = upload_handlers def parse ( self ) : from django . http import QueryDict encoding = self . _encoding handlers = self . _upload_handlers if self . _content_length == 0 : return QueryDict ( MultiValueDict ( ) , encoding = self . _encoding ) , MultiValueDict ( ) for handler in handlers : result = handler . handle_raw_input ( self . _input_data , self . _meta , self . _content_length , self . _boundary , encoding ) if result is not None : return result [ 0 ] , result [ 1 ] self . _post = QueryDict ( '' , mutable = True ) self . _files = MultiValueDict ( ) stream = LazyStream ( ChunkIter ( self . _input_data , self . _chunk_size ) ) old_field_name = None counters = [ 0 ] * len ( handlers ) try : for item_type , meta_data , field_stream in Parser ( stream , self . _boundary ) : if old_field_name : self . handle_file_complete ( old_field_name , counters ) old_field_name = None try : disposition = meta_data [ 'content-disposition' ] [ 1 ] field_name = disposition [ 'name' ] . strip ( ) except ( KeyError , IndexError , AttributeError ) : continue transfer_encoding = meta_data . get ( 'content-transfer-encoding' ) if transfer_encoding is not None : transfer_encoding = transfer_encoding [ 0 ] . strip ( ) field_name = force_unicode ( field_name , encoding , errors = 'replace' ) if item_type == FIELD : if transfer_encoding == 'base64' : raw_data = field_stream . read ( ) try : data = str ( raw_data ) . decode ( 'base64' ) except : data = raw_data else : data = field_stream . read ( ) self . _post . appendlist ( field_name , force_unicode ( data , encoding , errors = 'replace' ) ) elif item_type == FILE : file_name = disposition . get ( 'filename' ) if not file_name : continue file_name = force_unicode ( file_name , encoding , errors = 'replace' ) file_name = self . IE_sanitize ( unescape_entities ( file_name ) ) content_type = meta_data . get ( 'content-type' , ( '' , ) ) [ 0 ] . strip ( ) try : charset = meta_data . get ( 'content-type' , ( 0 , { } ) ) [ 1 ] . get ( 'charset' , None ) except : charset = None try : content_length = int ( meta_data . get ( 'content-length' ) [ 0 ] ) except ( IndexError , TypeError , ValueError ) : content_length = None counters = [ 0 ] * len ( handlers ) try : for handler in handlers : try : handler . new_file ( field_name , file_name , content_type , content_length , charset ) except StopFutureHandlers : break for chunk in field_stream : if transfer_encoding == 'base64' : try : chunk = str ( chunk ) . decode ( 'base64' ) except Exception , e : raise MultiPartParserError ( "Could not decode base64 data: %r" % e ) for i , handler in enumerate ( handlers ) : chunk_length = len ( chunk ) chunk = handler . receive_data_chunk ( chunk , counters [ i ] ) counters [ i ] += chunk_length if chunk is None : break except SkipFile , e : exhaust ( field_stream ) else : old_field_name = field_name else : exhaust ( stream ) except StopUpload , e : if not e . connection_reset : exhaust ( self . _input_data ) else : exhaust ( self . _input_data ) for handler in handlers : retval = handler . upload_complete ( ) if retval : break return self . _post , self . _files def handle_file_complete ( self , old_field_name , counters ) : for i , handler in enumerate ( self . _upload_handlers ) : file_obj = handler . file_complete ( counters [ i ] ) if file_obj : self . _files . appendlist ( force_unicode ( old_field_name , self . _encoding , errors = 'replace' ) , file_obj ) break def IE_sanitize ( self , filename ) : return filename and filename [ filename . rfind ( "\\" ) + 1 : ] . strip ( ) class LazyStream ( object ) : def __init__ ( self , producer , length = None ) : self . _producer = producer self . _empty = False self . _leftover = '' self . length = length self . position = 0 self . _remaining = length self . _unget_history = [ ] def tell ( self ) : return self . position def read ( self , size = None ) : def parts ( ) : remaining = ( size is not None and [ size ] or [ self . _remaining ] ) [ 0 ] if remaining is None : yield '' . join ( self ) return while remaining != 0 : assert remaining > 0 , 'remaining bytes to read should never go negative' chunk = self . next ( ) emitting = chunk [ : remaining ] self . unget ( chunk [ remaining : ] ) remaining -= len ( emitting ) yield emitting out = '' . join ( parts ( ) ) return out def next ( self ) : if self . _leftover : output = self . _leftover self . _leftover = '' else : output = self . _producer . next ( ) self . _unget_history = [ ] self . position += len ( output ) return output def close ( self ) : self . _producer = [ ] def __iter__ ( self ) : return self def unget ( self , bytes ) : if not bytes : return self . _update_unget_history ( len ( bytes ) ) self . position -= len ( bytes ) self . _leftover = '' . join ( [ bytes , self . _leftover ] ) def _update_unget_history ( self , num_bytes ) : self . _unget_history = [ num_bytes ] + self . _unget_history [ : 49 ] number_equal = len ( [ current_number for current_number in self . _unget_history if current_number == num_bytes ] ) if number_equal > 40 : raise SuspiciousOperation ( ) class ChunkIter ( object ) : def __init__ ( self , flo , chunk_size = 64 * 1024 ) : self . flo = flo self . chunk_size = chunk_size def next ( self ) : try : data = self . flo . read ( self . chunk_size ) except InputStreamExhausted : raise StopIteration ( ) if data : return data else : raise StopIteration ( ) def __iter__ ( self ) : return self class InterBoundaryIter ( object ) : def __init__ ( self , stream , boundary ) : self . _stream = stream self . _boundary = boundary def __iter__ ( self ) : return self def next ( self ) : try : return LazyStream ( BoundaryIter ( self . _stream , self . _boundary ) ) except InputStreamExhausted : raise StopIteration ( ) class BoundaryIter ( object ) : def __init__ ( self , stream , boundary ) : self . _stream = stream self . _boundary = boundary self . _done = False self . _rollback = len ( boundary ) + 6 unused_char = self . _stream . read ( 1 ) if not unused_char : raise InputStreamExhausted ( ) self . _stream . unget ( unused_char ) try : from mx . TextTools import FS self . _fs = FS ( boundary ) . find except ImportError : self . _fs = lambda data : data . find ( boundary ) def __iter__ ( self ) : return self def next ( self ) : if self . _done : raise StopIteration ( ) stream = self . _stream rollback = self . _rollback bytes_read = 0 chunks = [ ] for bytes in stream : bytes_read += len ( bytes ) chunks . append ( bytes ) if bytes_read > rollback : break if not bytes : break else : self . _done = True if not chunks : raise StopIteration ( ) chunk = '' . join ( chunks ) boundary = self . _find_boundary ( chunk , len ( chunk ) < self . _rollback ) if boundary : end , next = boundary stream . unget ( chunk [ next : ] ) self . _done = True return chunk [ : end ] else : if not chunk [ : - rollback ] : self . _done = True return chunk else : stream . unget ( chunk [ - rollback : ] ) return chunk [ : - rollback ] def _find_boundary ( self , data , eof = False ) : index = self . _fs ( data ) if index < 0 : return None else : end = index next = index + len ( self . _boundary ) if data [ max ( 0 , end - 1 ) ] == '\n' : end -= 1 if data [ max ( 0 , end - 1 ) ] == '\r' : end -= 1 return end , next def exhaust ( stream_or_iterable ) : iterator = None try : iterator = iter ( stream_or_iterable ) except TypeError : iterator = ChunkIter ( stream_or_iterable , 16384 ) if iterator is None : raise MultiPartParserError ( 'multipartparser.exhaust() was passed a non-iterable or stream parameter' ) for __ in iterator : pass def parse_boundary_stream ( stream , max_header_size ) : chunk = stream . read ( max_header_size ) header_end = chunk . find ( '\r\n\r\n' ) def _parse_header ( line ) : main_value_pair , params = parse_header ( line ) try : name , value = main_value_pair . split ( ':' , 1 ) except : raise ValueError ( "Invalid header: %r" % line ) return name , ( value , params ) if header_end == - 1 : stream . unget ( chunk ) return ( RAW , { } , stream ) header = chunk [ : header_end ] stream . unget ( chunk [ header_end + 4 : ] ) TYPE = RAW outdict = { } for line in header . split ( '\r\n' ) : try : name , ( value , params ) = _parse_header ( line ) except : continue if name == 'content-disposition' : TYPE = FIELD if params . get ( 'filename' ) : TYPE = FILE outdict [ name ] = value , params if TYPE == RAW : stream . unget ( chunk ) return ( TYPE , outdict , stream ) class Parser ( object ) : def __init__ ( self , stream , boundary ) : self . _stream = stream self . _separator = '--' + boundary def __iter__ ( self ) : boundarystream = InterBoundaryIter ( self . _stream , self . _separator ) for sub_stream in boundarystream : yield parse_boundary_stream ( sub_stream , 1024 ) def parse_header ( line ) : plist = _parse_header_params ( ';' + line ) key = plist . pop ( 0 ) . lower ( ) pdict = { } for p in plist : i = p . find ( '=' ) if i >= 0 : name = p [ : i ] . strip ( ) . lower ( ) value = p [ i + 1 : ] . strip ( ) if len ( value ) >= 2 and value [ 0 ] == value [ - 1 ] == '"' : value = value [ 1 : - 1 ] value = value . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) pdict [ name ] = value return key , pdict def _parse_header_params ( s ) : plist = [ ] while s [ : 1 ] == ';' : s = s [ 1 : ] end = s . find ( ';' ) while end > 0 and s . count ( '"' , 0 , end ) % 2 : end = s . find ( ';' , end + 1 ) if end < 0 : end = len ( s ) f = s [ : end ] plist . append ( f . strip ( ) ) s = s [ end : ] return plist
