'\u0623'
FATHATAN
'\u064F'
'\u0622'
DAMMA
FATHA
'\u064B'
SHADDA
'\u0651'
DAMMATAN
'\u0652'
'\u0640'
ALEF_HAMZA_BELOW
ALEF_MADDA
KASRATAN
SUKUN
DOTLESS_YEH
ALEF_HAMZA_ABOVE
'\u0650'
'\u0625'
'\u064D'
TATWEEL
KASRA
'\u0649'
'\u064C'
'\u064E'
'\u0641'
BEH
'\u0648'
'\u0628'
stemPrefix
WAW
TEH
'\u0646'
'\u062A'
'\u0644'
FEH
LAM
deleteN
NOON
stemSuffix
"овци"
"ият"
"ия"
"ци"
removePlural
"еве"
"ен"
"е"
"та"
removeArticle
"си"
"ят"
"ове"
"зи"
"ът"
"ища"
"outro"
"essa"
"ambas"
"sua"
"quanto"
"tuas"
"porque"
"nas"
"quem"
"pelos"
"cujas"
"dos"
"demais"
"ambos"
"pelo"
"suas"
"eles"
"seu"
"qual"
"dele"
"depois"
"mesmos"
"uma"
"por"
"apos"
"deste"
"todo"
"mediante"
"tua"
"outras"
"seus"
"outros"
"tudo"
"mesma"
"cujos"
"proprio"
"tambem"
"desde"
"portanto"
"mesmo"
"dispoem"
"assim"
"sobre"
"quer"
"quando"
"nao"
"qualquer"
"aonde"
"neste"
"durante"
"cujo"
"pois"
"isso"
"essas"
"elas"
"tal"
"todos"
"propios"
"umas"
"teus"
"ha"
"mas"
"diversos"
"deles"
"seja"
"dispoe"
"aqueles"
"outra"
"estas"
"nem"
"aos"
"quais"
"ainda"
"todas"
"desta"
"sob"
"teu"
"contudo"
"menos"
"toda"
"perante"
"mesmas"
"dela"
"aquele"
"alem"
"como"
"entao"
"pelas"
"uns"
"isto"
"sendo"
"ela"
"diversas"
"diversa"
"contra"
"cuja"
"nesse"
BRAZILIAN_STOP_WORDS
"esta"
"logo"
"ucion"
"erieis"
getRV
"assemos"
removeSuffix
"esseis"
createCT
suffixPreceded
toReplace
"iamos"
"ivel"
"issemos"
getR1
preceded
"aveis"
(RV
"irieis"
(R2
"arao"
(R1
"essemos"
"ariamos"
vvalue
"erao"
changeTerm
"isseis"
"acao"
"ieis"
(CT
isIndexable
"logias"
"irao"
"avamos"
"iriamos"
CT
"iramos"
"eramos"
replaceSuffix
"asseis"
"aramos"
"arieis"
altered
(TERM
"acoes"
changeTo
"eriamos"
"www"
65248
65374
WORD_TYPE
preIsTokened
65281
BASIC_LATIN
HALFWIDTH_AND_FULLWIDTH_FORMS
stopTable
decompose
addAllLowerCase
hyp
hyphenationFile
hyphenationFilename
partLength
hyphenationReader
cv
"}{"
preBreak
postBreak
hyphenPoints
iIgnoreAtBeginning
equivChar
packValues
findPattern
iLength
stoplist
unpackValues
bEndOfLetters
hw
classmap
ivalues
pushCharCount
vspace
remainCharCount
va
nc
hstrcmp
7842107987915665573L
searchPatterns
for\n"
languages
"pre"
fragments\n"
all\n"
separated.
writing,
2004/02/27
encoding=\"US-ASCII\"?>\n"
applicable
getColumnNumber
tag,
characters.
characters,
hyphenated.\n"
#REQUIRED>\n"
compliance
countr
HyphenationDTDGenerator
patterns)>\n"
separated
setEntityResolver
Default
Apache
as\n"
KIND,
"hyphen-char"
attributes.\n"
"hyphen"
"[Warning]
cases\n"
minimun
contain\n"
indicated
hyphenChar
the\n"
(hyphen-char?,
http://www.apache.org/licenses/LICENSE-2.0\n"
generateDTD
hyphen-min
"<!ATTLIST
ELEM_EXCEPTIONS
law
want\n"
XMLReader:
patterns,
post-break
License.\n"
IS\"
"-->\n"
absence
CONDITIONS
pre-break=\"-\"/>.
setConsumer
(the
hyphenation.dtd,v
SAXParserFactory
License
getXMLReader
resolveEntity
systemId
\"full
governing
'equivalent'\n"
WITHOUT
exceptions?,
jeremias
group's
not\n"
Copyright
"<!--
"post"
"<!--\n"
EMPTY>\n"
Patterns
zero.
See
aesthetic
treated
Error]
ELEM_PATTERNS
that\n"
errMsg
hyphen-min?,\n"
pre-break,
\"AS
\\discretionary\n"
'-'\n"
"[Fatal
equivalent\n"
agreed
getSystemId
<hyphen
digit\n"
WARRANTIES
(#PCDATA)>\n"
express
fatalError
normalizeException
hyphenation-info
ELEM_HYPHEN
DTD_STRING
Exp
hyphen-char
exceptions:
words.\n"
at\n"
hyphen
group\n"
software\n"
TeX's
ending\n"
for.\n"
hyphen\"
To
procedure
character.
"patterns"
publicId
"<!ELEMENT
begining
1999-2004
(#PCDATA|hyphen)*
"[Error]
Defaults
finds
#IMPLIED>\n"
break.
getLocationString
18:34:59
except
shortcut
permissions
-->\n"
groups,
OF
no-break
"exception:
"classes"
before,
getExceptionWord
limitations
currElement
equivalent
this\n"
i.e.
getInterletterValues
own
Foundation\n"
concerned.
$Id:
Unless
ELEM_CLASSES
BASIS,\n"
newSAXParser
"exceptions"
classes:
classes,
specified.
words.
warning
License,
\"License\");\n"
accounted
shortcut.
SAXParseException
purposes,
Licensed
implied.\n"
strkey
climb
"palos"
compact
newsize
freenode
"alto"
BLOCK_SIZE
strcmp
removeAllElements
leaf
insertBalanced
startB
startA
strcpy
"Carlos"
"Car"
redimNodeArrays
"Node
strlen
"Key
knows
curkey
rewind
kx
"m\u00edt"
"pouze"
"j\u00ed\u017e"
"m\u016fj"
"\u010dl\u00e1nku"
"proto\u017ee"
"nad"
"m\u011b"
"prav\u00e9"
"aj"
"pro\u010d"
"ho"
"zp\u011bt"
"atd"
"ani"
"bylo"
"jse\u0161"
"t\u011bmu"
"ne\u017e"
"sv\u00fdmi"
"jsem"
"ona"
"mezi"
"byla"
"sv\u00fdm"
"sv\u00fdch"
"p\u0159ed"
"j\u00ed"
"pak"
"tento"
"je\u017e"
"va\u0161e"
"jeho"
"tomu"
"byli"
"nic"
"jeho\u017e"
"cz"
"napi\u0161te"
"bude"
"ji"
"jste"
"tato"
"tomto"
"pro"
"tohle"
"kde"
"t\u00e9ma"
"tipy"
"tedy"
"co\u017e"
"jsme"
"zda"
"nejsou"
"ze"
"jen"
"bude\u0161"
"nebo"
"kdo"
"neg"
"t\u00edm"
"na\u010de\u017e"
"bez"
"m\u016f\u017ee"
"jej\u00ed"
"strana"
"nen\u00ed"
"p\u0159es"
"ony"
"b\u00fdt"
"oni"
"j\u00e1"
"kter\u00fd"
"ale"
"jako"
"jako\u017e"
"v\u00e1s"
"kam"
"podle"
"kter\u00e1"
"atp"
"\u010dl\u00e1nky"
"ke"
"n\u00e1s"
"v\u0161ak"
"tuto"
"v\u00edce"
"u\u017e"
"tak\u017ee"
"tohoto"
"jak"
"jsou"
"dnes"
"tak\u00e9"
"nov\u00fd"
"tomuto"
"prvn\u00ed"
"kte\u0159\u00ed"
"pod"
"jemu"
"p\u0159i"
"n\u00e1m"
"kter\u00e9"
CZECH_STOP_WORDS
"sv\u00e9"
"je\u0161t\u011b"
"aby"
"dal\u0161\u00ed"
"byl"
"toho"
"teto"
"n\u011bmu\u017e"
"kdy\u017e"
"m\u00e1te"
"kterou"
"\u010di"
"ji\u017e"
"ve"
"za"
"jeliko\u017e"
"jin\u00e9"
"pta"
"asi"
"zde"
"a\u017e"
"mne"
"t\u011bm"
"tyto"
"p\u0159i\u010dem\u017e"
"m\u00e1"
"jakmile"
"tom"
"jej"
"zpr\u00e1vy"
"n\u011bmu"
"t\u00edmto"
"jejich"
"na\u0161i"
"nov\u00e9"
"proto"
"budem"
"v\u00e1m"
"ům"
"ov"
"atům"
"čt"
"atech"
"ěte"
"ech"
"ého"
"ách"
"ích"
"ové"
"ětem"
"ami"
"imu"
"ama"
"ěmi"
"št"
"ímu"
"ěti"
"ův"
removeCase
"aty"
"ovi"
"ám"
"iho"
"ých"
"emi"
"ým"
"ému"
"eti"
"ího"
"ými"
"ém"
"ími"
"ím"
removePossessives
"ich"
"etem"
"danish_stop.txt"
"eine"
"sie"
"ihres"
"dich"
"und"
"daß"
GERMAN_STOP_WORDS
"mein"
"ihre"
"mich"
"oder"
"mir"
"ohne"
DEFAULT_SET_30
"der"
"eines"
"ihr"
"einem"
"german_stop.txt"
"sein"
"einer"
"für"
"dass"
"wird"
"wie"
"wegen"
"von"
"wir"
"einen"
"auf"
"kein"
"durch"
"wer"
"war"
substCount
doMore
removeParticleDenotion
"gege"
resubstitute
deleteCharAt
"erin*"
"εκεινο"
"αυτων"
"εισαι"
"τα"
"εκεινοσ"
"προσ"
"ειμαι"
"εκεινη"
"του"
"για"
"παρα"
"και"
"απο"
"μη"
"την"
"κατα"
"στη"
"ομωσ"
"στο"
"οι"
"των"
"εκεινα"
"οτι"
"θα"
"εκεινεσ"
"ειμαστε"
"αυτη"
"αυτοσ"
"ειναι"
"ειστε"
"ενω"
"σε"
"στον"
"ποιεσ"
"αυτοι"
"ωσ"
"με"
"δε"
"κι"
"ποια"
"η"
"στην"
"αυτουσ"
"αν"
"οσο"
"ο"
"αυτο"
"εκεινοι"
"μετα"
"επι"
"τησ"
"μην"
GREEK_STOP_WORDS
"αλλα"
"ποιο"
"το"
"αντι"
"ισωσ"
"που"
"τον"
"να"
"οπωσ"
"τοτε"
"δεν"
"μα"
"ποιουσ"
"ποιοσ"
"εαν"
"ποιοι"
"εκεινων"
"κ"
"εκεινουσ"
"αυτα"
"ποιων"
"πωσ"
"αυτεσ"
'\u03AC'
'\u038A'
'\u03B9'
'\u03CC'
'\u03C9'
codepoint
'\u03B5'
'\u0389'
'\u03A2'
'\u03CD'
'\u0386'
'\u03AA'
'\u0388'
'\u03CE'
'\u038C'
'\u03AF'
'\u038E'
'\u03BF'
'\u03C2'
'\u03C3'
'\u03C5'
'\u03AD'
'\u0390'
'\u038F'
'\u03AE'
'\u03B1'
'\u03CA'
'\u03AB'
'\u03B7'
'\u03B0'
'\u03CB'
"spanish_stop.txt"
'\u06C0'
HEH_YEH
'\u06A9'
'\u06D2'
FARSI_YEH
YEH_BARREE
KEHEH
'\u0654'
HAMZA_ABOVE
HEH_GOAL
'\u06CC'
'\u06C1'
"finnish_stop.txt"
poz
apos
minPoz
DEFAULT_ARTICLES
setArticles
apostrophes
"toutes"
"dans"
"une"
"notre"
"lui"
"desquelles"
"celle"
"avant"
"french_stop.txt"
"jusque"
"vôtres"
"moins"
"hormis"
"son"
"pourquoi"
"ainsi"
"ces"
"environ"
"aujourd"
stemExclutionSet
"qui"
"tout"
"divers"
"sur"
"ès"
"jusqu"
"voici"
"etre"
"excepté"
"chez"
"ceci"
"votre"
"plein"
"certaine"
"des"
"tiennes"
"lequel"
"donc"
"dès"
"comme"
"quel"
"diverses"
"pour"
"cependant"
"revoici"
"va"
"plusieurs"
"tous"
"autres"
"seront"
"sinon"
"pas"
"concernant"
"toute"
"pendant"
"depuis"
"ils"
"même"
"vu"
"toi"
"vous"
"autre"
"siens"
"ses"
"désormais"
"été"
"suivant"
"car"
"devers"
"devra"
"doit"
"dessous"
"avoir"
"nous"
"sont"
"auxquels"
"lesquels"
"quelles"
"contre"
"dedans"
"aussi"
"là"
"devant"
"celui"
"non"
"après"
"miens"
"vos"
"avait"
"moi"
"revoilà"
"mes"
"ceux"
"vôtre"
"diverse"
"quels"
"près"
"ma"
FRENCH_STOP_WORDS
"combien"
"mêmes"
"où"
"malgré"
"soi"
"hui"
"auxquelles"
"elles"
"sans"
"certain"
"outre"
"plus"
"moyennant"
"quand"
"afin"
"elle"
"sauf"
"voilà"
"hors"
"mon"
"laquelle"
"nôtres"
"avec"
"parmi"
"mienne"
"être"
"tien"
"passé"
"selon"
"ont"
"merci"
"quelle"
"tiens"
"ton"
"miennes"
"ça"
"nôtre"
"hélas"
"debout"
"partant"
"tienne"
"quoi"
"lesquelles"
"certaines"
"leur"
"puisque"
"derrière"
"sous"
"auquel"
"dessus"
"attendu"
"cet"
"leurs"
"sien"
"dont"
"desquels"
"néanmoins"
"certains"
"duquel"
"dehors"
"siennes"
"vers"
"sienne"
"quoique"
"mien"
"durant"
"proche"
"cette"
"lorsque"
"delà"
"soit"
"celles"
"icat"
"îtes"
retrieveRV
"ît"
deleteFrom
"Ière"
vowel
autre
"Aît"
"ité"
seenVowel
"èrent"
"ât"
deleteFromIfTestVowelBeforeIn
step2b
step2a
retrieveR
deleteButSuffixFrom
replaceFrom
"ées"
"Aient"
"aît"
"Ais"
"âmes"
"âtes"
consonne
deleteButSuffixFromElseReplace
modified
"ée"
seenConson
"ités"
"Ait"
"ière"
R0
treatVowels
"Ai"
"és"
setStrings
deleteFromIfPrecededIn
"îmes"
'\u0948'
'\u0929'
'\u093F'
'\u092F'
'\u0916'
'\u0921'
'\u0908'
'\u095A'
'\u090F'
'\u0917'
'\u0928'
'\u0947'
'\u0907'
'\u095B'
'\u090E'
'\u0910'
'\u091C'
'\u0946'
'\u0906'
'\u095C'
'\u0958'
'\u0933'
'\u0911'
'\u0922'
'\u090D'
'\u0940'
'\u0905'
'\u095D'
'\u0959'
'\u0961'
'\u0930'
'\u0945'
'\u0912'
'\u090C'
'\u0972'
'\u095E'
'\u0960'
'\u0931'
'\u092B'
'\u094C'
'\u0913'
'\u095F'
'\u0962'
'\u090B'
'\u0944'
'\u093C'
'\u0943'
'\u0963'
'\u094D'
'\u094B'
'\u0902'
'\u200C'
'\u090A'
'\u0942'
'\u0914'
'\u094A'
'\u0901'
'\u0934'
'\u0941'
'\u0949'
'\u0915'
'\u0909'
"नाओं"
"ूंगी"
"एंगे"
"एंगी"
"ाया"
"िए"
"ाकर"
"ू"
"ेंगे"
"ियों"
"ाओ"
"ाओं"
"ो"
"ाने"
"ेगा"
"ातीं"
"ाओगे"
"ियाँ"
"ाएगा"
"ाएगी"
"ोगे"
"ेंगी"
"ी"
"ाता"
"ाना"
"ना"
"ाएंगी"
"ोगी"
"ते"
"ेगी"
"ा"
"ाएंगे"
"े"
"ुएं"
"ताएं"
"ें"
"ाती"
"ाइए"
"ाए"
"ीं"
"ाइयों"
"ने"
"ि"
"ाओगी"
"ाई"
"नाएं"
"ाएं"
"ाते"
"ाईं"
"तीं"
"ाऊंगी"
"ताओं"
"ु"
"ों"
"ाँ"
"ता"
"ुआं"
"ती"
"ूंगा"
"ाइयाँ"
"ाऊंगा"
"ुओं"
"ां"
"ाइयां"
"ियां"
"कर"
"नी"
"hungarian_stop.txt"
decompMask
ORIYA
0x2B
0x0B80
decompositions
0x3E
0x28
0x29
0x3C
0x0980
0x22
0x24
KANNADA
GUJARATI
0x0B00
0x34
0x33
0x2C
0x2F
0x2E
TAMIL
0x0A00
0x45
0x46
0x47
0x48
0x49
0x5C
0x5B
0x5A
0x5F
TELUGU
0x5D
0x0A80
0x57
0x56
0x55
0x58
0x4A
0x4B
0x4C
0x4D
0x4E
compose
0x7B
MALAYALAM
0x0D00
0x7A
0x7C
0x7E
0x7D
GURMUKHI
0x0C80
0x0C00
DEVANAGARI
block2
block1
block0
0x1C
0x09
0x05
BENGALI
0x59
0x0900
ScriptData
scripts
0x0D
0x0B
0x35
0x5E
0x17
0x16
0x15
0x14
0x12
0x11
"italian_stop.txt"
"whensoever"
"themselves"
"ever"
FastStringTokenizer
"everyone"
"toward"
"onto"
"below"
"would"
"either"
"through"
"therefor"
"enough"
EXTENDED_ANALYZER
"among"
"had"
"thence"
"whilst"
"against"
"anyhow"
"sometimes"
"around"
"whence"
"everything"
"whereto"
"whomever"
"albeit"
"xauthor"
"whither"
"ltd"
"any"
"neither"
"thus"
"very"
"wherewith"
"can"
"whereafter"
"often"
"hereby"
"until"
"nowhere"
"although"
"wherefrom"
"otherwise"
"before"
"xother
"above"
"whereby"
"itself"
"hereupon"
"everywhere"
"along"
"hers"
"within"
"beyond"
"still"
"his"
"via"
"nor"
"which"
"though"
"may"
"whole"
"so"
"own"
1303507063
"per"
"whereas"
"its"
"elsewhere"
"even"
"beside"
"indeed"
"besides"
"whereunto"
"anywhere"
"yourself"
"yours"
"have"
"how"
"behind"
"thereafter"
"hence"
"becomes"
"much"
"seeming"
"seems"
"under"
"hereafter"
"well"
"whatsoever"
"could"
"ours"
"across"
"towards"
"several"
"amongst"
"formerly"
"always"
"whereof"
"without"
"except"
"ourselves"
"latterly"
"herein"
"somehow"
"mostly"
"else"
"adj"
"former"
"inc"
"became"
"myself"
"than"
"down"
"seemed"
"perhaps"
"others"
"why"
"whoever"
isStopWord
"pattern
"therein"
"while"
"every"
"thereupon"
"between"
"since"
"after"
"\\W+"
"being"
"eg"
"about"
"too"
"them"
EXTENDED_ENGLISH_STOP_WORDS
"herself"
"whomsoever"
"thereby"
"xsubj"
"wherever"
"xnote"
"those"
"sometime"
"someone"
"already"
"somewhere"
"moreover"
"been"
"himself"
"nobody"
"wherein"
"xcal"
"whereon"
"whereinto"
"you"
"whose"
"whom"
"together"
"who"
"because"
"anyone"
"latter"
"anything"
"almost"
"also"
"meanwhile"
"alone"
"noone"
"least"
"once
"whichever"
"rather"
"few"
"yet"
"whenever"
"whichsoever"
"afterwards"
"whosoever"
"thru"
"become"
"beforehand"
"nevertheless"
"however"
"upon"
"whereupon"
"might"
"whereat"
"whether"
"each"
"both"
"further"
"seem"
"when"
1218418418
eqPattern
"during"
"namely"
"our"
"most"
"becoming"
"throughout"
"many"
updateInputToken
lastInputToken
p_flagsAtt
p_payloadAtt
previousPrefixToken
prefixExhausted
setCurrentToken
p_posIncrAtt
getNextSuffixInputToken
p_typeAtt
setSuffix
p_termAtt
p_offsetAtt
getSuffix
getNextPrefixInputToken
oldPos
stemdictFile
"dutch_stop.txt"
DUTCH_STOP_WORDS
"kind"
"eier"
"bromfiets"
"kinder"
"fiets"
stemdict
stemdictionary
lengthR1
isValidEnEnding
getRIndex
step3b
enEnding
_removedE
storeYandI
_R2
enend
unDouble
isValidSEnding
_R1
step3a
_stemDict
reStoreYandI
"norwegian_stop.txt"
charsetName
firstTokenPositioned
"portuguese_stop.txt"
maxPercentDocs
streamMap
allStopWords
defaultMaxDocFreqPercent
withStopFilter
stopWordsPerField
internedFieldName
NOMARKER
charInput
'\u200F'
RTL_DIRECTION_MARKER
INFORMATION_SEPARATOR_MARKER
reverseUnicode3
'\u001F'
PUA_EC00_MARKER
'\uEC00'
'\uFFFF'
"вас"
"не"
"им"
"что"
"когда"
"из"
"но"
"же"
"ее"
"бы"
"под"
"ко"
"был"
"однако"
"ей"
"их"
"чтобы"
"все"
"эти"
"чье"
"наш"
"кто"
"чей"
"во"
"может"
"того"
"хотя"
"нет"
"либо"
"в"
"очень"
"только"
"всех"
"это"
"эта"
"всего"
"он"
"чего"
"том"
"ли"
"по"
"если"
"к"
"мне"
"ею"
"также"
"даже"
"со"
"уже"
"она"
"его"
"за"
"у"
"мы"
"надо"
"тоже"
"да"
"вот"
"более"
"есть"
"об"
"russian_stop.txt"
"они"
"оно"
"еще"
"для"
"так"
"нее"
"от"
"ты"
"той"
"быть"
"вы"
"было"
"чем"
"них"
"ни"
"там"
"до"
"него"
"на"
"была"
"или"
"где"
"были"
"ну"
"без"
"такой"
RUSSIAN_STOP_WORDS_30
"здесь"
"при"
"чья"
"весь"
"вам"
DIGIT_0
DIGIT_9
participleEndings2
participleEndings1
'\u043C'
perfectiveGerundEndings2
perfectiveGerundEndings1
reflexive
nounEndings
thePredessors
SHCH
derivationalEndings
reflexiveEndings
vowels
'\u043B'
verbEndings2
verbEndings1
verb
'\u044E'
findEnding
stemmingZone
markPositions
adjectiveEndings
perfectiveGerund
'\u044F'
undoubleN
removeSoft
superlative
'\u0435'
theWord
stemmingIndex
endingLength
doubleN
predessorLength
'\u0449'
IU
'\u0438'
S
'\u0445'
'\u044D'
findAndRemoveEnding
'\u0433'
IA
theEnding
'\u0442'
superlativeEndings
'\u044C'
'\u0432'
'\u044B'
participle1Predessors
'\u0443'
verb1Predessors
SH
removeI
perfectiveGerund1Predessors
'\u0448'
'\u043E'
derivational
SOFT
I_
'\u0441'
'\u043D'
'\u0430'
noun
'\u0439'
theEndingClass
adjectival
getMaxShingleSize
getTokenSeparator
isOutputUnigrams
getMinShingleSize
shingleBuilder
getNextShingle
shiftInputWindow
minValue
inputWindow
minTokNum
insertFillerToken
numFillerTokensToInsert
FILLER_TOKEN
CircularSequence
DEFAULT_TOKEN_TYPE
nextInputStreamToken
setTokenType
tokNum
atMinValue
getFirst
tokenAvailable
settingsCodec
"first="
ignoringSinglePrefixOrSuffixShingle
getMaximumShingleSize
columns
incrementColumnRowCounters
shingleToken
nextTokensPermutation
in_termAtt
currentReaderRow
defaultSpacerCharacter
deletedColumn
columnsHasBeenCreated
minimumShingleSize
request_next_token
currentPermutationRows
setSpacerCharacter
readColumn
getSpacerCharacter
currentReaderColumn
calculateShingleWeight
setFirst
setLast
readColumnBuf
in_payloadAtt
tokens="
getMatrix
in_posIncrAtt
"Matrix{"
0d
partWeight
in_offsetAtt
shinglesSeen
setMatrix
updateToken
deletedColumnTokens
currentPermutationTokensStartOffset
getNextInputToken
getMinimumShingleSize
1d
setMaximumShingleSize
currentPermuationTokens
isIgnoringSinglePrefixOrSuffixShingle
rows="
TokenSettingsCodec
deletedColumnToken
currentPermutationStartOffset
produceNextToken
"columns="
setIgnoringSinglePrefixOrSuffixShingle
positioning
permutations
ignoringSinglePrefixOrSuffixShingleByDefault
tokenPositioner
getTokenPositioner
permutationIterator
currentShingleLength
"Column{"
columnRowCounters
setMinimumShingleSize
maximumShingleSize
in_flagsAtt
isLast
shinglesSeenIterator
getColumns
"Row{"
in_typeAtt
rowsPermutation
spacerCharacter
DATE_TYPE
typeToMatch
"swedish_stop.txt"
THAI
getWordInstance
breaker
thaiState
'\u0307'
LATIN_SMALL_LETTER_DOTLESS_I
isBeforeDot
COMBINING_DOT_ABOVE
'\u0069'
iOrAfter
'\u0049'
LATIN_CAPITAL_LETTER_I
LATIN_SMALL_LETTER_I
EMPTY_PARAMS
methodname
adjustment
v_size
resobj
lb
common_j
common_i
operation"
0X1
0X7
out_range
c_ket
out_range_b
in_range_b
replace_s
in_range
"faulty
assign_to
first_key_inspected
EMPTY_ARGS
eq_v
slice_check
c_bra
<output
outstream
[-o
<algorithm>
file>]"
<input
stemMethod
TestApp
"ered"
"ernes"
"heder"
"erendes"
"heds"
"l\u00F8s"
"erende"
"erne"
"erer"
"erens"
"ethed"
"hedens"
"l\u00F8st"
"erede"
"erets"
"eret"
"eren"
r_en_ending
"\u00EF"
r_e_ending
"gem"
B_e_found
g_v_I
g_v_j
"gener"
"dying"
"ugli"
"eedly"
r_exception2
"idl"
"howe"
"fulli"
"earli"
r_Step_5
"ugly"
"proceed"
"ski"
"bias"
"atlas"
"idly"
"sky"
"exceed"
"earring"
g_valid_LI
"arsen"
r_exception1
"gentl"
"cosmos"
"news"
"onli"
"singly"
"outing"
"og"
"commun"
"lessli"
"skis"
"singl"
"lie"
"ogi"
"inning"
"gently"
"'s'"
"skies"
"lying"
"canning"
"herring"
"tying"
"tta"
"imma"
r_possessive
"mma"
r_particle_etc
"ll\u00E4"
"lt\u00E4"
"imm\u00E4"
B_ending_removed
"r_LONG"
"ns\u00E4"
"r_VI"
"imp\u00E4"
"h\u00E4n"
r_case_ending
"immi"
"st\u00E4"
"lta"
"tt\u00E4"
"k\u00E4\u00E4n"
"ksi"
"\u00E4\u00E4"
"p\u00E4"
"ssa"
"h\u00F6n"
"ko"
g_V1
"lle"
"eja"
"kaan"
r_i_plural
"han"
"k\u00F6"
"\u00F6\u00F6"
"\u00E4n"
"mme"
"sta"
"hen"
"mp\u00E4"
g_particle_end
"sti"
"hon"
r_other_endings
"hin"
"ss\u00E4"
g_AEI
"siin"
"mmi"
"nsa"
"mpa"
"ej\u00E4"
r_VI
"tten"
"lla"
S_x
g_V2
"impi"
r_t_plural
"mm\u00E4"
"t\u00E4"
r_tidy
"mpi"
"kin"
"kse"
r_LONG
"nne"
"impa"
"n\u00E4"
"it\u00E9"
"\u00EEtes"
"i\u00E8re"
"tap"
"\u00E8rent"
"\u00E9s"
"col"
r_i_verb_suffix
"\u00EEmes"
"I\u00E8re"
"\u00E2mes"
r_un_double
"I\u00E8r"
"\u00EEt"
"it\u00E9s"
"\u00E2t"
g_keep_with_s
"\u00E9es"
"\u00E9e"
"\u00E2tes"
"i\u00E8r"
r_un_accent
replab6
"\u00E9rt"
"aid"
"cs"
"jai"
"jei"
"jeik"
"ak"
"vel"
"\u00E1nk"
"eid"
"r\u00F3l"
"astul"
"nek"
"ok"
"\u00E1stul"
"v\u00E1"
"\u00E9\u00E9"
"k\u00E9pp"
"eim"
"\u00E9st\u00FCl"
r_case_other
"\u00E1m"
"hoz"
"\u00E9m"
"\u00FCk"
"tty"
r_owned
"ban"
"hez"
r_v_ending
"\u00E1ik"
"onk\u00E9nt"
"ak\u00E9"
"enk\u00E9nt"
"itek"
"gy"
"t\u00F3l"
"ek\u00E9"
"zs"
"\u00E1i"
"eink"
"ja"
"\u00E1id"
"\u00E9n"
"jeink"
"\u00F6k"
"b\u00F3l"
r_instrum
"ny"
"\u00E1n"
"k\u00E9"
"stul"
"aitok"
"ggy"
"aik"
"jeitek"
"k\u00E9nt"
"\u00E9im"
"dzs"
"\u00E9nk"
"jaim"
"\u00E1nk\u00E9nt"
"ccs"
"k\u00E9ppen"
"zzs"
"n\u00E9l"
"\u00E9k\u00E9"
"r\u00F5l"
"\u00E1ink"
"jaink"
"kor"
"\u00E9d"
"b\u00F5l"
"\u00E1juk"
"nny"
"eik"
"\u00FCnk"
r_plur_owner
"ssz"
"t\u00F5l"
"eitek"
"n\u00E1l"
"\u00E9k"
"\u00E1k\u00E9"
"\u00F6t"
"st\u00FCl"
r_case
"\u00E9ink"
r_sing_owner
"j\u00FCk"
"jaik"
"\u00FCl"
"ank\u00E9nt"
"\u00E9\u00E9i"
"\u00E9ik"
"om"
"\u00E9j\u00FCk"
"\u00E1k"
"aim"
"unk"
"uk"
"\u00E1\u00E9i"
r_plural
"h\u00F6z"
"\u00F6n"
"jeim"
"ok\u00E9"
"\u00E1d"
"juk"
r_factive
"nk"
"ek"
"\u00E9i"
"aink"
"\u00F6d"
"v\u00E9"
"ben"
"\u00E1im"
"\u00F6k\u00E9"
"jaid"
"nak"
"sz"
"lly"
"ot"
"\u00E9itek"
"\u00E9id"
"est\u00FCl"
"\u00E1itok"
r_double
"jeid"
"jaitok"
r_case_special
"assero"
"isco"
"ose"
"enza"
"tene"
"eva"
"amenti"
"erebbero"
"ireste"
"uzioni"
"veli"
"ivamo"
"vene"
"eremo"
"Yamo"
"ito"
"avate"
"issero"
"celi"
"uto"
"melo"
"evo"
"ist\u00E8"
"irebbe"
"gliene"
"ivano"
"mela"
"erono"
"qU"
"\u00E0"
"ereste"
"evamo"
"ichi"
"ano"
"irebbero"
"imenti"
"mele"
"vela"
"\u00EC"
"glielo"
"ato"
"enda"
"atrici"
"evate"
"avo"
"isci"
"ist\u00E0"
"usione"
"erebbe"
"celo"
"eranno"
"er\u00E0"
"evano"
g_AEIO
"mene"
"uzione"
"vele"
"meli"
"iremo"
"iranno"
"erete"
"azione"
"sene"
"ir\u00E0"
"ismi"
"usioni"
"immo"
"essero"
"gliele"
"arono"
"cene"
"teli"
g_CG
"eremmo"
"atore"
"ivate"
"gliela"
"endi"
"irono"
"iscono"
"isca"
"gli"
"avi"
"iremmo"
"evi"
"cele"
"tele"
"er\u00F2"
"assi"
"iamo"
"isce"
"it\u00E0"
"\u00F9"
"avamo"
"iscano"
"vi"
"irete"
"ir\u00F2"
"tela"
"\u00F2"
"eresti"
"telo"
"assimo"
"enze"
"avano"
"ammo"
"iche"
"glieli"
"ist\u00EC"
"emmo"
"anze"
"iresti"
"azioni"
"velo"
r_Step_1
r_Step_6
g_v_WX
"pt"
replab19
"ische"
B_GE_removed
"igst"
"erigst"
"hh"
"ioneel"
r_measure
r_Step_7
r_VX
"erig"
"tant"
replab8
golab21
"teer"
"achtigst"
"aar"
"laar"
g_AOU
"dst"
"atief"
"eriger"
"lijke"
"erij"
KpStemmer
"atie"
"arij"
replab10
"achtig"
"iger"
"tst"
r_Lose_prefix
"rnt"
"ster"
"gie"
"eer"
"raar"
"iteit"
"'t"
B_stemmed
"achtiger"
"naar"
r_lengthen_V
"lijker"
"rder"
"sel"
r_Lose_infix
"rn"
"ieve"
g_AIOU
"lijkst"
"ft"
"mp"
"pex"
"ines"
"eously"
"r_R"
"atives"
"ination"
"ished"
"icalize"
"alness"
"rpt"
"ivities"
"ented"
"eity"
"r_G"
"ying"
"ery"
"r_I"
"izational"
"ionalism"
"iousness"
"anced"
294
"oidally"
"ister"
"tic"
"uad"
"ariness"
"lis"
"oid"
LovinsStemmer
"r_P"
"ability"
"antic"
"arizer"
"entness"
"icists"
"lessness"
"ifully"
"iedly"
"ature"
"icide"
"ishness"
"ancy"
"entiality"
"mis"
"ativism"
"ialize"
"entiate"
"enly"
"aging"
"idine"
r_respell
"lud"
"r_M"
"antialness"
"ication"
"ibleness"
"iev"
"eris"
"meter"
"r_B"
"icism"
"antaneous"
"ently"
"ese"
"atingly"
"ional"
"alist"
"ionist"
"ionally"
"pic"
142
140
"ur"
"iteness"
148
"icalness"
r_P
"dex"
"ively"
"ials"
"ics"
"r_BB"
"yz"
"acity"
"isation"
"ond"
"ings"
"ancing"
135
"fully"
"ious"
"eal"
"ide"
"ealy"
"ily"
"icians"
"uas"
"izement"
"r_A"
"vad"
"cis"
"r_F"
"yt"
"ian"
"r_H"
"ated"
"ency"
"ancial"
"ons"
"entially"
"iness"
"eless"
"entials"
"iality"
"encible"
"eous"
"ward"
"itousness"
"ivity"
"icant"
"ionals"
"ates"
"r_S"
"ingness"
"itous"
"icianry"
"wise"
"bic"
"encing"
"enting"
"yl"
"ivistic"
"ides"
"ativeness"
"ities"
r_BB
"met"
"arily"
r_CC
"cid"
"aic"
"isms"
"bex"
"antiality"
"r_Z"
r_X
r_Y
r_Z
"r_L"
r_Q
r_R
r_S
"aceous"
r_U
r_W
r_H
r_J
r_K
r_L
r_M
r_O
r_A
r_B
r_D
r_E
r_F
r_G
"r_E"
"entally"
"ician"
"r_CC"
"ely"
"ary"
"ec"
"ryst"
"alities"
"ians"
"arize"
"metr"
"ateness"
"acy"
"ential"
"ihood"
"otide"
"ialist"
"icalism"
"r_W"
"r_N"
"arly"
r_endings
"hes"
"oidism"
"izability"
"r_Y"
"arizability"
"allic"
"ionalize"
"eful"
"ys"
"arity"
"olut"
"ogen"
"entations"
"alistic"
"iers"
"ibly"
"umpt"
"icalist"
"arisations"
"ages"
"efully"
"inity"
"aristic"
"r_K"
"ably"
"elity"
"istr"
"eousness"
"atory"
"ph"
"entialness"
"oides"
"allically"
"ionate"
"ionality"
"ality"
"ating"
"aric"
"ationally"
r_AA
258
257
"ately"
"ioning"
"rb"
r_T
"atic"
"enced"
"ix"
"acious"
r_I
"izers"
"icist"
"hood"
"dr"
"oidal"
"icality"
r_N
"uct"
"izable"
"istical"
"s'"
"ish"
"istic"
"arial"
"pans"
"r_O"
"r_D"
"ax"
"inate"
"ariser"
"izations"
"r_T"
"ening"
"ial"
"eness"
"arizable"
"inism"
"aical"
"izing"
288
"arization"
"pand"
280
282
"istically"
"icance"
"ially"
"iful"
"acies"
"r_V"
268
"urs"
"ars"
"ators"
"ancies"
"alistically"
"lily"
"arizations"
"eableness"
"vas"
"lux"
"r_Q"
"eature"
"izationally"
118
"ally"
"tex"
"arisation"
"rus"
"atable"
"iously"
"lessly"
"r_AA"
"r_C"
"ized"
"ehood"
"dic"
"lus"
"atively"
"r_X"
"ibility"
"ental"
"yish"
"r_J"
"ists"
"arized"
"entist"
"aically"
"itic"
"istics"
"ableness"
"age"
"entiation"
"aroid"
"olv"
"uc"
"encies"
"entation"
153
"aries"
"ium"
"r_U"
"ened"
"rud"
"erid"
"ically"
"ity"
"elihood"
"entialize"
"icity"
"nesses"
"ously"
"ionalness"
"ioned"
"arizing"
"ane"
"ede"
"eleg"
"hetenes"
"hetene"
"eig"
"slov"
"edes"
"vt"
"hetslov"
"leg"
"lov"
"erte"
"elov"
golab26
replab24
r_Step_5a
r_Step_5b
"\u00E2ncia"
"\u00EAncias"
"\u00EA"
"\u00EDssemos"
"\u00E1sseis"
"a\u00E7a~o"
r_residual_form
"\u00E3"
"\u00EDeis"
"a~"
"\u00EDsseis"
"a\u00E7o~es"
"er\u00EDeis"
"\u00E9sseis"
"\u00E1ssemos"
"\u00E1veis"
"\u00EDvel"
"\u00EDreis"
"\u00E1reis"
"\u00EAncia"
"\u00E1vamos"
"ara~o"
"\u00E1vel"
"ar\u00EDeis"
"\u00E1mos"
"\u00E9reis"
"\u00EAssemos"
"\u00EDramos"
"era~o"
"\u00E9ramos"
"ir\u00EDeis"
"o~"
"ira~o"
"\u00F5"
"ut\u0103"
"icator"
"ur\u0103\u0163i"
"ser\u0103m"
B_standard_suffix_removed
"\u00E2se"
"ivit\u0103\u0163i"
"ului"
"use\u015Fi"
"icit\u0103\u0163i"
"iua"
"iune"
"oas\u0103"
"e\u0163i"
"ant\u0103"
"ir\u0103"
"ic\u0103"
"iai"
"se\u015Fi"
"atoare"
"ea\u0163i"
"ativa"
"\u00E2ser\u0103m"
"i\u015Fi"
"user\u0103m"
"icale"
"ar\u0103\u0163i"
"icitate"
"\u0103\u015Fte"
"ibila"
"iuni"
"ase\u015Fi"
"ser\u0103"
"\u00E2r\u0103\u0163i"
"\u00E2m"
"ise\u015Fi"
"e\u015Fti"
"i\u0163iune"
"iau"
"iciv\u0103"
"at\u0103"
"ir\u0103m"
"icitati"
"aua"
"icala"
"iser\u0103\u0163i"
"it\u0103"
"icivi"
"ise"
r_step_0
"ivitati"
"e\u015Fte"
"\u0103\u015Fti"
"a\u0163iune"
"user\u0103"
"atei"
"sei"
"icatori"
"itori"
"aser\u0103"
"\u0103tor"
"eai"
"ur\u0103m"
"ind"
"aser\u0103m"
"itiv\u0103"
"seser\u0103\u0163i"
"isem"
"asem"
"ivit\u0103i"
"oase"
"a\u0163ie"
"\u00E2ndu"
"seser\u0103"
"icit\u0103i"
"ser\u0103\u0163i"
"abil\u0103"
"icive"
"sese\u015Fi"
"abilitati"
"itati"
"icali"
"ur\u0103"
"eam"
r_combo_suffix
"\u00E2r\u0103"
"iilor"
"\u0103m"
"oasa"
"use"
"ezi"
"ivitate"
"i\u0163i"
"abilit\u0103i"
"a\u0163i"
"itiva"
"seser\u0103m"
"\u00E2ser\u0103"
"a\u0163ia"
259
"ui"
"\u00E2i"
"indu"
"ibil\u0103"
"iciva"
"itor"
"ilor"
"itate"
"\u00E2\u0163i"
"usem"
"iv\u0103"
"ical\u0103"
"ia\u0163i"
"ar\u0103"
"ile"
"\u00E2nd"
"itive"
"\u00E2re"
"sese"
"itoare"
"iile"
"anta"
"\u0163"
"ut"
"user\u0103\u0163i"
"\u0103tori"
"\u00E2sem"
"\u0103toare"
"ist\u0103"
"abilit\u0103\u0163i"
"\u00E2\u015Fi"
"\u0103sc"
"o\u015Fi"
"ibilitate"
"iser\u0103"
"ativi"
"ar\u0103m"
"easc\u0103"
"ir\u0103\u0163i"
"it\u0103\u0163i"
"\u00E2se\u015Fi"
"iciv"
"abilitate"
"aser\u0103\u0163i"
"itivi"
"elor"
"\u00E2ser\u0103\u0163i"
"ativ\u0103"
"\u0103"
"abila"
"ea"
"a\u015Fi"
"i\u015Fti"
"eaz\u0103"
"it\u0103i"
"ibil"
"eze"
"iei"
"\u00E2r\u0103m"
"sesem"
"u\u015Fi"
"itiv"
"iser\u0103m"
"\u044F\u043C\u0438"
"\u044B\u043B\u0430"
"\u043E\u0435"
"\u0435\u043D\u043E"
"\u0430\u043C\u0438"
"\u0443\u0439"
"\u0435\u043D\u044B"
"\u044E\u044E"
"\u0443\u0435\u0442"
"\u044F\u0445"
"\u0435\u043D"
"\u043B\u0430"
1072
"\u0430\u0445"
r_reflexive
"\u0442\u044C"
r_adjectival
"\u0432\u0448\u0438\u0441\u044C"
"\u0438\u044F\u0445"
"\u0435\u043C"
"\u0438\u0435\u043C"
"\u0443"
r_adjective
"\u0435\u0435"
"\u0430"
"\u044B\u0439"
"\u044C\u0435"
"\u0435\u0432"
"\u0438\u0442"
"\u0438\u044F\u043C\u0438"
"\u044F"
"\u043D\u044B"
"\u043E\u044E"
"\u044B\u043B\u043E"
"\u0435\u0439\u0448"
"\u044B\u0442"
"\u0438\u044F"
"\u043E\u043C\u0443"
"\u0438"
"\u044E\u0449"
"\u044E"
"\u0438\u043B\u0438"
"\u0438\u0442\u0435"
"\u044F\u044F"
"\u043E\u0432"
r_perfective_gerund
"\u0438\u044F\u043C"
r_derivational
"\u0438\u0435"
golab1
"\u0449"
"\u0438\u044E"
"\u043D"
"\u044B\u0432"
"\u043E\u0441\u0442"
"\u0435\u0439\u0448\u0435"
"\u043D\u043E"
"\u0432\u0448\u0438"
"\u044B\u043B"
"\u0439"
"\u0430\u044F"
"\u0438\u0432"
"\u0438\u0432\u0448\u0438"
"\u044B\u0435"
r_verb
"\u0441\u044F"
"\u044B"
"\u0438\u043B\u0430"
"\u0432"
"\u043E"
"\u044C\u044F"
"\u043D\u043D"
"\u0435\u0442"
"\u0441\u044C"
"\u044B\u043B\u0438"
"\u044B\u043C\u0438"
"\u044B\u0442\u044C"
"\u0438\u043C\u0438"
"\u0438\u043B\u043E"
"\u043B"
"\u0438\u0432\u0448\u0438\u0441\u044C"
r_tidy_up
"\u0432\u0448"
"\u043E\u0439"
"\u044B\u0432\u0448\u0438\u0441\u044C"
"\u0443\u0439\u0442\u0435"
"\u0438\u0448\u044C"
"\u0435\u0439\u0442\u0435"
"\u0435\u043C\u0443"
"\u0443\u044E\u0449"
"\u044B\u0445"
"\u0438\u043B"
1103
"\u0443\u044E"
"\u044F\u043C"
"\u0435\u044E"
"\u0438\u0445"
r_noun
"\u043E\u043C"
"\u043E\u0441\u0442\u044C"
"\u0435\u043D\u0430"
"\u044C"
"\u043D\u043D\u043E"
"\u044E\u0442"
"\u0438\u0435\u0439"
"\u043D\u0430"
"\u0435\u0433\u043E"
"\u0435\u0439"
"\u0438\u043C"
"\u0435\u0442\u0435"
"\u0438\u0438"
"\u0430\u043C"
"\u043B\u0438"
"\u0443\u044E\u0442"
"\u0435\u0448\u044C"
"\u0439\u0442\u0435"
"\u044B\u0432\u0448"
"\u044B\u0432\u0448\u0438"
"\u043E\u0433\u043E"
"\u043B\u043E"
"\u044C\u044E"
"\u044B\u043C"
"\u0435"
"\u0435\u0438"
"\u0438\u0439"
"\u0438\u0442\u044C"
"\u0438\u0432\u0448"
"\u044F\u0442"
"\u00E1r"
"ir\u00EDais"
"ases"
"abais"
"er\u00EDais"
"yo"
"ierais"
"ar\u00E9is"
"aseis"
"isteis"
"aran"
"\u00E1semos"
96
"er\u00E1n"
"er\u00E9is"
"iendo"
"arais"
"i\u00E9ndo"
"aciones"
"yendo"
"selos"
"i\u00E9ramos"
"iese"
"er\u00EDan"
"yas"
"i\u00F3"
"idad"
"ieran"
"ye"
"ieron"
"selo"
"\u00E9is"
"ar\u00EDais"
"ir\u00E9is"
"amiento"
"\u00E9r"
"ya"
"er\u00EDa"
"ir\u00EDa"
"ir\u00EDas"
"ir\u00E9"
"\u00E1ndo"
"i\u00E9semos"
"ancias"
"las"
"ieseis"
"aban"
"\u00EDas"
"\u00EDr"
"imiento"
r_y_verb_suffix
"aron"
"aba"
"\u00E1bamos"
"er\u00EDas"
"iesen"
"yan"
"iera"
"asteis"
"ar\u00EDa"
"asen"
"er\u00E9"
"ar\u00EDan"
"yais"
"abas"
"\u00EDan"
"aci\u00F3n"
"ar\u00E9"
"sela"
"ancia"
"ar\u00E1n"
"\u00EDa"
"\u00EDais"
"\u00EDs"
"ir\u00EDan"
"ieses"
"ar\u00EDas"
"\u00E1is"
"ieras"
"amientos"
"ibles"
"yeron"
"imientos"
"anzas"
"y\u00F3"
"selas"
"yamos"
"ir\u00E1n"
"heterna"
"arne"
"erna"
"andet"
"orna"
"arnas"
"ornas"
"erns"
"arna"
"l\u00F6s"
"aren"
"ade"
"ades"
"arens"
"fullt"
"anden"
"l\u00F6st"
"ernas"
lab53
lab52
lab51
"m\u00FC\u015F"
r_mark_yUm
"ki"
r_mark_yUz
lab50
r_mark_yken
"tun"
r_mark_suffix_with_optional_s_consonant
golab22
golab24
"s\u0131n"
"m\u0131z"
"n\u00FCz"
r_stem_noun_suffixes
r_mark_ndAn
"ler"
a_18
r_mark_ylA
"m\u00FCz"
r_mark_DA
"s\u00FCn"
"d\u00FCn"
"tim"
golab19
r_mark_nU
r_is_reserved_word
golab10
"tum"
"d\u0131m"
r_mark_suffix_with_optional_n_consonant
r_mark_yDU
"t\u0131"
"s\u00FCn\u00FCz"
"d\u0131"
r_mark_nA
"n\u00FC"
"t\u00FCr"
"\u0131"
B_continue_stemming_noun_suffixes
r_more_than_one_syllable_word
"ken"
"di"
"\u00FCm"
v_23
v_22
v_21
v_27
v_26
v_25
v_24
"sam"
lab44
lab45
lab46
lab47
lab42
lab43
"niz"
"d\u00FCk"
r_mark_yU
"leri"
"\u00FCn"
r_mark_suffix_with_optional_U_vowel
"ca"
"n\u0131z"
r_mark_ki
"\u0131m"
"duk"
"d\u0131n"
lab38
r_mark_sUn
"dur"
"sen"
"cas\u0131na"
"n\u0131"
r_mark_ndA
"t\u00FCm"
r_mark_suffix_with_optional_y_consonant
"dan"
"t\u0131r"
r_mark_DAn
lab40
"\u0131n"
r_post_process_last_consonants
"dun"
r_mark_possessives
r_mark_ymUs_
r_mark_sU
I_strlen
r_stem_suffix_chain_before_ki
"d\u0131k"
g_vowel6
g_vowel5
g_vowel4
g_vowel3
g_vowel2
g_vowel1
"sun"
lab39
"miz"
lab31
lab30
lab33
lab32
lab35
lab34
lab37
lab36
"nuz"
"cesine"
"dik"
r_mark_lArI
"t\u00FCn"
"sak"
"tuk"
r_mark_ysA
"s\u0131n\u0131z"
r_mark_lAr
lab49
"mi\u015F"
"t\u0131m"
"d\u0131r"
r_mark_nUn
a_19
r_mark_cAsInA
r_mark_nUz
a_13
a_12
a_17
a_16
a_15
a_14
"siniz"
"tur"
lab48
"din"
lab41
"d\u00FCr"
"nda"
"t\u00FCk"
"\u00FCz"
"m\u0131\u015F"
r_stem_nominal_verb_suffixes
r_mark_ncA
"muz"
"t\u0131n"
"tik"
"sek"
"lar"
"sunuz"
g_U
lab28
lab29
lab26
305
lab24
"ndan"
"t\u00FC"
"tir"
"\u0131z"
"d\u00FC"
"soyad"
"dim"
g_vowel
"\u011F"
r_mark_sUnUz
"nden"
"mu\u015F"
"dum"
r_mark_yA
r_check_vowel_harmony
a_22
a_23
a_20
a_21
r_append_U_to_stems_ending_with_d_or_g
"t\u0131k"
"d\u00FCm"
"uz"
"lar\u0131"
r_mark_DUr
"tin"
testBasicFeatures
"كبيرة
text."
ساهدهات"
"امريك"
"امريكي"
"كبيرة"
"الكتاب"
"ما
"مشروب"
"كتاب"
ملكت
"كبير"
"أمريكيين"
"مشروبات"
TestArabicAnalyzer
testEnglishInput
testArabicLetterTokenizer
"\ud801\udc1c\u0300test"
"\u0300test"
TestArabicLetterTokenizer
\ud801\udc1c\u0300test"
testArabicLetterTokenizerBWCompat
testFathatan
"ولدٍ"
"بنى"
testAlifHamzaBelow
testKasra
"ولدٌ"
testDamma
"نلسون"
"نلْسون"
testDammatan
"بوات"
"مبنا"
testShaddah
"ولد"
testAlifMaksura
"روبرت"
testSukun
"أحمد"
"ولداً"
"احمد"
testAlifHamzaAbove
testTehMarbuta
"روبرـــــت"
"آجن"
TestArabicNormalizationFilter
"بُوات"
"إعاذ"
testTatweel
testFatha
"بني"
"علِي"
"هتميّ"
testAlifMadda
"علي"
"فاطمه"
"هتمي"
"ولدا"
"فاطمة"
"اجن"
testKasratan
"اعاذ"
"مَبنا"
testAtSuffix
"وساهدون"
"ساهده"
testHSuffix
"ساهدان"
"بالحسن"
"الو"
"والحسن"
testWnSuffix
"اخر"
testNonArabic
"ساهديه"
testAlPrefix
"ساهدية"
"ساهدين"
"ساهدي"
testAhSuffix
"زوجها"
"للاخر"
"الحسن"
TestArabicStemFilter
"كالحسن"
testWaPrefix
testYnSuffix
testYSuffix
"زوج"
testPSuffix
testWalPrefix
testBalPrefix
testAnSuffix
testYpSuffix
testComboPrefSuf
testYhSuffix
"حسن"
testLlPrefix
"ساهدات"
testKalPrefix
testFalPrefix
"ساهدة"
"فالحسن"
testComboSuf
"وحسن"
"ساهدون"
testShouldntStem
testBasicExamples
"казваш"
"Атомната
TestBulgarianAnalyzer
"енергийни
"се"
"Как
"криз"
енергия"
кризи"
"компютър"
"атомн"
"енерг"
"документи"
се
"документ"
казваш?"
"енергийн"
"изключение"
"морето"
"кожусите"
"народите"
"моретата"
"дядовци"
"крак"
"изключени"
"собственикът"
"промян"
"дървета"
"градовете"
"подлози"
"градовц"
"градът"
"строя"
"подлога"
"дърво"
"собственик"
"подлозите"
"брат"
"народе"
"красивата"
"мъжо"
"красива"
"центр"
"красивият"
"изключн"
"подлог"
"собственици"
"брата"
"пътя"
"красивото"
"дървото"
"краката"
"море"
"дървата"
"дяд"
"промяната"
"стр"
"кракът"
"собственика"
"мъж"
"собствениците"
"красивия"
"дядо"
"песн"
"дърв"
"центровете"
"изключението"
"градеца"
"народа"
"пътища"
"народи"
"изключения"
"промените"
"вест"
"подлогът"
"песните"
"народът"
"красиво"
"кожуха"
"вести"
"път"
"центъра"
"града"
"център"
"градецът"
"песен"
"брате"
"строевете"
"промяна"
"кожух"
"песента"
"мъжете"
"вестта"
"градец"
"братът"
"красиви"
"мъжа"
"мор"
"красивите"
"градовце"
"градовцете"
"кожухът"
"мъжът"
"пътищата"
"песни"
"строят"
"дядовците"
"дядото"
"дърветата"
TestBulgarianStemmer
"пътят"
"центрове"
"изключенията"
"мъже"
"вестите"
"братя"
"красив"
"кожуси"
"крака"
"центърът"
"промени"
"морета"
"дърва"
"народ"
"братята"
"bôas"
"quintanilh"
"quinz"
"bóia"
"bobs"
"Brasilia"
"quickly"
"quinta"
"brasília"
"quimico"
"quinhent"
"boas"
"boi"
"quilomb"
"boêmio"
"bobinho"
"boatos"
"bobeir"
"boa"
"quilates"
"quilinhos"
"quimio5terapicos"
"bogotá"
"boc"
"boates"
"boeing"
"boat"
"quimon"
"quinzen"
"boataria"
"quinze"
"boate"
"quinzena"
"quicaram"
"bocal"
"boca"
"boçal"
"boainain"
Brasilia"
"quintan"
"bobear"
"quimioterap"
"bocado"
"bobeira"
"boemia"
"quilômetros"
"quimic"
"quilombo"
"quilate"
"quintessente"
"bocaiuv"
"quietos"
"bocarr"
"quintal"
"áá"
"quinha"
"bodoque"
"quimioterápicos"
"bobinhos"
"bobo"
"quilometr"
"quintos"
"quiosqu"
"quilat"
"quilinh"
"bobalhões"
"bocarra"
"bobalho"
"bocaiúva"
"quintão"
"quiet"
"bod"
"quino"
testStemExclusionTableBWCompat
"quimono"
"quimicas"
"bocadas"
checkReuse
"bobag"
"bocadinho"
"boiando"
testStemExclusionTable
"quic"
"quiosque"
"quin"
"quintuplic"
"brasilia"
"bocadinh"
"quinhentos"
"boem"
"bobagem"
"bodoqu"
"quimicos"
"boassu"
"quintuplicou"
"quinn"
"quintino"
"quinhão"
"quilo"
"bogot"
"quintessência"
"quintana"
"quilômetro"
"bode"
"boba"
"quimioterapia"
"quimica"
"quimio5terápicos"
"bobinh"
"quieto"
"ááá"
"quiabo"
"quinc"
"Brasil"
"Brasília
"bocas"
"quiab"
"bobagens"
"quintin"
"quintanilha"
"quilos"
"quinto"
"quil"
"boemi"
"quint"
TestBrazilianStemmer
"quincas"
"\u4e5d\u5341"
"\u3051\u3053"
opqrstu
"\u4e01\u4e02"
vwxy
"بر"
"ير"
"وب"
\u4e94\u516d\u4e03\u516b\u4e5d
"\u4e8c\u4e09"
"vwxy"
"\u4e00\u4e01"
\u5341"
testMix2
testFinalOffset
"\u3046\u3048"
\u4e8c\u4e09\u56db
checkCJKToken
"\u4e09\u56db"
testJa2
testJa1
ijklmn
"\u3093"
testC
out_tokens
"\u4e03\u516b"
"رو"
"ijklmn"
"\u4e94\u516d"
"\u4e00\u4e01\u4e02"
\u3053"
"opqrstu"
testNonIdeographic
testNonIdeographicNonLetter
"مو"
"あい"
"testあい
"\u304b\u304d"
روبرت
out_tokens2
"\u4e00\u4e8c"
defgh
"あいtest"
"وي"
"\u5341"
testSingleChar
"\u516b\u4e5d"
"\u3042\u3044\u3046\u3048\u304aabc\u304b\u304d\u304f\u3051\u3053"
"defgh"
TestCJKTokenizer
"Ｔｅｓｔ
"\u516d\u4e03"
"\u3053"
"\u3044\u3046"
"\u56db\u4e94"
"\u4e00"
"\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341"
checkCJKTokenReusable
"\u3042\u3044"
موير"
"رت"
"\u3048\u304a"
testMix
"\u4e00
رُوبرت
１２３４"
"\u304d\u304f"
"\u3042\u3044\u3046\u3048\u304aab\u3093c\u304b\u304d\u304f\u3051
"\u304f\u3051"
"あい
chinese
"共"
correctStartOffset
"京"
TestChineseTokenizer
testOtherLetterOffset
correctEndOffset
"民"
"中华人民共和国"
"北"
"市"
"华"
"Test."
"中1234"
Test.
JustChineseTokenizerAnalyzer
"北京市"
JustChineseFilterAnalyzer
"a天b"
justFilter
justTokenizer
"lidt"
"Rind"
"Fiols"
"veninde"
testHyphenationCompoundWordsDA
Bilmotor
"Pelar"
"motor"
"Bilmotor"
"basketballkurv"
Glasögonfodral
"fiolsfodral"
"basketball"
"Motor"
Basfiolsfodralmakaregesäll
"Aufgabe"
"fiol"
"Fiolsfodral"
Basfiolsfodral
"Rute"
"Sko"
"Pelarborr"
"Torkare"
"kurv"
"Fleisch"
"Glasögonfodral"
"abba"
"Bildörr"
testDumbCompoundWordsSE
"Rindfleischüberwachungsgesetz"
"læse"
"Vindrutetorkarblad"
testDumbCompoundWordsSELongestMatch
getHyphenationReader
"Glas"
Skomakare
"Tak"
"borr"
testHyphenationCompoundWordsDELongestMatch
Vindrutetorkare
"Skomakare"
"Hammar"
"ögon"
"Hammarborr"
"Borr"
"hest"
"Draht"
"Basfiolsfodralmakaregesäll"
læsehest"
Biltak
"Slag"
som
Pelarborr
"læsehest"
veninde
"Ögon"
"Gesetz"
104
"da_UTF8.xml"
"Slagborr"
"Gesäll"
"blad"
"dörr"
abba"
"torkare"
"gesäll"
"Blad"
"Fiol"
"basket"
Vindrutetorkarblad
Slagborr
"af"
151
"Fodral"
"Vindrutetorkare"
"Bil"
"Dörr"
TestCompoundWordTokenFilter
"rute"
"Basfiolsfodral"
"Biltak"
"Schere"
155
er
"Bildörr
"makare"
lidt
"Vind"
"fodral"
"som"
"Makare"
Hammarborr
"Bas"
"Überwachung"
"Pokud
"mluvim"
testStopWordLegacy
testStopWord
"volnem"
testInvalidStopWordFile
"republika"
"Česká
mluvime
"mluvime"
volnem"
"republik"
testStopWordFileReuse
"voln"
Republika"
testReusableTokenStreamLegacy
UnreliableInputStream
TestCzechAnalyzer
"customStopWordFile.txt"
"česká"
"mladí"
"mužem"
"hrady"
"písním"
"předsedu"
"mořem"
"mazat"
"předsedů"
"hradů"
"klucích"
"kuřata"
"hezk"
"Karlův"
"jarního"
"předsd"
"růžemi"
"hrad"
"hůl"
"soudcích"
"předsedou"
"muž"
"jarnímu"
"kuřeti"
"žena"
"strojem"
"mladými"
"mladým"
"předsedovi"
"pánech"
"mužích"
"stroji"
"ženy"
"stroj"
"knih"
"žen"
"mladých"
"kostem"
"hradem"
"město"
"muži"
"písních"
"písn"
"jarním"
"mladý"
"hezčí"
"pán"
"pánové"
"kuř"
"desek"
"ženám"
"ženou"
"pány"
"kuře"
"moři"
"mladou"
"mužům"
"soudce"
"žn"
"písň"
"předsedové"
"staveními"
"růžích"
"mah"
"předsedech"
"jarních"
"karl"
"kostí"
"strojům"
"písněmi"
"pána"
"předseda"
"růží"
"soudcům"
"deska"
"předsedům"
"ženo"
"jazyk"
"hradech"
"růže"
"páni"
"mořích"
"mořím"
"pánů"
"knize"
"předsedy"
"růh"
"ženami"
"strojích"
TestCzechStemmer
"čeští"
"soudk"
"hradu"
"hradům"
"pánům"
"městy"
"soudci"
"mladá"
"mladém"
"městu"
"český"
"mladé"
testDontStem
"staveních"
"kost"
"písně"
"kostech"
"kluk"
"hrade"
"kniha"
"stavení"
"mažu"
"kuřat"
"kuřaty"
"pánem"
"ženách"
testPossessive
"městě"
"růžím"
"muže"
"mladému"
"jarn"
"soudcem"
"kuřete"
"stavením"
"městem"
"jarními"
"stavn"
"kuřatům"
"měst"
"anglický"
"moře"
"moř"
"moří"
"pánovi"
"strojů"
"mlad"
"jazykový"
"předsedo"
"městům"
"hezký"
"ženu"
"stroje"
"kluci"
"kosti"
"kuřatech"
"hol"
"mladého"
"páne"
"kostmi"
"písni"
"ženě"
"muh"
"píseň"
"zi"
"mužů"
"městech"
"města"
"jarní"
"růži"
"soudců"
"kuřetem"
"undersøg"
"undersøgelse"
TestDanishAnalyzer
"på"
"Schaltflächen"
"tischen"
"Fischen
"schaltflach"
TestGermanAnalyzer
testGermanSpecials
"trink"
"Tische"
"fischen"
testExclusionTableBWCompat
"trinken"
Trinken"
"Schaltflaechen"
"schaltflaech"
"Tisch"
"iso-8859-1"
TestGermanStemFilter
"data.txt"
breader
\u03bc\u03b5\u03c3\u03c4\u03cc\u03c2
"\u03bc\u03b5\u03c3\u03c4\u03bf\u03c3"
\u03c3\u03b5\u03b9\u03c1\u03ac
\u03ac\u03bb\u03bb\u03bf\u03b9"
"\u03b1\u03bb\u03bb\u03bf\u03b9"
\u0386\u03c8\u03bf\u03b3\u03bf\u03c2,
\u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03ae\u03c1\u03c9\u03bd
"\u03ba\u03b1\u03bb\u03b7"
\u03bf
"\u039c\u03af\u03b1
"\u03a0\u03a1\u039f\u03ab\u03a0\u039f\u0398\u0395\u03a3\u0395\u0399\u03a3
"\u03a0\u03c1\u03bf\u03ca\u03cc\u03bd\u03c4\u03b1
\u0391\u039d\u0391\u0393\u039a\u0395\u03a3"
testAcronymBWCompat
testAcronym
\u03b3\u03bb\u03ce\u03c3\u03c3\u03b1\u03c2"
"\u03c3\u03b5\u03b9\u03c1\u03b1"
"\u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03c9\u03bd"
"απτ"
\u03ba\u03b1\u03b9
"\u03b3\u03bb\u03c9\u03c3\u03c3\u03b1\u03c3"
"\u03b5\u03be\u03b1\u03b9\u03c1\u03b5\u03c4\u03b9\u03ba\u03b1"
"\u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03b7\u03c3"
"\u03c0\u03c1\u03bf\u03c5\u03c0\u03bf\u03b8\u03b5\u03c3\u03b5\u03b9\u03c3"
GreekAnalyzerTest
"\u03c0\u03c1\u03bf\u03b9\u03bf\u03bd\u03c4\u03b1"
"\u03bc\u03b9\u03b1"
\u03b5\u03be\u03b1\u03b9\u03c1\u03b5\u03c4\u03b9\u03ba\u03ac
\u03c4\u03b7\u03c2
"\u03c0\u03bf\u03bb\u03bb\u03b1\u03c0\u03bb\u03b5\u03c3"
(\u03ba\u03b1\u03b9)
"\u03b1\u03c8\u03bf\u03b3\u03bf\u03c3"
"Α.Π.Τ."
"\u03c0\u03bb\u03bf\u03c5\u03c3\u03b9\u03b1"
\u03bf\u03b9
\u03c0\u03bb\u03bf\u03cd\u03c3\u03b9\u03b1
[\u03c0\u03bf\u03bb\u03bb\u03b1\u03c0\u03bb\u03ad\u03c2]
"α.π.τ."
\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ae\u03c2
\u03ba\u03b1\u03bb\u03ae
"\u03b1\u03bd\u03b1\u03b3\u03ba\u03b5\u03c3"
TestEnglishAnalyzer
"chicano"
TestSpanishAnalyzer
"chican"
"chicana"
خواهد
testBehaviorVerbsDefective
شده‌است"
می‌خورد"
testBehaviorNonPersian
"بخورد"
ها"
"برگ‌ها"
"خورد"
شود"
می‌شد"
خورد"
خورده
شد"
"برگ
"خواهد
"خورده"
"خورده
بود"
مي
می‌شده‌است"
testBehaviorVerbs
testBehaviorNouns
باشد"
بوده
می‌شده
"خورده‌است"
"مي
شده
"می‌خورده
"برگ"
"داشت
"می‌خورد"
"می‌خورده‌است"
"دارد
می‌شود"
TestPersianAnalyzer
است"
"زاده"
"زادہ"
"هاے"
"كتابۀ"
testKeheh
"کشاندن"
testFarsiYeh
testHehYeh
testHehGoal
testYehBarree
"كتابه"
"كشاندن"
testHehHamzaAbove
"كتابهٔ"
TestPersianNormalizationFilter
TestFinnishAnalyzer
"edeltäj"
"olla"
"edeltäjiinsä"
"edeltäjistään"
TestElision
tas
pour
"enfin"
"Plop,
"O'brian"
voir
M'enfin."
juste
"embrouille"
avec
l'embrouille
O'brian.
"aujourd'hui"
éléments
ïâöûàä
cheval"
la
"lanc"
"françois"
captifs"
"souffr"
"C3PO
du
chiste"
"lances
"chien
"oeuf"
"chien"
"chien++"
testAnalyzer30
"votr"
"habitable
anticonstitutionnellement
"anticonstitutionnel"
les
"1940-1945"
CHEVAL"
"rug"
\"entreguillemet\""
"jean"
"élément"
"jav"
"captif"
CHAT
chiste
"mot
(---i+++)*"
l'embrouille"
"1945"
"chat"
"cheval"
chismes
rugissante"
1940:1945
"Jean-François"
"chist"
TestFrenchAnalyzer
des
"chism"
"voir"
"le
"33bis"
"ïâöûàä"
"1940"
chat
habitable
aujourd'hui
"33Bis
"finissions
souffrirent
,?
1940-1945
oeuf
"voir
Java++
"embrouill"
"entreguillemet"
/:
chien
"Votre"
"हिंद"
testExclusionSet
TestHindiAnalyzer
TestHindiNormalizer
"शार्‌मा"
"आईऊॠॡऐऔीूॄॣैौ"
"अंगरेजी"
"फ़र्ज़"
"अँगरेजी"
"अइउऋऌएओिुृॢेो"
"ेेोोएएओओअ"
"रळखगडढय"
"अँग्रेज़ी"
"अंगरेज़ी"
"अँगरेज़ी"
"फरज"
"अंग्रेज़ी"
"ऱऴख़ग़ड़ढ़य़"
"ॅॆॉॊऍऎऑऒ\u0972"
"करज"
"अंगरेजि"
"अंग्रेजी"
"क़र्ज़"
"शारमा"
testDecompositions
"अँग्रेजी"
"खाता"
"गुरुओं"
"खाना"
"गुर"
"आध्यापीकाएं"
"लडके"
"आध्यापीकाओं"
"लडकियों"
TestHindiStemmer
"किताबों"
"खा"
"आध्यापीक"
testVerbs
"लडका"
"गुरु"
"दोस्त"
"दोस्तों"
"खाती"
"लडक"
"लडकों"
"लडकी"
"कठिन"
"आध्यापीका"
"कठिनाइयां"
"babakocsi"
TestHungarianAnalyzer
"babakocs"
"babakocsijáért"
"által"
"अाैअाै"
"ऑऑ"
"अाॆअाॆ"
"अाअा"
"आआ"
"औऔ"
"अाॅअाॅ"
"ऒऒ"
"ত্‍"
TestIndicNormalizer
"ओओ"
"अाेअाे"
"अाैर"
testFormat
TestIndicTokenizer
"शार्‍मा
शार्‍मा"
"abbandonati"
"abbandon"
TestItalianAnalyzer
"dallo"
"abbandonata"
"dc."
largeWord
"(56.78)"
"abcd1234"
dc."
"Here,Are,some,Comma,separated,words,"
"separated"
testWhitespacePattern
"Comma"
"fox,the"
testCustomPattern
"Fox,the"
"comma"
Fox,the
testHugeDocument
largeWord2
(56.78)
testNonWordPattern
"Are"
abcd1234
PatternAnalyzerTest
TestEmptyTokenStream
TestPrefixAndSuffixAwareTokenFilter
TestPrefixAwareTokenFilter
TestSingleTokenTokenFilter
"someType"
"booked"
testOverride
TestStemmerOverrideFilter
EdgeNGramTokenFilterTest
EdgeNGramTokenizerTest
NGramTokenFilterTest
NGramTokenizerTest
"ophalen"
"lichamen"
"ophoogzand"
"lichtbruin"
"ophal"
"opgroei"
"ophaaltruck"
"lidvereniging"
"ophouden"
"lichtdoorlat"
"ophoping"
"opglanzing"
"lichthoeveelheid"
"ophog"
"opglimping"
"lichtverontreinigd"
"lichtverontreinigde"
"lichtten"
"ophiev"
"opgroeiplaats"
"lichtgewicht"
"ophaalkosten"
"lichtkrant"
"lidveren"
"opheldering"
"ophoepelt"
"opheffen"
"lichtgevoel"
"opgrijz"
"lichtende"
"opgrijnzen"
"lidia"
"opglimlacht"
"lichtstromende"
"lichamelijke"
"lidstaten"
"opglimpen"
"opglanzingen"
"ophaler"
"zelf"
"opgrijnz"
"opglimpingen"
"opging"
"ophemelde"
"licht"
"ophaalt"
"lichter"
testSnowballCorrectness
"ophief"
"customStemDict.txt"
"opglanz"
"opgraven"
"ophieven"
"ophaalkost"
"lidstat"
"lichttoetreding"
"opgroeiende"
"lichtdoorlatende"
"ophaalsystem"
"lichtste"
"ophalers"
"lichamelijk"
"lichtstrom"
"lichttoetred"
"licher"
"lichaamsziek"
"lichters"
"opheff"
"ophaalsystemen"
"lichtintensiteit"
"lichtregelsystemen"
"ophopen"
testOldBuggyStemmer
"lichere"
"lichamelijk
lichamelijke"
"opgrav"
"opglimlachten"
"ophalend"
"lichtzinn"
"lichtkranten"
"lidmaatschap"
"opgrijzende"
"opglimpende"
"ophelder"
"opglimp"
"opheffing"
"lichtkringen"
"lichtgrijs"
"lichtkring"
"ophop"
"lichtj"
"ophemeld"
"lichtenvoorde"
"lichtjes"
"lichtzinnige"
"lichtregelsystem"
"ophoog"
"lichtst"
"opgingen"
"lichten"
"lichte"
testStemDictionaryReuse
"ophemel"
"lichtje"
"lichtbeeld"
"opgroeiplat"
customDictFile
"ophield"
"lichtenvoord"
"lichthoevel"
"ophaal"
"lichtte"
"ophemelen"
"lichtgevoeligheid"
"ophef"
"lichtend"
"Zelf"
"ophoud"
TestDutchStemmer
"lichtere"
"opheusden"
"ophaaldienst"
"opheffende"
"opgroeien"
"opheusd"
"somethingentirelydifferent"
"havnedistriktene"
TestNorwegianAnalyzer
"det"
"havnedistrikt"
"havnedistrikter"
jumped|VB
testIntEncoding
lazy|JJ
99.3f
"expectPay
dogs|NN"
jumped|0.5
red|2.0
"JJ"
lazy|5
fox|3
quick|1
brown|JJ
fox|NN
quick|1.0
dogs|83"
DelimitedPayloadTokenFilterTest
brown|99
brown|99.3
quick|JJ
dogs|83.7"
expectPay
"NN"
red|JJ
fox|3.5
testFloatEncoding
83.7f
red|2
"VB"
"payloadAtt.getPayload()
NumericPayloadTokenFilterTest
TokenOffsetPayloadTokenFilterTest
"pay
"nextToken.getPayload()
TypeAsPayloadTokenFilterTest
TEST_TOKEN_POSITION_INCREMENTS
testNonZeroPositionIncrement
SIX_GRAM_NO_POSITIONS_TOKENS
TEST_TOKEN_NON_ZERO_POSITION_INCREMENTS
PositionFilterTest
SIX_GRAM_NO_POSITIONS_TYPES
SIX_GRAM_NO_POSITIONS_INCREMENTS
test6GramFilterNoPositions
TestPortugueseAnalyzer
"quilométr"
"não"
"repetitiveField"
"variedField:boring"
boring"
testAddStopWordsIndexReaderStringFloat
"repetitiveField:boring"
"variedField"
"vaguelyboring"
affect
repetitiveFieldValue
identified
2f
boring
numHits2
numHits1
repetitiveFieldValues
variedFieldValues
QueryAutoStopWordAnalyzerTest
quarter
variedFieldValue
testNoFieldNamePollution
repetitiveField:boring"
testAddStopWordsIndexReaderInt
protectedAnalyzer
"boring"
appAnalyzer
queris
another"
testWrappingNonReusableAnalyzer
vaguelyBoring
testDefaultAddStopWordsIndexReader
testUninitializedAnalyzer
testAddStopWordsIndexReaderStringInt
"repetitiveField:vaguelyboring"
numNewStopWords
"variedField:quick
4f
"ABEDCF"
"\u0001oD"
"\u0001evah"
"ABC"
"\u0001ecin"
"abcd𩬅艱鍟䇹愯瀛"
"abcfed𩬅愯瀛"
"abc𩬅艱鍟䇹愯瀛"
"\uDF05\uD866\uDF05\uD866"
"BA"
"瀛愯䇹鍟艱𩬅a"
TestReverseStringFilter
testFilterWithMark
"yad"
"fedcba𩬅z"
"𩬅𩬅"
"\u0001a"
"z𩬅abcdef"
"\u0001yad"
"Do
"abc瀛愯𩬅def"
"fedcba𩬅"
testReverseChar
testBackCompat
"abcd𩬅瀛愯䇹鍟艱"
testReverseSupplementary
"瀛愯䇹鍟艱𩬅"
"abc艱鍟䇹愯瀛𩬅d"
"gfe𩬅dcba"
"abcd𩬅efg"
"abc艱鍟䇹愯瀛𩬅"
testReverseString
"evah"
"𩬅abcdef"
"ecin"
"oD"
nice
"abc瀛愯䇹鍟艱𩬅"
testReverseSupplementaryChar
"abc𩬅瀛愯䇹鍟艱"
"a𩬅艱鍟䇹愯瀛"
"abc瀛愯䇹鍟艱𩬅d"
"CBA"
"𩬅艱鍟䇹愯瀛"
TestRomanianAnalyzer
"absenţ"
"absenţa"
"îl"
"absenţi"
энергии
"RussianAnalyzer's
nextSampleToken
еще"
"имел"
sampleUnicode
ra
в
представление
"resUTF8.htm"
"хран"
имели
testUnicode30
"тайн"
"эт"
skips
TestRussianAnalyzer
testDigitsInRussianCharset
"Unicode"
"знан"
хранилось
"testUTF8.txt"
знание
"представление"
тайне"
"Но
testReusableTokenStream30
sample
"энерг"
это
sampleText
"представлен"
testRussianLetterTokenizerBWCompat
TestRussianLetterTokenizer
testRussianLetterTokenizer
Вместе
stems
TestRussianStem
"unicode"
testStem
"wordsUTF8.txt"
inStems
"stemsUTF8.txt"
realStem
setUpSearcher
"me
testShingleAnalyzerWrapperPhraseQueryParsing
testNullTokenSeparator
compareRanks
sentence\""
me"
queryParsingTest
testShingleAnalyzerWrapperPhraseQueryParsingFails
testWrappedAnalyzerDoesNotReuse
testAltTokenSeparator
testShingleAnalyzerWrapperRequiredQueryParsing
"divideinto"
"+test
"divide<SEP>into"
testShingleAnalyzerWrapperQueryParsing
testNonDefaultMinAndSameMaxShingleSize
"\"test
testNonDefaultMinShingleSize
testShingleAnalyzerWrapperBooleanQuery
ShingleAnalyzerWrapperTest
testShingleAnalyzerWrapperPhraseQuery
testNoTokenSeparator
"shingles."
+sentence"
ranks
shingles."
testBiGramFilterWithSingleToken
FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS
shingleFilterTestCommon
"dividethis"
testTriGramFilterWithoutUnigrams
testBiGramFilterWithHolesWithoutUnigrams
testFourGramFilter
TRI_GRAM_TYPES_NO_SEPARATOR
BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS
"sentenceintoshingles"
testFourGramFilterWithoutUnigramsMinFourGram
"sentenceinto"
testFourGramFilterMinFourGram
"divide<SEP>this"
positionIncrements
tokensToShingle
testBiGramFilterWithSingleTokenWithoutUnigrams
FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM
testBiGramFilterWithoutUnigrams
TRI_GRAM_TOKENS_MIN_TRI_GRAM
TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_NO_SEPARATOR
"thissentence"
FOUR_GRAM_POSITION_INCREMENTS
"sentence<SEP>into<SEP>shingles"
TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS
TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_ALT_SEPARATOR
testBiGramFilterWithHoles
TRI_GRAM_TOKENS_NO_SEPARATOR
testTriGramFilterMinTriGram
testBiGramFilterAltSeparator
TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_ALT_SEPARATOR
TRI_GRAM_TOKENS_NULL_SEPARATOR
TRI_GRAM_TOKENS
testTriGramFilterNoSeparator
TRI_GRAM_TYPES
_"
"thissentenceinto"
TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_TRI_GRAM
testTokenWithHoles
FOUR_GRAM_TYPES_MIN_FOUR_GRAM
BI_GRAM_TOKENS_WITHOUT_UNIGRAMS_ALT_SEPARATOR
TEST_SINGLE_TOKEN
SINGLE_TOKEN_TYPES
FOUR_GRAM_TOKENS
ShingleFilterTest
"divide<SEP>this<SEP>sentence"
testFourGramFilterMinTriGram
EMPTY_TOKEN_ARRAY
BI_GRAM_TOKENS_WITHOUT_UNIGRAMS
FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM
BI_GRAM_TOKENS_WITH_HOLES
FOUR_GRAM_TYPES_MIN_TRI_GRAM
shingleFilterTest
BI_GRAM_POSITION_INCREMENTS
BI_GRAM_TYPES_WITHOUT_UNIGRAMS
testBiGramFilterNoSeparator
testBiGramFilterWithEmptyTokenStream
FOUR_GRAM_POSITION_INCREMENTS_MIN_TRI_GRAM
BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_ALT_SEPARATOR
"sentence<SEP>into"
TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_NO_SEPARATOR
BI_GRAM_POSITION_INCREMENTS_WITH_HOLES_WITHOUT_UNIGRAMS
testBiGramFilterWithoutUnigramsNoSeparator
TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM
TRI_GRAM_POSITION_INCREMENTS
BI_GRAM_TOKENS_WITH_HOLES_WITHOUT_UNIGRAMS
EMPTY_TOKEN_INCREMENTS_ARRAY
FOUR_GRAM_TYPES
BI_GRAM_POSITION_INCREMENTS_ALT_SEPARATOR
BI_GRAM_POSITION_INCREMENTS_NO_SEPARATOR
BI_GRAM_TOKENS_NO_SEPARATOR
testTriGramFilterNullSeparator
BI_GRAM_TOKENS
TRI_GRAM_TYPES_MIN_TRI_GRAM
SINGLE_TOKEN
testTriGramFilterWithoutUnigramsNoSeparator
testTriGramFilterWithoutUnigramsMinTriGram
FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_TRI_GRAM
"this<SEP>sentence"
BI_GRAM_POSITION_INCREMENTS_WITH_HOLES
TRI_GRAM_TYPES_NULL_SEPARATOR
BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_NO_SEPARATOR
BI_GRAM_TOKENS_ALT_SEPARATOR
TRI_GRAM_POSITION_INCREMENTS_NO_SEPARATOR
FOUR_GRAM_TOKENS_MIN_FOUR_GRAM
TRI_GRAM_POSITION_INCREMENTS_MIN_TRI_GRAM
"please<SEP>divide<SEP>this"
BI_GRAM_TYPES_WITHOUT_UNIGRAMS_NO_SEPARATOR
TRI_GRAM_TOKENS_ALT_SEPARATOR
testBiGramFilterWithoutUnigramsAltSeparator
TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_ALT_SEPARATOR
"dividethissentence"
SINGLE_TOKEN_INCREMENTS
TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM
testFourGramFilterWithoutUnigrams
testBiGramFilterWithEmptyTokenStreamWithoutUnigrams
TRI_GRAM_TYPES_WITHOUT_UNIGRAMS
testTriGramFilter
FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS
FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS
"_
"this<SEP>sentence<SEP>into"
TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS
BI_GRAM_TYPES_NO_SEPARATOR
BI_GRAM_TYPES
testBiGramFilter
tokensToCompare
"pleasedividethis"
FOUR_GRAM_POSITION_INCREMENTS_MIN_FOUR_GRAM
EMPTY_TOKEN_TYPES_ARRAY
TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_NO_SEPARATOR
BI_GRAM_TYPES_ALT_SEPARATOR
FOUR_GRAM_TOKENS_MIN_TRI_GRAM
BI_GRAM_TOKENS_WITHOUT_UNIGRAMS_NO_SEPARATOR
testTriGramFilterWithoutUnigramsAltSeparator
TRI_GRAM_TYPES_ALT_SEPARATOR
FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM
FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM
TRI_GRAM_POSITION_INCREMENTS_ALT_SEPARATOR
testTriGramFilterAltSeparator
BI_GRAM_TYPES_WITHOUT_UNIGRAMS_ALT_SEPARATOR
TRI_GRAM_POSITION_INCREMENTS_NULL_SEPARATOR
FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM
testFourGramFilterWithoutUnigramsMinTriGram
"surprise_to"
"england_manager_sven"
"helloearth"
"sven"
"greetingsworld"
"^_greetings"
"manager_svennis_in_the"
"tellus_$"
"^_hello_world_$"
"hello_earth_$"
"^_hello_world"
"hello_earth"
"eriksson_in_the"
"hello_tellus_$"
testMatrix
smf
assertNext
7.071068f
"^_hello_earth"
"^_hello"
"manager_svennis"
"^_greetings_tellus_$"
10.099504f
"see_england"
wst
1.4142135f
"göran_eriksson_in_the"
TestShingleMatrixFilter
"greetingstellus"
"no_surprise_to"
"england"
"^_hello_tellus"
"^_hello_earth_$"
"sven_göran_eriksson"
"eriksson_in"
"and_salutations"
positioner
"surprise_to_see_england"
"earth"
"greetings_tellus"
"no_surprise_to_see"
"tellus"
"göran_eriksson"
"in_the_croud"
"earth_$"
"to_see_england_manager"
12.328828f
"svennis"
10.049875f
"sven_göran_eriksson_in"
"hello_world_$"
"^_greetings_tellus"
"and_salutations_tellus"
"salutations"
"to_see"
tls
"greetingsearth"
"greetings_tellus_$"
"greetings"
"surprise_to_see"
"^_greetings_earth_$"
"greetings_earth"
"world_$"
"see_england_manager"
"manager"
"svennis_in_the_croud"
"greetings_world_$"
"^_greetings_earth"
"and_salutations_earth"
"^_greetings_world_$"
testBehavingAsShingleFilter
"manager_sven_göran_eriksson"
1.7320508f
"sven_göran"
"in_the"
"see"
"^_greetings_world"
7.1414285f
"salutations_tellus"
"greetings_and"
"greetings_and_salutations"
"manager_svennis_in"
"greetings_earth_$"
"göran_eriksson_in"
"svennis_in"
tokenFactory
"manager_sven"
"the_croud"
"manager_sven_göran"
"see_england_manager_svennis"
"eriksson"
"svennis_in_the"
"^_hello_tellus_$"
"see_england_manager_sven"
"surprise"
"salutations_world"
"hello_tellus"
"to_see_england"
7.2111025f
"eriksson_in_the_croud"
"göran"
"hello_world"
TokenListStream
"greetings_world"
"croud"
"england_manager_sven_göran"
"england_manager_svennis"
"england_manager"
"england_manager_svennis_in"
"hellotellus"
"no_surprise"
"salutations_earth"
"and_salutations_world"
7/12/2006"
7/11/2006
DateRecognizerSinkTokenizerTest
reacted
TokenRangeSinkTokenizerTest
"rangeToks
rangeToks
ttf
TokenTypeSinkTokenizerTest
testEnglishLowerCase
testFilterTokens
abhorred
"he
"ağaci"
"accent"
"wrd"
"abhor"
testTurkishBWComp
him"
"cryogenic"
testTurkish
"she
TestSnowball
"accents"
"cryogen"
accents"
"CRYOGENIC"
"danish"
"Romanian"
"Kp"
"french"
"hungarian"
"French"
"dutch"
language:
"Norwegian"
"Russian"
"Dutch"
"kraaij_pohlmann"
"Swedish"
"turkish"
"german2"
"norwegian"
"German"
"italian"
"TestSnowballVocabData.zip"
"Portuguese"
"swedish"
snowballLanguage
"Danish"
testStemmers
"german"
"spanish"
"russian"
"romanian"
"Porter"
assertCorrectOutput
"Hungarian"
"/output.txt"
"/voc.txt"
"portuguese"
TestSnowballVocab
"porter"
"Spanish"
"German2"
"jaktkarlarne"
TestSwedishAnalyzer
"jaktkarlens"
"jaktkarl"
"och"
testBuggyTokenType
"กับ"
"ไทมส์"
"ชื่อ"
"ประโยคว่า
"เดอะนิวยอร์กไทมส์
"xyz@demo.com"
"เด"
"ร์ก"
TestThaiAnalyzer
"บริษัท"
คุยกับ
"ยอ"
ประโยคว่า"
XY&Z
"ประโยค"
๑๒๓"
xyz@demo.com"
"คุย"
"xy&z"
"ประโยคว่าtheประโยคว่า"
"เดอะนิวยอร์กไทมส์"
"๑๒๓"
"อะนิว"
"บริษัทชื่อ
TestTurkishAnalyzer
"dolayı"
"\u0049\u0316\u0307STANBUL
"istanbul"
I\u0316SPARTA"
testTurkishLowerCaseFilter
\u0049\u0307ZM\u0049\u0307R
"izmir"
"\u0131\u0316sparta"
testDecomposed2
"\u0130STANBUL
"\u0131sparta"
\u0130ZM\u0130R
"i\u0316stanbul"
testDecomposed
"\u0049\u0307STANBUL
ISPARTA"
TestTurkishLowerCaseFilter
application!"
Can
manual
unpredictable
refer
"analysis.data.dir"
"analysis.properties"
cadidateFiles
dictionaries."
directory!"
getAnalysisDataDir
"analysis-data"
tokenEnd
atBegin
PUNCTION
ci
pch
"。，！？；,!?;"
useDefaultStopWords
loadDefaultStopWordSet
STOPWORD_FILE_COMMENT
0x9FA5
shortArray
rstartIndex
li
0xFE30
0x007A
0xFF3A
"未##数"
0xFF5A
0x0061
0xFF41
ri
2079997
shortIndex
0xFF21
0x0021
0x00BB
lstartIndex
longArray
"始##始"
0x4E00
larray
0x0030
0x0039
"末##末"
rarray
0x2010
0xFF63
"未##串"
longIndex
80000
0x301E
0x2642
　\t\r\n"
0xFF19
0x3001
convertSegToken
hhmmSegmenter
sentenceStartOffset
tokenFilter
nextWord
wordSegmenter
tokenIter
tokenBuffer
cc1
5381
0xcbf29ce484222325L
1099511628211L
1410
6768
0x0FF
cc2
cchar
ccid
0x00FF
loadFromFile
loadFromInputStream
currentStr
"/bigramdict.dct"
"bigramdict.mem"
402137
bigramDictPath
PRIME_BIGRAM_LENGTH
getBigramItemIndex
bigramHashTable
hashId
dictRoot
getAvaliableIndex
frequencyTable
"/bigramdict.mem"
lastNode
nodeCount
bigramDict
wordPairFreq
resultPath
isToExist
tokenPairListTable
getToList
nextTokens
segList
minWeight
oneWordFreq
getToCount
idInteger
minEdge
tinyDouble
generateBiSegGraph
idBuffer
addSegTokenPair
currentPathNode
smooth
newNode
tokenPair
rpath
zeroPath
edges
tokenPairList
getCharTypes
wordDict
charTypeArray
wordBuf
biSegGraph
hasFullWidth
shortPath
createSegGraph
foundIndex
pn
toTokenList
tokenListTable
0x0020
0xFEE0
tempFreq
expandDelimiterData
getAvaliableTableIndex
"/coredict.mem"
12071
sortEachItems
tempArray
setTableIndex
charIndexTable
loadFromObjectInputStream
dctFileRoot
wordItem_frequencyTable
mergeSameWords
"coredict.mem"
knownStart
wordItem_charArrayTable
getWordItemTableIndex
PRIME_INDEX_LENGTH
cmpResult
tmpArray
delimiterIndex
knownHashIndex
loadMainDataFromFile
itemIndex
"/coredict.dct"
wordDictRoot
findInTable
hashIndex
tmpFreq
wordIndexTable
testChineseStopWordsDefaultTwoPhrasesIdeoSpace
"扎"
"尼"
"我购买︱
"福"
"素"
testChineseStopWordsOff
testChineseStopWordsDefault
testNonChinese
testChineseAnalyzer
"我购买了道具和服装。"
"ت"
了道具和服装"
"购买"
testOOV
"我购买了道具和服装。
TestSmartChineseAnalyzer
Ｔｅｓｔｓ
testChineseStopWordsDefaultTwoPhrases
了道具和服装１２３４"
了道具和服装1234"
"吉"
"优素福拉扎吉拉尼"
"我购买了道具和服装"
testMixedLatinChinese
testChineseStopWords2
ca
"ب"
"优素福·拉扎·吉拉尼"
روبرتTests
"Title:San"
"了"
"我购买了道具和服装　我购买了道具和服装。"
"و"
"服装"
"优"
"我购买
"道具"
我购买了道具和服装。"
"拉"
"---
---"
Nested
"Body
setQuiet
setShowWarnings
tidyConfigFile
parseDOM
titleElement
setConfigurationFromFile
getBodyText
htmlDoc
Tidy
tidy
"Title
pathTerm
EnumeratedAttribute
handlerConfig
indexIt
totalIgnored
"Sub
totalIndexed
AnalyzerType
"Analyzer
analyzerLookup
setDynamicAttribute
DynamicConfigurator
totalFiles
checkLastModified
indexModified
"checkLastModified
FileResource
elementName
classname
handlerClassName
getClassname
ResourceCollection
MSG_VERBOSE
rcs
addConfig
readable."
isExists
HandlerConfig
setUseCompoundIndex
BuildException
isFilesystemOnly
"IOException:
createDynamicElement
"whitespace"
ignored)
elementAt
useCompoundIndex
setAnalyzerClassName
textDoc
fullname
HtmlDocumentTest
"test.html"
"Body"
Title"
"Title"
IndexTaskTest
"Find
docsDir
"index.dir"
docHandler
"org.apache.lucene.ant.FileExtensionDocumentHandler"
document(s)"
setDir
setProject
TextDocumentTest
Contents"
"Contents"
DEFAULT_SCALE_UP
DEFAULT_RUN_COUNT
DEFAULT_MAXIMUM_DOCUMENTS
<algorithm
algorithm!"
find/read
"####################"
algFile
file>"
executed"
algorithm:"
"###
"Benchmark
PerfRunData!"
understand
!!!
algorithm!
###"
"org.apache.lucene.benchmark.byTask.feeds.DocMaker"
readTaskQueryMaker
eraseIndex
qm
queries:"
"org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker"
runFinalization
"log.queries"
readTask
qmkrClass
startTimeMillis
readTaskClass
nextQnum
size)
supported!"
".makeQuery(int
bytesCount
docsCount
".bzip"
BufferedInputStream
shouldLog
BZIP
"content.source.verbose"
".bz2"
totalDocsCount
extensionToType
"content.source.encoding"
dirFiles
typeInt
CompressorException
"content.source.log.step"
totalBytesCount
"ignoring
_b
_a
DirContentSource
"dir-out"
texts:
reuseFields
bodyTokenized
"doc.random.id.limit"
indexProperties
bodyStored
printDocStatistics
"docid"
printNum
lastPrintedNumUniqueTexts
updateDocIDLimit
indexVal
incrNumDocsCreated
docdata
resetLeftovers
termVecPositions
NAME_FIELD
nut
numDocsCreated
lvr
LeftOver
"doc.body.stored"
bdy
bodyNorms
"doc.body.tokenized"
bodyField
lastPrintedNumUniqueBytes
bodyStore
bytesField
bodyIndex
leftovr
"doc.store.body.bytes"
storeBytes
termVecVal
reset:
termVecOffsets
bodyIndexVal
"doc.reuse.fields"
dd2
getDocState
"org.apache.lucene.benchmark.byTask.feeds.SingleDocSource"
nub
BYTES_FIELD
nameField
bodyStoreVal
"docdate"
"bytes"
termVec
"JAN"
"JUL"
TITLE
keepImages
createXMLReader
BODY
qualified
LENGTH
"APR"
localFileIS
ELEMENTS
threadDone
"MAR"
nmde
tmpTuple
"[\t\n]"
elemType
getElementType
tuple
XMLReaderFactory
"OCT"
PAGE
"JUN"
"SEP"
sae
"DEC"
"#redirect"
EnwikiContentSource
"FEB"
"Image:"
"MAY"
"AUG"
"NOV"
Automobiles"
Trains
United
"Queen"
"Lily
"50
Night
"enwikiQueryMaker.disableSpanQueries"
Geldof"
Youth"
"Green
Dark
"Shakepearian
"Audio
"Dad's
"pen-pal"
"Turtle"
gif"
Shippuuden"
Filthy
haram"
"Nehalem"
hip
"London
Last
Woods"
games\""
James
"Neda"
"Superman-Prime"
Ingalls"
"operating
ZIP
countries
Global
Army"
"Radiohead"
Universities\""
"Nightwish"
Offer."
Guitarists
Time\""
jackson"
Martin
"\"Top
Song"
Place"
Expendables"
hobbies"
"Little
Life
"\"100
"Oksana
"U2"
Lesseps"
"Frank
"Deadpool"
Paradiso
"Imunisasi
Jimenez"
"Nicola
"\"Red
Bouvier
A4"
encoding"
"Solanum
"Freddy
Xbox
GDP"
"campaign"
Portable
region
peppers\""
"Polyfecalia"
Elder
games
Chef"
Deleon\""
Case"
PlayStation
Sullivan"
"Juxtaposition"
Discography"
"Michael
3.0"
Was
"Favicon
Monday
melanocerasum"
Pixie
"freeloader"
"\"List
"Edith
Losers\""
"AUDI
"LuAnn
Bini
artist"
region"
"\"The
Explanation"
"lifehouse"
American
"Bolton
Greatest
Woody
chili
"Iron
States\""
maiden"
Pictures"
catbox
sonnet"
"Skil
Girls"
Codes
Gimenez"
"Enthesopathy
EnwikiQueryMaker
ico"
Grigorieva"
Beale:
"Web
Thai"
wcq
V"
Tesla"
"Jennifer
Jr"
floyd"
Miracle\""
"List
"Images
hot
actors"
"Blood
greatest
"Pink
Lucas
"\"Skylar
Taylor"
"Naruto
Show"
hernia"
"Calculus"
"Dean
"Drake
Freeman"
Return
Wi-Fi
"Pelvic
Scrolls
"Paloma
beatles"
"Muhammad."
WITHER"
"Vince
"Acdc"
"3Me4Ph"
"Planes,
gunners"
"Metallica"
Artists
"Murder
diet"
"mouse"
"Exception:
asStream
"file.query.maker.default.field"
FileBasedQueryMaker
"file.query.maker.file"
line:
Text:
"line:
spot2
openFile
"title_"
LongToEnglishContentSource
"doc_"
getNextCounter
LongToEnglishQueryMaker
"LongToEnglish:
ReutersContentSource
Canada
trade
"\"Ford
"Japan
"ministers
Tariffs
trading"
necessary
Nigeria"
Bank\"
"\"World
"Comex"
Credit\"~5"
talks
"\"food
"airline
Agreement
Bank\"^2
"night
Trade
(GATT)
"Long
"Salomon"
Europe
Japan\""
Sony"
Uruguay
needs\"~3"
"\"Sony
General
-Nigeria"
"zoom*"
body\""
text\"~3"
"\"synthetic
"synth*"
"synthetic
SimpleSloppyPhraseQueryMaker
wd
qlen
remainedSlop
wind
newdocid
"Bahrain"
"Lesotho"
SortableSingleDocSource
Verde"
"Niue"
"Fiji"
"Iraq"
"Armenia"
"Aruba"
"Uruguay"
Man"
"Uganda"
"Belize"
"Georgia"
Kingdom"
"Venezuela"
Kong"
"Jamaica"
"Lithuania"
"Liberia"
Islands"
Vincent
"Canada"
Kitts
Gaza"
"Swaziland"
"Samoa"
"Indonesia"
"Ecuador"
"Morocco"
"Romania"
"Yemen"
"Dominican
"Latvia"
"Uzbekistan"
"Chad"
"Slovakia"
"British
"Isle
Herzegovina"
"Malta"
"Mongolia"
"Bosnia
"Spain"
Arabia"
"Switzerland"
"Turkey"
"Belgium"
"alestinian
"Egypt"
"Angola"
"rand.seed"
"Tuvalu"
"Jordan"
"Tajikistan"
"Cameroon"
China"
"Israel"
Republic
"Hungary"
"Mozambique"
"Iran"
"Tanzania"
"Syria"
"Bangladesh"
"Benin"
"San
"Australia"
"Croatia"
"Ghana"
"Gabon"
"Afghanistan"
"Tonga"
"Antigua
"Guernsey"
"Mexico"
"French
"Greece"
"Hong
"Austria"
"Kosovo"
"Bhutan"
"Burundi"
"United
"Philippines"
"Eritrea"
"Cape
"Zambia"
"Slovenia"
Marino"
Rica"
"Vanuatu"
"random_string"
Emirates"
Salvador"
"Cyprus"
Guinea"
"Mauritius"
"Maldives"
"Germany"
"Brunei"
"Equatorial
"Honduras"
"Ukraine"
"Palau"
"Faroe
"Ireland"
African
Timor"
"Djibouti"
"Guinea"
"Malaysia"
"Portugal"
"Jersey"
"Japan"
"Papua
Nevis"
"Saudi
"Solomon
"Colombia"
Faso"
"Estonia"
"Moldova"
"Trinidad
"Kuwait"
"sort_field"
"Paraguay"
"Ivory
"Dominica"
"Cuba"
"Sudan"
"Nicaragua"
Samoa"
"Panama"
"Botswana"
"Mauritania"
"Qatar"
"Costa
"East
"Kenya"
Mariana
Caledonia"
"Guam"
"China
"Serbia"
"Suriname"
"Northern
"Micronesia"
"Kazakhstan"
"Seychelles"
"Namibia"
Zealand"
"Macau"
Union"
"Togo"
Congo"
States"
"Rwanda"
"Greenland"
"Liechtenstein"
"Zimbabwe"
"Niger"
Lanka"
"Saint
"Comoros"
"Nepal"
Korea"
"S�o
Lucia"
"Nigeria"
"Poland"
"Russia"
"Chile"
"Pakistan"
Virgin
"Czech
"Argentina"
"Grenada"
"Gibraltar"
Macedonia"
"Belarus"
"Luxembourg"
Leone"
"Malawi"
"Turkmenistan"
Polynesia"
"Iceland"
Barbuda"
Republic"
sortRange
"Mali"
Pr�ncipe"
"Anguilla"
Gambia"
"Finland"
"Haiti"
Bank
"Lebanon"
"European
"Kiribati"
"Republic
"Albania"
"American
Bahamas"
Africa"
"Peru"
"Bulgaria"
"Sri
COUNTRIES
Arab
"Sierra
"Cambodia"
"Thailand"
"Guatemala"
Tobago"
(PRC)"
"El
"Bolivia"
"Singapore"
"Ethiopia"
"Central
"Madagascar"
"Kyrgyzstan"
"Cook
"Tunisia"
"Montenegro"
"Burkina
Grenadines"
Coast"
"Guyana"
"Norway"
"Vietnam"
"Azerbaijan"
"Laos"
"Guinea-Bissau"
"Algeria"
"Oman"
"Somalia"
"Barbados"
"sort.rng"
"Brazil"
Tom�
"Burma"
"Libya"
"Senegal"
"South
terminatingTag
getTrecDocReader
"org.apache.lucene.benchmark.byTask.feeds.DemoHTMLParser"
"<DOC>"
docBuf
'bad'
TERMINATING_DOCHDR
dd-MMM-':'y
"opening:
excludeDocnameIteration
trecDocBuffer
"content.source.excludeIteration"
"html.parser"
trecDocReader
findTerminatingDocHdr
"</DOCHDR>"
docBuffer
DOCHDR
zis
"trec"
rawDocSize
parserClassName
DOCNO
"<DOCNO>"
"</DOCNO>"
kk:mm:ss
"<DOCHDR>"
dd-MMM-yyy
#retries="
getDocBuffer
TERMINATING_DOCNO
TERMINATING_DOC
"AddDocs"
programmatic
"org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker"
rep
seq1
"cmpnd:true:true:true:true:false:false:false:false"
"buf:10:10:100:100:10:10:100:100"
"mrg:10:100:10:100:10:100:10:100"
"org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource"
Sample
nextTaskRunNum
currentStats
outOf
step"
countsByTimeStepMSec
totMem
getNumParallelTasks
maxUsedMem
usedMem
msecStep
getCountsByTimeStepMSec
numRuns
by-time
taskRunNum
maxTotMem
ClearStatsTask
CloseReaderTask
CloseReader:
"neither
open"
CommitIndexTask
ConsumeContentSourceTask
"content.source
"org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy"
policy"
"concurrent.merge.scheduler.max.thread.count"
cnfe
ramBuffer
deletionPolicyName
"max.field.length"
scheduler"
"ram.flush.mb"
"merge.policy"
"org.apache.lucene.index.ConcurrentMergeScheduler"
"deletion.policy"
indexDeletionPolicy
maxBuffered
"concurrent.merge.scheduler.max.merge.count"
infoStreamVal
"org.apache.lucene.index.LogByteSizeMergePolicy"
IndexDeletionPolicy"
"merge.scheduler"
delRate
1717
DeleteByPercentTask
(delete)
"delete.percent.rand.seed"
deleted:
DeleteDocTask
DEFAULT_DOC_DELETE_STEP
lastDeleted
byStep
deleteStep
FlushReaderTask
"NRT
times:"
reopenCount
NearRealtimeReaderTask
reopenTimes
NearRealtimeReader"
delay
waitForMsec
pauseMSec
analyzerClassNames
"org.apache.lucene.collation.ICUCollationKeyAnalyzer"
"com.ibm.icu.text.Collator"
"jdk"
NewCollationAnalyzerTask
Implementation
collatorClassName
"impl"
"Locale
task!"
"org.apache.lucene.collation.CollationKeyAnalyzer"
"getInstance"
"icu"
collatorMethod
"java.text.Collator"
NewLocale
collatorClazz
ICU
locale"
getDisplayName
NewLocaleTask
"root
createLocale
NewRoundTask
"standard.StandardAnalyzer"
NewShingleAnalyzerTask
Analyzer"
wrapping
ShingleAnalyzerWrapper,
over"
wrappedAnalyzer
userData:
OptimizeTask
disableCounting
reportStats
records"
getMaxDepthLogStart
"log.step."
taskLogStepAtt
pnts
isDisableCounting
task:
parameters."
logStepCount
runInBackground
logStepAtt
-->
"processed
maxDepthLogStart
dels:"
PrintReaderTask
numDocs:"
printHitsField
DEFAULT_SEARCH_NUM_HITS
"print.hits.field"
"MultiTermQuery
"search.num.hits"
"maxDoc()
"numDocs()
fieldsToHighlight
numHighlight
"totalHits
ReadTokensTask
totalTokenCount
ReopenReaderTask
reportAll
RepAllTask
elapsedSec"
longest
partOfTasks
recsPerRun"
USEDMEM
round"
byTime
ELAPSED
time:"
longetOp
ROUND
avgUsedMem"
Select
RepSelectByPrefTask
reportSelectByPrefix
reportSumByNameRound
RepSumByNameRoundTask
reportSumByName
RepSumByPrefRoundTask
reportSumByPrefixRound
reportSumByPrefix
ResetSystemEraseTask
RollbackIndexTask
"highlighter.maxDocCharsToAnalyze"
"mergeContiguous["
SearchTravRetLoadFieldSelectorTask
SearchTravRetTask
SearchTravRetVectorHighlightTask
"doc.term.vector.offsets
"fragSize["
fragSize
"doc.term.vector.positions
SearchWithSortTask
"string_val"
"nomaxscore"
"noscore"
typeString
doScore
sortField0
newSortFields
page:int,subject:string"
SetPropTask
letChildReport
"/min"
setSequenceName
RunBackgroundTask
tasksArray
numTasks
startlThreadsWithRate
bgTask
nextStartTime
tasks"
initTasksArray
"Par"
anyExhaustibleTasks
"report.time.step.msec"
doParallelTasks
"/sec"
doSerialTasksWithRate
runTime
fixedTime
"_Par"
doSerialTasks
resetExhausted
collapsable
rate:
ParallelTask
seqName
delayStep
waitMore
EXHAUST"
bgTasks
runTimeSec
"REPEAT_EXHAUST
logByTimeMsec
perMin
runningParallelTasks
updateExhausted
startThreads
"_Exhaust"
"Seq"
UpdateDocTask
"updated
"you
eg:
waitTimeSec
multiplier
2h"
4.5m,
WaitTask
10.0s,
WarmTask
"[\t\r\n]+"
lineFileOut
doBZCompress
doBzipCompression
"line.file.out
NORMALIZER
"bz2"
EOF:
nval
'sec'
sequences"
stok
XXXs:
sequence
"Unmatched
extrct
taskPackage
isDisableCountNextTask
unexpected"
prevTask
colonOk
TT_EOF
commentChar
seq2
"double
algTxt
"named
within
ordinaryChar
unexpexted:
eolIsSignificant
ttype
"colon
task"
currSequence
problem
StreamTokenizer
prm
serial
"unexpexted
'min'
number:
TT_NUMBER
unit:
TT_WORD
colForValByRound
propToBooleanArray
dflt
ai
propToIntArray
propKeys
algorithmText
roundNum
lastConfigLine
"-------------------------------"
property!"
ad
roundNumber
printProps
valByRound
"-->"
propToDoubleArray
formatPaddRight
cls
cannpt
cbuf.length="
negative:
"off="
ready
"Read-ahead
Stats:"
getMaxResults
qualityQueries
qualityLog
maxQueries
setMaxQueries
stts
getMaxQueries
nQueries
analyzeQueryResults
submitRep
nameValPairs
getNames
nOther
Points:
getNumGoodPoints
rank
pReleventSum
recallPoints
getRecallPoints
"MRR:
RecallPoint
Good
docNamesExtractTime
queries!"
1E-6
numPoints
getMaxGoodPoints
maxGoodPoints
[1,"
pAt
"n="
points!"
"DocName
"Recall:
Precision:
illegal
fracFormat
getRank
paddLines
\"good\"
getDocNamesExtractTime
"Fishy:
setGroupingUsed
getSearchTime
frac
numGoodPoints
"point
"Precision
getMRR
mrr
At
range!"
getNumPoints
Seconds:
<indexDir>
Title
relevance
TD
QueryDriver
submission
consisting
"topicsFile:
Description).
"qrelsFile:
<qrelsFile>
only)"
"submissionFile:
"\texample:
T=title,D=description,N=narrative:"
[querySpec]"
composed
(title
<topicsFile>
"querySpec:
judgements"
trec_eval"
"SUMMARY"
<submissionFile>
fieldSpec
"indexDir:
Trec1MQReader
qtext
judgments
relevant
missingJudgements
next:
missingQueries
addRelevandDoc
relevantDocs
qrj
judgements
query!
curr
judgments!
QRelJudgement
"<title>"
"<num>"
"<desc>"
"<narr>"
fldSel
ttxt
TermDf
Description:"
Narrative:"
"<num>
numQueries
bestQueries
word:
QualityQueriesFinder
tdf
tf1
"<title>
"<desc>
tf2
formatQueryAsTrecTopic
bestTerms
TermsDfQueue
<index-dir>"
qqf
"<narr>
qqName
qqNames
"Q0"
total="
"free="
R-reopen,
N-no"
W-warmup,
vqd
qd
"NT"
data:
"NR"
"qd-"
T-retrieve,
warmup
"NW"
ldc
setRunData
directory="
lc
avgFreeMem"
setOptimize
ID:
speed
isOptimize
MAX_BUFFERED_DOCS_COUNTS
Ltotal1
Ltotal0
maxBufferedDocs="
mapMem
Index"
showRunData
DrecordCount
isCompound
Warm
optimize="
recCnt"
DATA]"
Traverse
MERGEFACTOR_COUNTS
retrieving
Lcount0
Lcount1
"#--
TestData
--\n"
lineSep
source="
"trav
optionally
setHeap
[NO
heap="
getHeap
RUN
recordCount
Dtotal
setQueries
DCounter
FREEMEM
mapSpeed
getTestDataMinMaxMergeAndMaxBuffered
trd
setSource
"warm
setCompound
"td-"
LDCounter
LCounter
"srch
Reader"
makeSpeed
resByTask
Dcount
endRun
"MB"
startRun
"TOTAL
rps
count\telapsed\trec/s\tfreeMem\ttotalMem"
withMem
Reuters
"<TITLE>(.*?)</TITLE>|<DATE>(.*?)</DATE>|<BODY>(.*?)</BODY>"
META_CHARS_SERIALIZATIONS
".sgm"
LINE_SEPARATOR
"</REUTERS"
sgmFile
.sgm
files>
sgmFiles
"&apos;"
org.apache.lucene.benchmark.utils.ExtractReuters
outBuffer
ExtractReuters
reutersDir
SGM
Path>"
extractFile
EXTRACTION_PATTERN
META_CHARS
"org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource"
"[--output|-o
ExtractWikipedia
BASE
Wikipedia
--input|-i
"--discardImageOnlyDocs
Extraction"
keepImageOnlyDocs
EnwikiContentSource"
org.apache.lucene.benchmark.utils.ExtractWikipedia
images"
"Extraction
"--output"
[--discardImageOnlyDocs|-d]"
tells
Path>]
"./enwiki"
"--discardImageOnlyDocs"
"Extracting
Wiki
"--input"
NoDeletionPolicy
"test/benchmark"
logTstLogic
testLocale
testParallelExhausted
"doc.term.vector=vector:false:true"
testHighlighting
testHighlightingNoTvNoStore
testIndexAndSearchTasks
collationParam
"doc.tokenized=false"
propLines
testBGSearchTaskThreads
CreateIndex!"
"maxShingleSize:3,outputUnigrams:false"
"doc.stored=false"
getShingleConfig
"debug.level=1"
AddDoc]:
"content.source.forever=true"
"one,two,three,"
testIndexWriterSettings
"/contrib/benchmark/src/test/org/apache/lucene/benchmark/reuters.first20.lines.txt"
testParallelDocMaker
"RepSumByPref
testLineDocFile
testDocMakerThreadSafety
assertEqualShingle
"1000
"NewShingleAnalyzer("
NUM_TRY_DOCS
"CloseReader"
ResetSystemErase"
check!"
"content.source=org.apache.lucene.benchmark.byTask.feeds.SingleDocSource"
term(s),
CloseIndex(false)"
"{WriteLineDoc()}:"
"query.maker="
algText
"doc.reuse.fields=false"
"log.time.step.msec
\"AddDocs\"
"content.source.log.step=1"
\"Rounds\""
"analyzer=org.apache.lucene.analysis.WhitespaceAnalyzer"
testDisableCounting
139
find!"
"RepSumByName"
getReuters20LinesFile
testHighlightingTV
"Optimize"
algLines1
algLines2
70"
getLocaleConfig
Optimize(3)"
"max.buffered=3"
CloseIndex!"
testCloseIndexFalse
CloseIndex"
termAtt1
"OpenReader(false)"
"log.step.AddDoc=10000"
testExhaustContentSource
localeParam
ndocsExpected
.5s"
cfsCount
"analyzer:WhitespaceAnalyzer"
"{ReadTokens}:
supposed
count!"
numLines
"Extra
"OpenReader"
algLinesToText
"content.source=org.apache.lucene.benchmark.byTask.feeds.SortableSingleDocSource"
"one,two,three,
"Mismatch
"{AddDoc}:
six"
"elapsed
"ram.flush.mb=4"
&-1"
NewLocale("
"merge.policy=org.apache.lucene.index.LogDocMergePolicy"
"compound=cmpnd:true:false"
testCollator
"line.file.out="
"ResetInputs
"max.buffered=2"
called!"
"task.max.depth.log=1"
testReadTokens
testShingleAnalyzer
nChecked
getCollatorConfig
MyMergePolicy
"merge.scheduler="
"test.reuters.lines.txt"
doTestDisableCounting
MergeScheduler"
"}
"OpenReader(true)"
lineFile
"doc.index.props=true"
CloseIndex()"
"Rounds"
"doc.term.vector.offsets=true"
NewCollationAnalyzer("
"TestSearchTask
CreateIndex"
CountingHighlighterTest(size[1],highlight[1],mergeContiguous[true],maxFrags[1],fields[body])
"en,US"
testMergeScheduler
CountingSearchTest
"content.source.log.step=3"
exist?...!"
"impl:jdk"
"merge.factor=3"
testExhaustedLooped
"content.source.log.step=30"
"NO"
30"
NewRound"
"outputUnigrams:false,maxShingleSize:3,analyzer:WhitespaceAnalyzer"
assertEqualCollation
totalTokenCount2
totalTokenCount1
Wait(0.5)"
"ReadTokens"
"merge.policy="
"ram.flush.mb=-1"
\"XSearch\"
"no,NO,NY"
"analyzer=org.apache.lucene.analysis.SimpleAnalyzer"
disableCountingLines
"log.step=100000"
"100
"CountingHighlighterTest
testTimedSearchTask
200"
4}
testParseParallelTaskSequenceRepetition
INDENT
sequential!"
propPart
TestPerfTasksParse
testParseTaskSequenceRepetition
"AddDoc"
taskStr
"Task
parallel!"
algTasks
foundAdd
"repetions
parsedTasks
createTestNormsDocument
OneDocSource
indexPropsVal
normsPropVal
setNormsProp
numExpectedResults
bodyNormsVal
setIndexProps
doTestIndexProperties
testIndexProperties
DocMakerTest
setBodyNormsProp
LineDocSourceTest
createRegularLineFile
testInvalidFormat
doIndexAndSearchTest
createBZ2LineFile
testCases
"testBzip2"
StringableTrecSource
assertNoMoreDataException
"<body>\r\n"
"Last-Modified:
Sun,
testBadDate
08:00:00
Bad
expName
stdm
614\r\n"
close\r\n"
TrecContentSourceTest
GMT\r\n"
"Connection:
testForever
"HTTP/1.1
testOneDocument
"</title>\r\n"
"TEST-000
"TEST-001
testMissingDate
"Server:
"</DOC>\r\n"
title\r\n"
"</head>\r\n"
text\r\n"
"</body>\r\n"
"<html>\r\n"
assertDocData
(Unix)\r\n"
"http://lucene.apache.org.trecdocmaker.test\r\n"
"<DOCNO>TEST-000</DOCNO>\r\n"
"TEST-000_0"
"<title>\r\n"
"<DOC>\r\n"
"TEST-001_0"
"<DOCNO>TEST-001</DOCNO>\r\n"
testTwoDocuments
"TEST-000_1"
08:01:00
"Sun,
Apache/1.3.27
"</DOCHDR>\r\n"
OK\r\n"
Date\r\n"
"Content-Length:
NoMoreDataException"
"Content-Type:
"<DOCHDR>\r\n"
text/html\r\n"
"<head>\r\n"
2008
2009
incrNumSearches
lastMillis
getElapsedMillis
infoStreamValue
"infoStreamTest"
setErr
curOut
testInfoStream_File
cit
testInfoStream_SystemOutErr
setOut
curErr
CreateIndexTaskTest
doLogStepTest
testLogStep
setLogStep
expLogStepValue
taskLogStepVal
MyPerfTask
mpt
getLogStep
logStepVal
"log.step.MyPerf"
setTaskLogStep
PerfTaskTest
testEmptyBody
NoBodyDocMaker
"body\r\ntext\ttwo"
"title\r\ntext"
doReadTest
numExpParts
wldt
WriteLineDocMaker
WriteLineDocTaskTest
testJustDate
testCharsReplace
"body
bz2File
NoTitleDocMaker
testEmptyTitle
"date\r\ntext"
NewLinesDocMaker
JustDateDocMaker
docMakerName
topicsFile:
perfect:
"trecTopics.txt"
testTrecTopicsReader
"apache"
"org"
"20
Line
1E-2
p_at_"
week"
Topic
"TestRun"
1987"
"agreed
hurt:
createReutersIndex
Narrative
workDir:
TestQualityRun
"benchmark"
"quality"
statistis:"
hurt"
getReuters578LinesFile
"trecQRels.txt"
"p_at_"
"content.source.log.step=2500"
"statement
"avg
"Topic
testTrecQuality
"/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2"
qrelsFile:
srcTestDir
avg-p
Description
"recall
"mean
"doc.tokenized=true"
"avg-p
transaction
"__"
setInitializeLocking
"log."
DatabaseType
setInitializeLogging
BTREE
setThreaded
DbStoreTest
setInitializeCache
SanityLoadLibrary
getSearchKey
Reading
JEStoreTest
foundData
"blocks
SUCCESS
foundKey
LockMode
"jdb"
"je.lck"
makeFragment
style=\"background:palegreen\">"
getPreTag
style=\"background:yellow\">"
getPostTag
style=\"background:lime\">"
preTags/postTags
"maxNumFragments("
number."
style=\"background:wheat\">"
style=\"background:lawngreen\">"
style=\"background:aquamarine\">"
style=\"background:magenta\">"
"</b>"
style=\"background:khaki\">"
srcIndex
getFragmentSource
nextValueIndex
style=\"background:deepskyblue\">"
fragInfo
style=\"background:coral\">"
"<b>"
String[]"
checkTagsArgument
DEFAULT_PHRASE_HIGHLIGHT
isFieldMatch
isPhraseHighlight
DEFAULT_FIELD_MATCH
fragmentsBuilder
getFieldFragList
getSubInfos
getTermsOffsets
"subInfos=("
")/"
getSeqnum
getTotalBoost
phraseInfoList
oso
existWpi
oeo
isOffsetOverlap
nextMap
wpi
addIfNoOverlap
sourceQuery
getOrNewMap
termOrPhraseNumber
qj
checkOverlap
bts
rootMap
srcTerm
nextTermOrPhraseNumber
ats
expandQueries
saveTerms
markTerminal
flatQuery
first."
poss
x:b"
tvois
ScoreComparator
ite
MARGIN
wpil
small.
"fragCharSize("
higher."
hardware."
speed,
biMVValues
DEFAULT_N_SIZE
"\nWhen
strMVValues
customizable
BigramAnalyzer
lenTerm
"\nLucene/Solr
shortMVValues
DEFAULT_DELIMITERS
getFinalOffset
charBufferLen
snippetBuffer
BasicNGramTokenizer
readCharFromBuffer
getNextPartialSnippet
charBufferIndex
getNextSnippet
additional
engines
basically"
"hijkl"
nextStartOffset
0.0F
customization:"
\t\n.,"
analyzerK
analyzerB
"Followings
delimiters
make1dmfIndexNA
isDelimiter
longMVValues
talk
"ab(1.0)((0,2))"
test2PhrasesOverlap
"a(1.0)((0,1))"
"d(1.0)((6,7))"
"a(1.0)((2,3))"
"abc(1.0)((6,11))"
test1PhraseIndexB
test2TermsIndex
"ac(2.0)((4,5)(8,9))"
test1PhraseIndex
testSearchLongestPhrase
"abc(1.0)((2,7))"
"ab(1.0)((4,7))"
"bbbacbabc"
"abc(1.0)((10,15))"
"ab(1.0)((2,5))"
"b(1.0)((2,3))"
testPhraseSlop
"baac(1.0)((2,5))"
"ab(1.0)((2,4))"
test2ConcatTermsIndexB
"sppeeeed(1.0)((88,93))"
test3TermsPhrase
"abab"
test2Terms1PhraseIndex
"searchengines(1.0)((102,116))"
FieldPhraseListTest
"searchengines(1.0)((157,171))"
"ab(1.0)((0,3))"
testQueryPhraseMap1Phrase
testQueryPhraseMapOverlap2gram
"CD"
testGetTermSet
testFlattenDisjunctionMaxQuery
(D
testExpand
testSearchPhraseSlop
testFlatten1TermPhrase
\"B
rootMap4
rootMap3
rootMap1
FieldQueryTest
qpm4
testNoExpand
testFlattenTermAndPhrase2gram
testFlattenBoolean
3F
rootMap2
bcd"
C\""
EFGH"
pqm
testGetFieldTermMap
"BC"
"AA
BCD
testFlattenTermAndPhrase
"GH"
testGetRootMap
x:C
E)"
testGetRootMapNotFieldMatch
testQueryPhraseMap1Term
testQueryPhraseMapOverlapPhrases
testQueryPhraseMap2Phrases
"FG"
testQueryPhraseMapOverlapPhrases3
"EF"
testQueryPhraseMapOverlapPhrases2
qpm2
qpm3
qpm5
qpm6
qpm7
testSearchPhrase
testExpandNotFieldMatch
testQueryPhraseMap1PhraseAnother
testQueryPhraseMap2PhrasesFields
"a(28,29,14)"
"engines(109,116,15)"
"ef(11,13,11)"
"bb(3,5,3)"
test1PhraseMVB
makeIndexB
test1Term
test1TermB
"c(18,19,9)"
"b(6,7,3)"
"c(10,11,5)"
"bc(4,6,4)"
"pe(89,91,62)"
test2Terms
"b(30,31,15)"
"bb(7,9,7)"
"sp(88,90,61)"
"aaabbcabbcdef"
"b(14,15,7)"
"d(20,21,10)"
"engines(164,171,25)"
"a(0,1,0)"
"a(32,33,16)"
"ee(90,92,63)"
value2
"search(102,108,14)"
"b(26,27,13)"
"ed(91,93,64)"
"b(16,17,8)"
"d(6,7,3)"
"ab(2,4,2)"
"b(8,9,4)"
test2TermsB
test1PhraseB
"a(12,13,6)"
"bc(8,10,8)"
"a(4,5,2)"
"search(157,163,24)"
"ab(6,8,6)"
"a(2,3,1)"
test1Phrase
FieldTermStackTest
testFieldTermStackIndex2w1wSearch1term
testFieldTermStackIndex2w1wSearch1term1phrase
"I'll
Macintosh"
"pc(3,5,1)"
expectedSet
testFieldPhraseListIndex1w2wSearch1partial
testFieldTermStackIndex1w2wSearch1phrase
testFieldTermStackIndex1w2wSearch1term
"personalcomputer(1.0)((3,5))"
"I'll"
broken"
makeIndex1w
"Mac"
"(1.0)((3,5))"
"(1.0)((3,20))"
"pc"
testFieldPhraseListIndex2w1wSearch1term1phrase
"computer(3,5,2)"
testFieldPhraseListIndex2w1wSearch1partial
testFieldTermStackIndex2w1wSearch1partial
testFieldPhraseListIndex2w1wSearch1phrase
personal
"MacBook(11,20,3)"
testFieldTermStackIndex1w2wSearch1term1phrase
"MacBook"
"Macintosh"
"pc(1.0)((3,20))"
makeSynonymIndex
makeIndex2w1w
testFieldTermStackIndex1wSearch1term
buy
"buy"
TokenArrayAnalyzer
pc
testFieldTermStackIndex2w1wSearch1phrase
testFieldPhraseListIndex1w2wSearch1term1phrase
IndexTimeSynonymTest
"computer(1.0)((3,20))"
"computer(3,20,2)"
"computer(1.0)((3,5))"
"personal(3,20,1)"
testFieldTermStackIndex1wSearch2terms
testFieldPhraseListIndex2w1wSearch1term
"personal(3,5,1)"
"Mac(11,20,3)"
testFieldPhraseListIndex1w2wSearch1phrase
testFieldTermStackIndex1w2wSearch1partial
makeIndex1w2w
"pc(3,20,1)"
"personalcomputer(1.0)((3,20))"
"personal"
sofb
ScoreOrderFragmentsBuilderTest
"subInfos=(b((2,3)))/1.0(0,20)"
"subInfos=(sppeeeed((88,93)))/1.0(82,182)"
jklmnopqrs\""
"\"abcdefgh
testSmallerFragSizeThanPhraseQuery
test2TermsQuery
"subInfos=(a((0,1))a((2,3)))/2.0(0,100)"
testNullFieldFragList
"abcdefghijklmnopqrs"
"subInfos=(a((0,1)))/1.0(0,20)"
test2TermsIndex1Frag
"subInfos=(abcdefghijklmnopqrs((0,19)))/1.0(0,19)"
"subInfos=(searchengines((102,116))searchengines((157,171)))/2.0(96,196)"
"subInfos=(ab((0,1)(4,5)))/1.0(0,20)"
"subInfos=(a((0,1)))/1.0(0,100)"
testTooSmallFragSize
jklmnopqrs"
"subInfos=(a((0,1))b((2,3)))/2.0(0,20)"
b\"~1"
"subInfos=(a((8,9))a((18,19)))/2.0(2,22)"
testPhraseQuerySlop
"subInfos=(ab((0,3)))/1.0(0,20)"
"subInfos=(abcdefghjklmnopqrs((0,21)))/1.0(0,21)"
test2TermsIndex2Frags
SimpleFragListBuilderTest
"subInfos=(d((6,7)))/1.0(0,100)"
"subInfos=(a((20,21)))/1.0(20,40)"
"subInfos=(a((28,29)))/1.0(22,42)"
"subInfos=(a((26,27)))/1.0(20,40)"
"subInfos=(a((0,1))a((18,19)))/2.0(0,20)"
"abcdefgh
testSmallerFragSizeThanTermQuery
test2Frags
t"
"[a]"
makeUnstoredIndex
"abc<b>defg</b>hijkl"
sfb
SimpleFragmentsBuilderTest
"<b>a</b>"
"ssing
<b>search
<b>d</b>
<b>speed</b>,
engines</b>
test1StrMV
testUnstoredField
"minForegroundColor
fgBMax
gVal
hexDigits
fgBMin
bgRMin
relScorePercent
fgRMax
"</font>"
fgRMin
fgGMax
bgBMax
bgBMin
fgGMin
"color=\""
bVal
"minBackgroundColor
bgRMax
#FFFFFF"
rVal
bgGMin
"RGB
bgGMax
"<font
colScore
"bgcolor=\""
colorMin
colorMax
intToHex
getColorVal
bestScoringFragNum
docFrags
setFragmentScorer
fragA
fragB
setEncoder
worstScoringFragNum
frag1
getFragmentScorer
fragQueue
frag2Num
FragmentQueue
mergingStillBeingDone
newStream
currentFrag
sized
sections
textFragmenter
getTextFragmenter
getEncoder
fragmentScorer
frag1Num
foundTerms
fieldWeightedSpanTerms
skipInitExtractor
isExpandMultiTermQuery
qse
initExtractor
getTermsFromFilteredQuery
getTermsFromBooleanQuery
uniqueTermsInFragment
termsToFind
allFragmentsProcessed
currentTextFragment
setFragmentSize
htmlEncode
plainText
DEFAULT_POST_TAG
preTag
postTag
DEFAULT_PRE_TAG
returnBuffer
textSize
queryScorer
wSpanTerm
waitForPos
"color:
SpanGradientFormatter
"background:
#000000;\">...</span>"
style=\"background:
#EEEEEE;
color:
"</span>"
style=\""
EXTRA
"<span
getFragNum
textStartPos
termEndOffset
MAX_NUM_TOKENS_PER_GROUP
getNumTokens
termStartOffset
analyzed"
StoredTokenStream
tokensInOriginalOrder
unsortedTokens
totalTokens
tokenPositionsGuaranteedContiguous
tokenComparator
tokensAtCurrentPosition
positionedTokens
thisPosition
positionSpanIt
isPositionSensitive
setPositionSensitive
posSpan
readerSet
getReaderForField
getExpandMultiTermQuery
cachedTokenStream
prevTerm
collectSpanQueryFields
extractWeightedSpanTerms
FakeReader
EMPTY_MEMORY_INDEX_READER
fieldNameToCheck
spanPositions
mustRewriteQuery
mtq
fReader
PositionCheckingMap
largestInc
extractWeightedTerms
testSparseSpan
testConcurrentPhrase
indexreader
testConcurrentSpan
TokenStreamConcurrent
jump"
"did"
TokenStreamSparse
HighlighterPhraseTest
testSparsePhrase
<B>did</B>
<B>jump</B>"
baseDoc
testSparsePhraseWithNoPositions
del"
Encoding
prices
testSimpleQueryScorerPhraseHighlighting
lets"
referring
nfield
TRUNCATED)"
currentRealToken
"wordz"
testGetFuzzyFragments
Document</title>\n"
QueryTermScorer"
expanMultiTerm
"\"This
"</h2>\n"
testGetBestFragmentsMultiTerm
"text:\"world
"<B>Hi-Speed</B>10
<B>footie</B>
testNumericRangeQuery
"<B>fred</B>
getTS2
lets\""
"<B>Hello</B>"
testQueryScorerMultiPhraseQueryHighlighting
"wordx
<B>foo</B>"
expandedQueries
getTS2a
preamble
"random_field"
"\"Smith
named
"JFK
"SOME_FIELD_NAME"
"<body>\n"
typo
footie
"<title>My
expected:"
"\"world
testWeightedTermsWithDeletes
fragmentResults
<B>people</B>"
long\""
"JF?
<B>world</B>
fragmentSeparator
fieldSpecificScorer
testGetBestFragmentsPhrase
"\"text
xml:lang=\"en\"
"ken.*"
"AnInvalidQueryWhichShouldYieldNoResults"
xhtml
"\"very
"wordb"
"overlapping
assertExpectedHighlightCount
fm
testGetBestFragmentsQueryScorer
testSimpleQueryScorerPhraseHighlighting3
testSimpleQueryScorerPhraseHighlighting2
testPosTermStdTerm
searchterm
QueryScorer"
record:
wordy
realPosIncrAtt
article"
QUERY_TERM
realOffsetAtt
testNotSpanSimpleQuery
testGetBestFragmentsSimpleQuery
SynonymAnalyzer
meat
MaxDocBytesToAnalyze
testNearSpanSimpleQuery
testSimpleSpanFragmenter
numHighlights
speed"
"kennedy"
testOverlapAnalyzer
"searchterm"
"bookid"
expandMT
testSpanHighlighting
"K?nnedy"
testGetSimpleHighlight
testGetMidWildCardFragments
lang=\"en\">\n"
"help
"</html>"
fieldInSpecificScorer
"us
"</body>\n"
encodedSnippet
Found:
f1c
Kenn*"
euro
fieldSpecificHighlighter
testMaxSizeEndHighlight
testMultiSearcher
"multiOne"
"\"been
"<B>help</B>
doSearching
stringResults
testNoFragments
wTerms
wordc"
OK,
testGetWildCardFragments
"john"
"football"
testSimpleQueryTermScorerHighlighter
expectedHighlights
category:people"
testQueryScorerMultiPhraseQueryHighlightingWithGap
section
"wordc"
>4\"
shot"
"fred
worda
wordb
maxNumFragmentsRequired
Flatland\"~3"
testGetBestFragmentsFilteredPhraseQuery
\"x
wordx
srchkey
testGetRangeFragments
expansions
unReWrittenQuery
"shot"
"\"John
long\"~5"
"wordx"
observed
doStandardHighlights
testMaxSizeHighlightTruncates
"beginning"
"Hi-Speed<B>10</B>
sons'
fieldInSpecificHighlighter
testRepeatingTermsInMultBooleans
docMainText
<B>random</B>
"speed"
worked:"
"soccer,footie"
"Expected:
realStream
z\""
"multiTwo"
f2c
so,"
[54-65]"
testGetBestFragmentsWithOr
contains\""
"hispeed"
testGetBestSingleFragment
"lets
<B>Flatland</B>,
"(FIELD
"<B>football</B>-<B>soccer</B>
ph2
ph1
*ANY*
highlighted
rawDocContent
mode"
"\"b
"football-soccer
"nfield"
Kenned*"
"\"x
highlighted"
people"
claims
testMaxSizeHighlight
testUnRewrittenQuery
shot\""
"Hi-<B>Speed</B>10
"goodtoken"
testHighlightingWithDefaultField
testGetTextFragments
"\"piece
g\""
lets
"result:"
"\"lets
testSpanRegexQuery
Fragments:
testGetBestFragmentsFilteredQuery
testEncoding
"stoppedtoken"
kennedy
"XHTML
testSimpleSpanHighlighter
"meat"
"<B>kennedy</B>"
"<head>\n"
decodedSnippet
highlightedText
"ken*"
"h2"
Flatland,
"Hi-Speed10
realTermAtt
testFieldSpecificHighlighting
"<B>Hi</B>-Speed10
testConstantScoreMultiTermQuery
"wordy"
"Kennedy"
Keneddy"
competition"
highlightField
actual:"
primitive
xmlns=\"http://www.w3.org/1999/xhtml\"
testGetBestSingleFragmentWithWeights
"John
"K*dy"
"</head>\n"
testOffByOne
testOverlapAnalyzer2
Kennedy\""
hg
testRegexQuery
"<html
TestHighlightRunner
goodWord
SynonymTokenizer
"Kinnedy~"
testQueryScorerHits
"t_text1:random"
"<h2>"
NUMERIC_FIELD_NAME
kznnedy]"
"Matched
":[kannedy
"Observed:
reusableKey
RawCollationKey
getRawCollationKey
TestICUCollationKeyAnalyzer
TestICUCollationKeyFilter
vectorSpace
indexedVecNames
sourceIndexReader
termVecNames
termsByFieldAndText
allFieldNames
indexedNoVecNames
termVecOffsetNames
termVecPosNames
termVecPosOffNames
sourceDocument
unindexedNames
indexedNames
payloadNames
termsByField
normUpdate
tdi
NormUpdate
uncommittedDeletedDocuments
startPos
uncommittedNormsByFieldNameAndDocumentNumber
tokensByField
informationByTermOfCurrentDocument
documentFieldSettingsByFieldName
termDocumentInformationsByField
indexDeleter
eField_TermDocInfos
dirtyTerms
unflushedDocuments
Are
termsInDocument
eField_Tokens
eFieldTermDocInfoFactoriesByTermText
index?"
fieldNameBuffer
termDocumentInformationFactoryByDocument
numFieldsWithTermVectorsInDocument
eFieldSetting_TermDocInfoFactoriesByTermText
fieldVal
"untokenized"
trying
termDocumentInformationFactoryByTermText
TermDocumentInformationFactory
termDocumentInformationFactory
fieldSettingsByFieldName
eTermText_TermDocInfoFactory
fieldLength
orderedTermsDirty
set!
termDocumentInformationFactoryByTermTextAndFieldSetting
termsByText
2500
eTerm_TermDocumentInformation
unflushedDeletions
eDocumentTermDocInfoByTermTextAndField
pivotVal
nPreviousSteps
termIndex
endPosition
pivot
instantiatedTerm1
binarySearchAssociatedDocuments
currentDocumentIndex
setDocument
doumentNumberIntegerComparator
setTermPositions
nextTermIndex
startPosition
termFrequencies
currentTermPositionIndex
TestEmptyIndex
termEnumTest
ever.
testPos
"norms
"b0"
termFreqVector1
testTermPositions
termPositionIndex
positionVectorIndex
testTermDocsSomeMore
docIndex
testLoadIndexReader
winter?"
"danny"
testInstantiatedIndexWriter
aprioriPayloads
termFreqVector
aprioriTermEnum
aprioriPos
Heres
testIndex
instantiatedIndexWriter
testEqualBehaviour
air
redrum"
testTermPositionVector
Danny,
aprioriIndex
testTermDocsSeeker
freqVectorIndex
aprioriTermDocsSeeker
house
aprioriFreqVectors
Johnny!"
aprioriReader
aprioriTermDocs
aprioriTermPositionVector
us...
Redrum
testFreqVectors
tir
Jack
ever."
TestIndicesEquals
aprioriTermPositions
aprioriNorms
assembleDocument
TestRealTime
TestSerialization
rba"
"moo"
rab
abr
tellus!"
wendy
unoptimized
TestUnoptimizedReaderOnConstructor
girl"
"Hello,
earth!"
unoptimizedReader
danny
reader!"
world!"
Took
explainQuery
available>"
printHit
fieldsArray
"----------------
"#################################################"
<not
invertDocument
computeCount
instructions."
currField
"---------------------"
showTokens
indexedFields
termDetails
score:"
'help'
sortedHash
Fields:"
"Indexed
termIterator
tokenMap
indexedArray
"Switched
initSearch
currentEntry
"Explanation:"
CLI.
analyzerClassFQN
optimizing
HITS_PER_PAGE
msecs"
"'.
"StandardAnalyzer"
getSortedMapEntries
cce
"--------------------------------------"
EXPLAIN
MAX_TERMS
Verbose!
lucli.Lucli"
parseArgs
org.apache.lucene.analysis.SimpleAnalyzer"
field.
DEFAULT_INDEX
QUIT
Example
getCommandId
Example:
describes
"Quit/exit
INDEX
HELP
UNKOWN
commandSet
SEARCH
"Error:"
luceneMethods
COUNT
Completor
setHistory
Command
FileNameCompletor
addCompletor
"Show
commandMap
SimpleCompletor
Lucli
enableReadline
commands"
"user.home"
commands."
takes
"lucli>
\t"
newLm
"quit"
supported)"
History
Supply
ArgumentCompletor
arguments."
commandUsage
"Specifies
query.
search.
NOCOMMAND
"Display
historyFile
"(currently,
program"
scored
HISTORYFILE
handleCommand
".lucli"
tokenizeCommand
"Choose
document.
"Return
getCommandsAsArray
addCommand
my_index"
minCapacity
ArrayIntList
2282195016849084649L
2882195016849084649L
sortedFields
sizeOfHashMap
"MemoryIndexReader.getTermFreqVectors"
Kchars="
"MemoryIndexReader.document"
"TermEnum.close"
"TermEnum.docFreq"
keywordTokenStream
".skipTo:
sizeOfString
newCapacity
sumChars
cachedNorms
"MemoryIndexReader.getFieldNamesOption"
MemoryIndexReader
"MemoryIndexReader.doCommit"
sumTerms
CHAR
"MemoryIndexReader.terms:
"\nfields="
srtFldsIdx
sizeOfObject
".nextPosition:
stride
".close"
"MemoryIndexReader.termPositions"
"MemoryIndexReader.isDeleted"
0.0"
"MemoryIndexReader.terms()"
"MemoryIndexReader.termDocs"
"MemoryIndexReader.norms*:
OBJECT_HEADER
sortedTerms
"MemoryIndexReader.maxDoc"
sizeOfArrayIntList
".read:
freq:"
"index:
sizeOfIntArray
positions="
Info
newElements
"TermEnum.next"
MATCH_ALL_TERM
"\tterms="
".next:
".doc"
cachedFieldName
jx
ix
terms="
2782195016849084649L
"keywords
sumPositions
"MemoryIndexReader.hasDeletions"
numOverlapTokens
cachedSimilarity
sizeOfObjectArray
oldHasNext="
"MemoryIndexReader.norms:
"\t'"
PTR
LOG_PTR
"MemoryIndexReader.docFreq:
"TermEnum.term:
".seekEnum"
sizeOf
throwIndex
invertState
"MemoryIndexReader.getTermFreqVector"
sizeOfArrayList
sizeOfCharArray
"MemoryIndexReader.doClose"
srtTermsIdx
"MemoryIndexReader.numDocs"
".seek:
".freq:
sortTerms
"fieldName
LinkedHashSet
readLines
createRAMIndex
"Apache"
"Apach~
FILE="
"\n***********
iteration="
Copy*"
useRAMIndex
anal
"memram"
"diff="
memind
anal="
"*.txt"
measureIndexing
iters
MemoryIndexTest
"queries/sec=
createMemoryIndex
"Fatal
s2="
useMemIndex
"BUG
"\n###########
ramind
correctness)."
"\nsecs
"xdocs/*.xml"
query="
(without
"src/java/test/org/apache/lucene/queryParser/*.java"
1024.0f
"mem"
"*.xml"
xargs
"MB/sec
testMany
"contrib/memory/src/java/org/apache/lucene/index/memory/*.java"
"@contrib/memory/src/test/org/apache/lucene/index/memory/testqueries.txt"
ramsearcher
getEncoding
DEFAULT_PLATFORM_CHARSET
s1="
memsearcher
"CHANGES.txt"
"*.html"
"LICENSE.txt"
"files
DETECTED:"
benchmarking
setNumLargeSegments
totalLargeSegSize
_mergeFactor
mergeFactor"
maxDelCount
doPartialExpunge
infoLen
sumVariance
expunge
mergeEnd
sizeArr
_useCompoundFile
createVarianceTable
maxMergeSegments
findOneSegmentToExpunge
numLargeSegments
startSeg
_numLargeSegments
expungeCandidate
"maxSmallSegments
_maxSegments
residual
setMergePolicyParams
MergePolicyParams
isUseCompoundFile
getMaxSmallSegments
setMaxSmallSegments
_maxMergeDocs
variance
BalancedSegmentMergePolicy
maxSmallSegments
optSize
totalSmallSegSize
mergeStart
backLink
_doPartialExpunge
numLargeSegs
getNumLargeSegments
_maxSmallSegments
"numLargeSegments
_partialExpunge
minK
partialExpunge
DEFAULT_NUM_LARGE_SEGMENTS
smallSegments
minV
setPartialExpunge
sizeThreshold
findBalancedMerges
targetSegSize
getPartialExpunge
[field2]
"Updating
-n>
"-n"
<index>
termCounts
<package.SimilarityClassName
constructor:
<field1>
"###,###.###"
"-l"
destFile
-d
<destDir>
sizes)"
(delete
srcDir
srcName
destFSDir
(list
listSegments
-l
<segments>+"
<srcDir>
"srcdir:"
destInfos
targetDir
"IndexSplitter
getIdx
<outputDir>
...]"
dels
produce"
missing:
-out
numParts
round-robin)"
<inputIndex1>
numParts\tnumber
"-seq"
skipping:
indexes"
(default
"\tinputIndex\tpath
"\t-seq\tsequential
splitting"
outputDir"
[-seq]
"\t-out
outputs
ouputDir\tpath
numParts"
"-out"
-num
"part-"
<numParts>
outDir
"\t-num
"-num"
partLen
FakeDeleteIndexReader
docid-range
outputs."
[<inputIndex2
oldDels
frequencies
termVectorStored
TermVectorMapperDecorator
decoratedMapper
decorated
doChain
logicArray
"ChainedFilter:
initialResult
org.apache.lucene.misc.HighFreqTerms
dir>
HighFreqTerms
TermInfoQueue
[field]\n\n"
<index2>
IndexMergeTool
<index1>
[index3]
<mergedIndex>
mergedIndex
"Merging..."
tf_hyper_base
overlaps
steepness
ln_maxs
tf_min
baselineTf
ln_max
ln_min
ln_steep
xoffset
tf_hyper_xoffset
tf_base
tf_hyper_max
tf_hyper_min
ln_steeps
ln_overlaps
ln_mins
1.3d
consumed"
ioobe
tlist
token(s)
multipleTokens
part1"
part2"
isWithinToken
wildcard"
tmpBuffer
RangeQuery
"getWildcardQuery
snqs
positiveClauses
phrasedQueryStringContents
numNegatives
currentPhraseQuery
"Dummy
parsePhraseElements
ors
spanClauses
childQuery
bclauses
type:"
complexPhrases
isPass2ResolvingPhrases
includeClauses
checkPhraseClauseIsForSameField
cbq
addComplexPhraseClause
allSpanClauses
chosenList
oldMethod
nots
ComplexPhraseQuery
cpq
slopFactor
DEFAULT_EXTENSION
rawQueryString
topLevelParser
getTopLevelParser
extfield
Cud
Cur
extensionFieldDelimiter
extensionKey
0x180
posincrAtt
0xfb1000
0xfb1f00
orPresent
caDest
0xe00
0x40000
0xfb0000
caSource
0x9a0000
0xc0000000
0x10000000
0x1000000
andExpression
0xc000000
6>"
0xffffff81L
0x97ffffff97ffffffL
0x84002f0600000000L
0x1000000L
0x40L
0x6800000078000000L
20071212"
newNorms
testNormKiller
"20061212"
"untokfield"
TestFieldNormModifier
"20061212
"testfilesplitterdest"
TestIndexSplitter
"testfilesplitterdest2"
destDir2
"testfilesplitter"
"index3"
splitSegName
testSplitRR
testSplitSeq
TestMultiPassIndexSplitter
TestTermVectorAccessor
accessor
1041397200000L
sue"
queryFilter
"owner"
"ANDNOT
testXOR
sueFilter
bobFilter
testOR
MAX
testWithCachingFilter
testANDNOT
"XOR
sues"
testAND
bob"
"sue"
getChainedFilter
cachingFilter
bob
testSingleFilter
ChainedFilterTest
dateFilter
cachingFilter2
a="
6,9:
d="
"3,10:
b="
"f:
"yak"
flat2:
"MIN
"MAX
0.00001f
8,13:
flat1:
testSweetSpotLengthNorm
3,10:
case:
tf:
3.3f
13<x
SweetSpotSimilarityTest
"base
testSweetSpotTf
"tf
testHyperbolicSweetSpot
"MID
7.7f
9<x
10<x
"s:
s="
"tf:
TestLengthNormModifier
motley~0.75
Zellweger~"
Zoé}"
"ubersetzung
rangeInput
Zellw?ger"
ren??
TestAnalyzingQueryParser
zellw?ger"
prefixExpected
übersetz*"
Mötl*
Crü?"
parseWithAnalyzingQueryParser
uber*ung"
Crüe~0.5"
Renée~0.9
"übersetzung
Mötley~0.75
über*ung"
zellw*"
motl*
testWildCardQuery
ranges
fuzzyInput
wildcardExpected
cru?"
Cr\u00fce
bb]"
wildcards
renee~0.9
crue~0.5"
"René?
ubersetz*"
"motley
"renee
"Renée
"{anais
fuzzyExpected
rangeExpected
Crüe
fuzzys
Zellweger
"rene?
Übersetzung~0.9"
crü*"
"{Anaïs
Zellw*"
zellweger
Mötl?*
ASCIIAnalyzer
Ren??
motl?*
"Testing
"[aa
"Übersetzung
prefixInput
cru*"
zellweger~0.5"
zoe}"
crue
ubersetzung~0.9"
"Mötley
wildcardInput
\"smith\"
doc#"
smZ]\"
"johathon
"matched
docsContent
johathon)
"\"j*
smyth~\""
"jackson
"\"(jo*
"\"jo*
nosuchword*\""
"\"john\""
smith\""
"1,3"
[sma
TestComplexPhraseQuery
expectedVals
smith\"~2"
waits
"\"john
-john)
"1,2"
checkMatches
"john
qString
percival
tom"
id:1
checkBadQuery
"1,2,3"
"\"(john
testComplexPhrases
"%s:foo
"aField:testExt:\"foo
"%s:\"foo
TestExtendableQueryParser
testExtField
testExtFieldUnqoted
"extension
\\&
testExtDefaultField
testUnescapedExtDelimiter
"testExt"
"aField"
DELIMITERS
newExtensions
"afield"
booleanClause
tquery
testAddGetExtension
".key"
"abc:?{}[]\\()+-!~"
"field.key"
TestExtensions
testBuildExtensionField
testSplitExtensionField
"abc\\:\\?\\{\\}\\[\\]\\\\\\(\\)\\+\\-\\!\\~"
testGetExtDelimiter
"field\\:key"
"\\:key"
testEscapeExtension
"(A
"(+term
"+a"
(-B
-B"
_testPrecedence
"(+A
(-b)"
"stop
C"
_testSimple
(C
+B)
+C)"
B)
phrase2\")
D)"
"-a"
"term^2.0~0.5"
_testWildcard
TestPrecedenceQueryParser
filters2
mustFilters
"BooleanFilter("
shouldFilters
occurString
appendFilters
notFilters
equalFilters
filters1
filterClause
currTerm
getKeepMode
setFieldName
correctBits
fastBits
getProcessingMode
processedTerms
isIgnoreTF
numVariants
variants
corpusNumDocs
ScoreTermQueue
MAX_VARIANTS_PER_TERM
FuzzyTermQuery
avgDf
termB
termA
FieldVals
fieldVals
variantsQ
totalVariantDocFreqs
possibleMatch
internSavingTemplateTerm
variantQueries
fuzziedSourceTerm
termVariants
ignoreTF
retrieveTerms
"localhost_index"
"-f"
qterms
"\ttitle
FreqQ
"fieldNames
maxWordLen
getBoostFactor
getMaxNumTokensParsed
maxPercentage
minWordLen
getMaxDocFreq
bestScore
"found:
DEFAULT_FIELD_NAMES
"-url"
getMaxWordLen
getMinWordLen
"\tsummary:
DEFAULT_STOP_WORDS
addTermFrequencies
"minTermFreq
DEFAULT_MAX_DOC_FREQ
Files/Apache
myScore
createQuery
"minWordLen
"maxWordLen
termFreqMap
getMinTermFreq
retrieveInterestingTerms
parameters:"
describeParams
"q:
setMaxDocFreqPct
minTermFreq
isBoost
Int
isNoiseWord
"minDocFreq
"maxQueryTerms
"c:/Program
setMaxDocFreq
topField
createQueue
maxNumTokensParsed
Group/Apache/htdocs/manual/vhosts/index.html.en"
"url
likeText
setMoreLikeFields
getMoreLikeFields
minTermFrequency
getMinTermFrequency
getLikeText
percentTermsToMatch
"like:"
setLikeText
moreLikeFields
getPercentTermsToMatch
formSimilarQuery
SimilarityQueries
AndNots"
Ored
"030"
"admin
"010"
testMustAndMustNot
ANDED"
MUSTs
MustNot"
"MUST_NOT"
"Shoulds
lowerPrice
testJustMustNot
getRangeFilter
ANDED
"020"
accessRights
retrieves
"accessRights"
"admin"
"guest"
upperPrice
"Maybe"
"MUST_NOT
testShouldsAndMustNot
"040"
together"
testShouldsAndMusts
MUST"
testShould
testShouldsAndMust
guest"
getTermsFilter
wins
booleanFilter
"20041231"
testJustMust
testShouldsAndMustsAndMustNot
testShoulds
price
AndNot"
BooleanFilterTest
"MUST"
tstFilterCard
"20051231"
"BoostingQuery
BoostingQueryTest
testBoostingQueryEquals
"subject:"
"java"
TermQueries
Lucene"
"Local
"Oops.
"Filtered
DuplicateFilterTest
"20050102"
urls"
"http://www.bar.com"
"20040102"
testKeepsLastFilter
KEY_FIELD
"Dog
testDefaultFilter
testKeepsFirstFilter
bites
1.9
2.1
dupsFound
1.4.3
testNoFilter
testFastFilter
"jonathan"
variant"
"smythe"
smyth"
"johnathon
"javi"
"johnny
jonathan"
testNoMatchFirstWordBug
"jonathon
testMultiWord
rare
"jonathin
"jonathan
"FuzzyLikeThisQuery
testFuzzyLikeThisQueryEquals
"fernando
FuzzyLikeThisQueryTest
testClosestEditDistanceMatchComesFirst
smoth"
"jonny
"smyth"
smythe"
fltq1
fltq2
"00"
testCachability
cachedFilters
cached"
TermsFilterTest
testBoostFactor
originalValues
release"
getOriginalValues
termBoost
clauses."
TestMoreLikeThis
1804855832182710327L
5962648855261624214L
setBeginLine
getErrorToken
setBeginColumn
errorToken
setErrorToken
8197535103538766773L
"queryBuilder
getSyntaxParser
setQueryNodeProcessor
setSyntaxParser
queryBuilder
"textParser
fieldNameBuilders
actualClass
getBuilder
queryNodeBuilders
queryNodeClass
"<fieldconfig
attributes=\""
listeners
WILDCARD_NOT_SUPPORTED
118496077529151825L
operation='and'>"
operation='and'/>"
minimumMatchingmElements
"<any
"\n</any>"
1000791433562954187L
anySTR
"ANY
minimumMatchingElements
matchelements="
operation='default'>"
operation='default'/>"
2206623652088638072L
3929082630855807593L
"<boost
"\n</boost>"
"[DELETEDCHILD]"
"<deleted/>"
9151675506000425293L
setBegin
3634521145130758265L
setEnd
pi
1794537213032589441L
"<fuzzy
similarity='"
9204673493869114999L
"\n</group>"
"<group>"
term='*'/>"
field='*'
7050381275423477809L
"<matchAllDocs
"<matchNoDocsQueryNode/>"
8081805751679581497L
391209837953928169L
toDigitString
toLargeString
operation='"
leftParenthensis
"MOD_NOT"
"MOD_DEFAULT"
"MOD_NONE"
"<modifier
"MOD_REQ"
"\n</modifier>"
"NOT
rightParenthensis
"[NTF]"
"<notokenfound/>"
7332975497586993833L
schema='"
"<opaque
OpaqueQueryNode
":'"
3692323307688017852L
operation='or'>"
operation='or'/>"
5770038129741218116L
"<parametric
NE
EQ
operator='"
getOperand
">="
"<="
"<parametricRange>\n\t"
"</parametricRange>\n"
7120958816535573935L
"upper
"lower
getPathElement
pathelement
8325921322405804789L
PathQueryNode
getPathString
path='"
setPathElements
getFirstPathElement
"<path
"/\""
getPathElements
QueryText
rValues
elements."
pathElements
"PathQuerynode
localValues
"\n</phraseslop>"
"<phraseslop
PhraseSlopQueryNode
clearFields
"\n</proximity>"
SENTENCE
pDistance
9018220596680832916L
SENTENCE"
inorder='"
"<proximity
distanceSTR
distance='"
getProximityType
PARAGRAPH
pType
withinSTR
"WITHIN
PARAGRAPH"
ProximityQueryNode
type='"
INORDER"
"distance"
proximityType
ProximityType
5569870883474845989L
PLAINTEXT_FIELD_NAME
setParent
"org.apache.lucene.queryParser.messages.QueryParserMessages"
"_plain"
"Q0008E.NODE_ACTION_NOT_SUPPORTED"
localClauses
bundle
"<quotedfield
6675157780051428987L
"\n</slop>"
"<slop
"<tokenizedtphrase>"
"<tokenizedphrase/>"
"[TP["
"\n</tokenizedphrase>"
7185108320787917541L
processIteration
newChildren
childrenListPool
allocateChildrenList
auxList
beingUsed
ChildrenList
2613518456949297135L
orderedChildrenList
removeBoolean
Q1
ANDOperation
Q2
QueryNodeOperation
logicalAnd
enabledChars
processorPipeline
setQueryConfig
getUseOldRangeQuery
setUseOldRangeQuery
"parse
useOldRangeQuery
"<StandardQueryParser
config=\""
TooManyListenersException
AnyQueryNodeBuilder
andNode
boostNode
groupNode
modifierNode
positionTermMap
upperInclusive
lowerInclusive
rangeNode
2804763012723049527L
AllowLeadingWildcardAttributeImpl
allowLeadingWildcard="
"<allowLeadingWildcard
AnalyzerAttributeImpl
analyzer='"
analyzerAttr
"<analyzerAttribute
BoostAttributeImpl
boost="
"<boostAttribute
"<dateResolutionAttribute
DateResolutionAttributeImpl
dateResolution='"
DefaultOperatorAttributeImpl
"<defaultOperatorAttribute
operator="
defaultOperatorAttr
DefaultPhraseSlopAttributeImpl
defaultPhraseSlop="
"<defaultPhraseSlop
"<fieldBoostMapAttribute
FieldBoostMapAttributeImpl
fieldBoostMapAttr
dateResMapAttr
fieldDateResAttr
"<fieldDateResolutionMapAttribute
FieldDateResolutionMapAttributeImpl
"<fuzzyAttribute
FuzzyAttributeImpl
prefixLength="
6804760312720049526L
locale="
"<localeAttribute
localeAttr
LocaleAttributeImpl
LowercaseExpandedTermsAttributeImpl
"<lowercaseExpandedTerms
lowercaseExpandedTerms="
2804760312723049527L
fields="
fieldsAttr
6809760312720049526L
MultiFieldAttributeImpl
"<fieldsAttribute
"MultiTermRewriteMethodAttribute"
multiTermRewriteMethod="
"<multiTermRewriteMethod
2104763012723049527L
MultiTermRewriteMethodAttributeImpl
PositionIncrementsAttributeImpl
positionIncrementsEnabled="
2804763012793049527L
"<positionIncrements
rangeCollatorAttr
"<rangeCollatorAttribute
rangeCollator='"
RangeCollatorAttributeImpl
557816496416587068L
2138501723963320158L
"<multiPhrase>"
"\n</multiPhrase>"
"<multiPhrase/>"
"[MTP["
6851557641826407515L
"<prefixWildcard
"<range>\n\t"
7400866652044314657L
"</range>\n"
1938287817191138787L
"<wildcard
escapableTermChars
escapeChar
"INORDER"
"\b"
escapableTermExtraFirstChars
copyStart
escapeWhiteChar
escapableQuotedChars
escapableWordTokens
"\r"
escapeTerm
escapeQuoted
replaceIgnoreCase
"\u3000"
wildcardChars
sequence1
"SENTENCE"
"\f"
sequence1Length
escapableWhiteChars
"PARAGRAPH"
ReadByte
hexval
AdjustBuffSize
nextCharInd
nextCharBuf
backSlashCnt
initialise
"Error"
0x762000
ConjQuery
0x440000
0x60000000
0x760000
DisjQuery
ModClause
0x763c00
qUpper
qLower
addElement
0x6000000
querynode
0x800000L
0x7fffff01L
0x8000000L
alwAttr
termGroupCount
newFieldNode
fuzzyAttr
actualQueryNodeList
latestNodeVerified
applyModifier
"DefaultOperatorAttribute
usingAnd
readTree
queryNodeList
"MultiFieldAttribute
"MultiTermRewriteMethodAttribute
parametricRangeNode
childrenBuffer
grandChildren
wildcardQN
isPrefixWildcard
prefixWildcardQN
isWildcard
nodeA
nodeB
testAddChildren
TestQueryNode
fieldQueryNode
"field:term"
parenthesized
"anotherField:term"
term2])"
"a~0.5"
testOrSpans
testQueryValidator
"term1
testTermSpans
"QueryNodeException
TestSpanQueryParser
"spanOr([term1,
"a^0.5"
testUniqueField
"index:text"
"body:text"
spanquery
TestSpanQueryParserSimpleSample
testBasicDemo
8553318595851064232L
"<uniqueField
UniqueFieldAttributeImpl
uniqueField='"
uniqueFieldAttr
UniqueFieldQueryNodeProcessor!"
"UniqueFieldAttribute
TestMultiAnalyzerQPHelper
TestMultiAnalyzerWrapper
TestMultiFieldQPHelper
TestMultiFieldQueryParserWrapper
CannedAnalyzer
testConstantScoreAutoRewrite
CannedTokenStream
"\"a\""
alsoIgnored
"foo*"
"foo*bar"
assertQueryNodeException
"*leadingWildcard"
newProcessorPipeline
MATCH_CASEINDEPENDENT
FLAG_MATCH_NORMAL
MATCH_NORMAL
FLAG_CANON_EQ
FLAG_MULTILINE
FLAG_LITERAL
FLAG_DOTALL
FLAG_UNIX_LINES
FLAG_COMMENTS
FLAG_UNICODE_CASE
rewritten"
"spanRegexQuery("
getProgram
REProgram
testShakyPrefix
"(ab|ac)"
TestJakartaRegexpCapabilities
"luce"
"luc[e]?"
cap
testGetPrefix
testJakartaCaseSensativeFail
"foo.*"
spanRegexQueryNrHits
regex2
testMatchAll
"jum."
regex1
testJavaUtilCaseInsensative
srq1
testRegex1
testRegex3
testRegex2
testJavaUtilCaseSensativeFail
"^.*DOG.*$"
FN
TestRegexQuery
capability
"^.[aeiou]c.*$"
regexQueryNrHits
srq2
testJakartaCaseInsensative
testSpanRegex1
testSpanRegex2
"^q.[aeiou]c.*$"
"^q.[aeiou]c$"
update"
TestSpanRegexQuery
"a2
arrSearcher
srq
testSpanRegex
testSpanRegexBug
createRAMDirectories
b1"
auto
"aut.*"
setSecurityManager
"//localhost/Searchable"
UnicastRemoteObject
RMISecurityManager
org.apache.lucene.search.RemoteSearchable
<index>"
Remote
cwfh
testTermRemoteFilter
rcwfh
TestRemoteCachingWrapperFilter
hitNumber
typeValue
testQueryFilter
TestRemoteSearchable
nohits
"non-existent-term"
testConstantScoreQuery
500000
testRemoteSort
"server"
Locale.US"
Basics"
TestRemoteSort
"/SortedSearchable"
geoHashField
geoHashValues
lngInterval
BASE_32
PRECISION
DECODE_MAP
isEven
BITS
latInterval
2D
deltaX
"Point("
deltaY
MILES_KILOMETRES_RATIO
earthCircumference
24902
3959
earthCircumfence
getUnit
40076
earthRadius
6371
fixed
lattitude"
setLng
setLat
SCALE_FACTOR
fixedToDouble
toCartesian
3.14159265358979323846
ll2
ll1
radians
cosB
lUnits
0.01745329251994
fromCartesian
dLon
39.674504f
65.402355f
68.907753f
45.379915f
getMilesPerLatDeg
52.203713f
28.134073f
setTestPoint
KILOMETERS_PER_MILE
v1x
v1y
25.911621f
58.660106f
6.028572f
38.679582f
1.000000f
59.903632f
4.825062f
63.190698f
57.345111f
21.374762f
61.631630f
50.588151f
68.655373f
59.290899f
m_mpd
69.002475f
2.414002f
getDistanceSq
36.654698f
1.609347f
51.403761f
8.429716f
34.585159f
68.497792f
24.788387f
v2x
55.242144f
33.534429f
67.116253f
13.198283f
7.230245f
DistanceApproximation
64.576564f
63.672096f
67.900079f
9.626619f
69.076177f
66.813976f
67.398085f
14.381280f
22.519612f
53.755675f
v2y
12.011266f
56.661310f
31.402650f
m_milesPerLngDeg
1.207185f
60.498118f
32.473485f
35.625354f
lngMiles
67.659387f
69.128838f
19.065881f
17.902554f
15.559897f
65.785428f
40.657342f
68.319345f
58.011443f
20.223401f
16.733774f
48.910956f
61.074176f
46.284093f
getMilesPerLngDeg
49.757131f
64.134098f
30.322249f
54.507211f
23.657602f
41.627796f
66.491346f
29.232613f
43.530372f
latMiles
66.148462f
44.461915f
62.170310f
42.585570f
m_testLng
MILES_PER_LATITUDE
68.120088f
64.999359f
69.160441f
55.960250f
3.620083f
m_testLat
62.690052f
37.672877f
52.987764f
27.026963f
69.170976f
10.820591f
68.792041f
47.174172f
48.049882f
k3
lr
dy
pt1
pt0
SQR
ul
q0
angleRad
angle
Ellipse
closestPt
P
normLng
getMidpoint
45.0
225.0
normLat
heightMi
boxCorners
widthMi
brngdeg
3963.0
R
lon1
lon2
brng
getMinPoint
ptMin
ptMax
getMaxPoint
normSqr
getShapeLoop
latY
latX
BigDecimal
itY
longX2
MILES_FLOOR
beginAt
tierPrefix
xInc
startX
startY
endX
tierVert
box1
RoundingMode
endY
setScale
longY
longX
endAt
HALF_EVEN
DistanceScoreDocLookupComparator
cleanUp
dsdlc
startingFilter;
QueryWrapperFilter(MatchAllDocsQuery)
no-op
setDistances
docd
1234.123456789
thisPrecise
centerLat
TWENTYFEET
getPrecision
xLng
xLat
TWOHUNDREDFEET
TWOFEET
EXACT
centerLng
"DistanceQuery
lng:
needPrecise
cartesianFilter
tierFieldPrefix
geoHashFieldPrefix
getMiles
getDistanceFilter
miles:
getBoundary
box
latIndex
lngIndex
ck
nextOffset
isInside
tierLength
corner
setTierVerticalPosDivider
tierVerticalPosDivider
tierLen
circ
"_tier_"
setTierLength
tierBoxes
idd
fieldPrefix
tierLevel
getBoxId
28892
setTierBoxes
2.0d
nlat
rlat
0.000001d
latitudeLongitude
"ezs42e44yx96"
testDecodePreciseLongitudeLatitude
"u4pruydqqvj8"
57.64911
"u173"
5.6
testDecodeImpreciseLongitudeLatitude
84.6
52.3738007
0.00001D
TestGeoHashUtils
42.6
"u173zq37x014"
10.40744
4.8909343d
testEncode
testDecodeEncode
52.37380061d
4.8909347
testConvert
testFindDistanceUnit
"mls"
testFindDistanceUnit_unknownUnit
0D
TestDistanceUnits
"####.000000"
DistanceCheck
formatDistance
long2
long1
"_localTier"
PolyShape
llm
171.7
171.2
testGeoHashRange
harvesine):"
"Name:
55.0
41.6032207
"Distance
lastDistance
"Results:
"testDistances"
rsLng
ctps
fsQuery
7.06
Pole
73.087749
"_geoHash_"
"geo_distance"
TestCartesian
milesToTest
ctpsize
"Results
testPoleFlipping
"testPoleFlipping"
"Distances
dsort
(res,
Island"
p3
p4
setUpPlotter
"============================="
Distance
158.0
2800.0
3500.0
"Midway
Island
|"
Way"
geo_distance
testDistances
25.7
21.6032207
2288.82495932794
ortho,
filtered:
3474.331719997617
rsLat
"miles:"
testAntiM
geoHashPrefix
"testAntiM"
Airfield"
raised
serializable
NotSerializableException
TestCartesianShapeFilter
testLatLongFilterOnDeletedDocs
"Potomac"
TestDistance
matchIndexes
transpositions
jw
xi
xn
matchFlags
mtp
mi
setThreshold
LuceneIterator
ni
ec
tn
fileIterator
bStart
goalFreq
aSearcher
ng1
ng2
obtainSearcher
releaseSearcher
swapSearcher
maxHits
bEnd
gram
spellIndexDir
"Spellchecker
lengthWord
sugWord
morePopular
F_WORD_TERM
formGrams
ramMB
grams
addGram
modifyCurrentIndexLock
0.961
0.962
TestJaroWinklerDistance
0.841
0.959
0.84
0.958
0.813
0.814
0.832
0.833
TestLevenshteinDistance
"Jerry"
testFieldAaa
ld
be."
"Tom"
testSpellchecker
testFieldZzz
"nonexistent_field"
TestLuceneDictionary
testFieldContents_1
"Jarry"
testFieldContents_2
"Tam"
"Nonexistent
testFieldNonExistent
"natural"
0.5625
testGetDistance1
0.625
0.7222
nsd
0.5277
0.5833
0.4762
"0012890678"
TestNGramDistance
0.2083
"0072385698"
0.4583
"contrary"
testGetDistance3
testGetDistance2
"12345678"
001
"72385698"
"12890678"
"0012345678"
0.25
"treeword"
ptd
"oneword"
"twoword"
"threeword"
LF
TestPlainTextDictionary
addwords
workers
SpellCheckWorker
"worker
spellCheckWorker
"fives"
terminated"
"spellchecker
"tousand"
numSearchers
"fiv"
testConcurrentAccess
checkCommonSuggestions
NPE,
checkLevenshteinSuggestions
terminated
assertSearchersClosed
assertLastSearcherOpen
0.8f
spellindex
"onety"
checkJaroWinklerSuggestions
numdoc
%d
userindex
num_field1
num_field2
checkNGramSuggestions
searcherArray
SpellCheckerMock
"threw
60L
orToken
NQuery
carat
TopSrndQuery
getOrQuery
dt
nrNormalChars
0x1000
suffixed
minimumCharsInTrunc
distq
allowedTruncation
oprt
PrimaryQuery
getTruncQuery
unrestrictive
label_7
0x7c3b00
label_8
allowedSuffix
minimumPrefixLength
OptionalFields
boostErrorMessage
0x7c0000
PrefixOperatorQuery
truncationErrorMessage
checkDistanceSubQueries
truncation:
floatExc
getNotQuery
dToken
getAndQuery
0x1b00
fieldOperator
notToken
getFieldsQuery
anyChar
getTermQuery
parse2
FieldsQueryList
getDistanceQuery
andToken
distanceOp
WQuery
OptionalWeights
_STAR
"<W>"
"<TRUNCQUOTED>"
"<N>"
_ONE_CHAR
"\"?\""
"<_DISTOP_NUM>"
"<TRUNCTERM>"
_DISTOP_NUM
"<SUFFIXTERM>"
"\",\""
"\54"
0x8000040000000000L
0x3fc000000000000L
0xffffffffbfffffffL
0xffff01L
0x7bffe8faffffd9ffL
0x400000004000L
0x80000000800000L
0xfbffecfaffffd9ffL
inf
checkMax
getNrQueriesMade
getMaxBasicQueries
queriesMade
getBracketClose
getPrefixSeparator
qn
isOperatorInfix
getBracketOpen
subqueries"
prefixToString
getSubQuery
recompose
operatorInfix
infixToString
opDistance
qi
dsq
getSpanNearQuery
subQueriesOrdered
spanNearClauses
expanded:
makeLuceneQueryNoBoost
fieldNamesToString
OrOperatorName
fieldOp
fni
allowed:
"subquery
ost
getQuote
isQuoted
addSpanQueryWeighted
SpanNearQuery:
getTermEnum
weightBySpanQuery
subqueries:
getSuffixOperator
getLucenePrefixTerm
getWeightString
isWeighted
cns
getWeightOperator
getLuceneTerm
getTruncated
matchingChar
truncatedToPrefixAndPattern
appendRegExpForChar
queries."
totalMatched
checkNrHits
dBase
TestCollector
testCase
expectedDocNrs
failQueries
as:
query:\n"
"\nParsed
getFieldname
fName
"a*b?"
xyz,def)"
"OR(word2)"
"AND(word2,word1,)"
,)"
"a???b"
"AND(word2)"
word1
ord+,
"or(word2+
Test01Exceptions
OR"
test01Exceptions
AND"
"AND(word2,)"
NOT"
,"
"ab?"
"OR(word2
"word?"
and(xyz,def))"
"wor*"
test02Terms20
test02Terms21
test02Terms22
test02Terms23
Test02Boolean
"abc?"
"word1"
test05Not02
test03And02
"or(
word2*
defg)"
"word*"
"ord2"
test05Not01
ord*,
"and(word1,word2)"
"w*rd?"
test04Or01
"w?rd?"
test04Or02
test03And01
test03And03
(word1,
"w?da?"
test06AndOr01
"w*?da?"
test07AndOrNot02
test02Terms14
test02Terms10
test02Terms13
"kxork*"
"w*rd??"
or(word2,xyz,
word2)"
ab)and
"(word1
test02Terms06
test02Terms04
test02Terms05
test02Terms02
test02Terms03
test04Or03
test02Terms01
normalTest1
orda3"
"(aa
orda3)
gradient*))),"
test3Example01
"2N(w1,w2,w3)"
99n
db3
test1Ntrunc09
test2Wnested01
test1Ntrunc02r
2W
distanceTst
Test03Distance
test1Ntrunc02
2N
db2
precipitation
word*"
inversion
2w
test2Wprefix02
test2Wprefix01
docs3
(bi:cc)"
test2Nprefix01a
test0N04r
"2W(w1,w2,w3)"
"N(w1,
distanceTest3
test0W01
bi:cc"
(invers*
depression
test0W06
gradient
test1Ntrunc06r
3w
(bb
test2Wnested02
3n
test2Nprefix02a
test2Nprefix02b
(w1,
distanceTest2
distanceTest1
test00Exceptions
test0N03r
pressure*)
"50n((low
test0N01
test0N04
test0W04
"low
(negativ*
word2*"
test2Nprefix01b
test0W05
test1Ntrunc04
test1Ntrunc05
test1Ntrunc06
test1Ntrunc03
test0W03
test0W02
"N(w3,
w3)"
dd)"
test2Nnested02
test2Nnested01
test1Wtrunc02r
dd))
worda3)"
"5n(temperat*,
bb)
word1*"
(cc
w1,
precipitat*)"
test0N01r
"(orda2
kxor*"
test0N03
(word2
bi:bb)
"ord*
"2N(w2,w3,w1)"
"W
"word1*
"when
test1Wtrunc09
test1Wtrunc08
test1Wtrunc07
test1Wtrunc06
test1Wtrunc05
test1Wtrunc04
test1Wtrunc03
test1Wtrunc02
test1Wtrunc01
"rain*
depression*,"
(bi:bb))
height
temperature
"kxork*
rain"
w2,
ListDataEvent
addListDataListener
ListDataListener
setListModel
intervalRemoved
listModelListener
intervalAdded
somethingChanged
ListModelHandler
contentsChanged
newModel
"FIELD_NAME"
removeListDataListener
tableChanged
TableModelHandler
TableModelListener
tableModelListener
setTableModel
getTableModel
removeTableModelListener
fireTableStructureChanged
columnValue
TableModelEvent
addTableModelListener
fireTableChanged
toAdd
"Zip"
"Type"
restaurantInfo
"Phone"
rowIndex
"City"
"State"
columnIndex
"Street"
columnNames
"Steak"
11th
201-5438"
"(954)
"Cuban"
746-7865"
versailles
"Aventura"
"10018"
354-8885"
"21
"12115
"348
Merais"
12th
"110
203rd
laCaretta3
restaurants
outback4
outback3
outback2
leMerais
pinos
"12123
107th
"La
533-6522"
23rd
Chris
laCaretta2
"10117
"23543
"Ruth's
"Canoli's"
136th
"Rancha
"31224
"Outback"
ITALIAN_CATEGORY
"33176"
York"
234-5543"
"Picadillo"
342-9876"
CUBAN_CATEGORY
"32154"
"Le
221-3312"
"312
85th
"FL"
Seakhouse"
getNextId
Street"
picadillo
"Cafe
STEAK_CATEGORY
"(212)
682-9876"
"Miami"
"10
"11
777-4384"
Versailles"
"12365
laCaretta
"109
"(305)
Luna"
556-9876"
105th
outback
111-2222"
207th
ranchaLuna
244-7623"
46th
Street
Carretta"
654-9187"
8th
"Search:
JList
JLabel
insertUpdate
removeUpdate
DocumentListener
addDocumentListener
ListSearcherSimulator
DocumentEvent
changedUpdate
street
phone
searchButton
addActionListener
ActionListener
"Go"
JTable
requestFocus
searchTableModel
actionPerformed
searchListener
TableSearcherSimulator
ActionEvent
JButton
TestBasicList
TestBasicTable
testColumns
TestSearchingList
TestSearchingTable
TestUpdatingList
TestUpdatingTable
collapseTokens
collapseAndSaveTokens
tmpTokType
theStart
setupToken
currPos
"sh"
numAdded
untokenizedTypes
"elu"
setupSavedToken
tokenOutput
"\15\127\3\0\3\127\3\0\1\177\1\0\1\102\2\200"
"\0\u1810\0\u183c\0\u1868\0\u1894\0\u18c0\0\u18ec\0\u1918\0\u1944"
"\1\145\2\146\1\147\7\0\15\145\3\0\3\145\16\0"
"\2\77\1\0\1\100\3\0\1\100\1\17\1\20\1\21"
"\3\0\1\100\2\0\2\100\1\0\1\100\3\0\1\100"
7040
"\0\u157c\0\u15a8\0\u15d4\0\u1600\0\u162c\0\u1658\0\u1684\0\u16b0"
"\3\0\3\70\14\0\1\36\1\0\4\123\1\0\3\124"
"\16\0\4\132\7\0\15\132\3\0\3\132\16\0\4\222"
"\15\134\3\0\3\134\3\0\1\204\2\0\1\204\7\0"
"\3\31\10\27\1\30\5\27\4\34\1\27\1\32\3\27"
"\3\0\3\40\2\0\1\114\67\0\4\44\7\0\15\44"
"\5\36\4\44\1\36\1\45\2\36\1\46\2\36\15\44"
"\15\62\2\56\1\65\3\62\1\56\55\0\1\66\62\0"
"\u026c\15\12\17\166\15\12\17\166\15\12\17\166\15\12\17\166\15\12\17"
THREE_SINGLE_QUOTES_STATE
"\5\36\4\54\1\36\1\45\5\36\15\54\1\36\1\55"
"\167\15\11\17\166\15\12\17\166\15\12\17\166\15\12\17\340\15\12\17"
"\4\135\7\0\15\135\3\0\3\135\3\0\1\176\1\0"
"\1\10\1\21\1\10\4\22\1\23\1\22\1\24\1\25"
"\0\u01b8\0\u01b8\0\u0aa8\0\u0ad4\0\u0b00\0\u0b00\0\u0b2c\0\u0b58"
"\1\67\4\0\4\70\7\0\6\70\1\71\6\70\3\0"
"\32\0\1\241\2\0\4\233\7\0\15\233\3\0\3\233"
"\1\0\3\124\3\0\15\123\3\0\3\123\16\0\1\220"
"\1\61\3\56\4\62\1\56\1\63\2\56\1\64\2\56"
"\7\0\15\54\3\0\3\54\47\0\1\111\6\0\1\120"
DOUBLE_BRACE_STATE
"\0\0\0\54\0\130\0\204\0\260\0\334\0\u0108\0\u0134"
"\7\0\15\20\3\0\3\20\2\0\1\73\1\105\1\75"
"\3\1\3\0\1\1\2\0\1\11\30\0\1\1\2\0"
"\1\132\1\220\1\132\7\0\15\220\3\0\3\220\16\0"
"\0\u070c\0\u0738\0\u0764\0\u0790\0\u01b8\0\u01b8\0\u07bc\0\u07e8"
"\4\31\1\27\1\32\3\27\1\33\1\27\15\31\3\27"
"\0\u01b8\0\u16dc\0\u1708\0\u1734\0\u1760\0\u178c\0\u17b8\0\u17e4"
"\1\36\1\0\4\123\1\0\3\124\3\0\10\123\1\174"
"\1\102\2\103\1\0\1\104\3\0\1\104\3\20\1\22"
"\1\0\4\123\1\0\3\124\3\0\3\123\1\213\11\123"
"\3\0\3\173\12\0\1\172\1\0\1\217\1\0\4\123"
"\0\u1970\0\u199c\0\u19c8\0\u19f4\0\u1a20\0\u1a4c\0\u1a78\0\u1aa4"
"\1\46\102\0\27\15\1\0\37\15\1\0\u0568\15\12\17\206\15\12\17"
"\0\u1340\0\u136c\0\u1398\0\u13c4\0\u086c\0\u09f8\0\u13f0\0\u141c"
"\0\u0e18\0\u0e44\0\u0e70\0\u0e9c\0\u0ec8\0\u0ef4\0\u0f20\0\u0f4c"
"\3\36\3\47\10\36\1\37\1\36\1\51\3\36\4\52"
"\3\0\3\44\24\0\1\36\55\0\1\115\43\0\4\47"
"\1\11\5\0\1\31\1\41\1\42\1\50\3\0\1\11"
"\15\40\3\36\1\42\2\40\2\36\1\43\5\36\1\37"
"\34\0\1\237\31\0\1\172\1\0\1\111\1\0\4\123"
"\16\0\1\155\2\130\1\132\7\0\15\155\3\0\3\155"
"\4\152\7\0\15\152\3\0\3\152\3\0\1\205\1\0"
"\1\211\1\40\31\0\1\164\54\0\1\212\35\0\1\36"
"\1\171\34\0\1\172\1\0\1\36\1\0\4\123\1\0"
"\4\0\1\1\2\0\2\1\2\0\1\1\5\0\1\11"
"\0\u10d8\0\u01b8\0\u1104\0\u1130\0\u115c\0\u1188\0\u01b8\0\u11b4"
"\3\0\1\100\1\142\2\143\1\144\7\0\15\142\3\0"
"\1\0\3\124\3\0\10\123\1\230\4\123\3\0\3\123"
"\5\36\4\40\1\36\1\32\2\27\1\36\1\41\1\36"
"\4\140\7\0\15\140\3\0\3\140\16\0\4\144\7\0"
"\1\0\3\124\3\0\15\123\3\0\3\123\34\0\1\240"
"\1\154\1\144\7\0\15\153\3\0\3\153\3\0\1\206"
"\1\153\1\143\1\154\1\144\7\0\15\153\3\0\3\153"
"\1\26\3\0\1\27\14\0\1\30\1\31\1\32\1\33"
"\1\76\2\103\1\0\1\104\3\0\1\104\1\21\1\20"
"\1\0\3\124\3\0\3\123\1\125\11\123\3\0\3\123"
"\1\27\1\0\1\7\1\11\1\13\1\53\1\4\2\15\1\30\5\15"
"\35\0\1\242\62\0\1\243\20\0\1\244\77\0\1\245"
"\1\0\14\42\1\41\3\0\1\11\1\44\3\0\1\45"
"\64\0\1\210\26\0\4\40\7\0\15\40\3\0\1\40"
"\1\102\2\77\1\0\1\100\3\0\1\100\4\147\7\0"
"\3\0\3\150\3\0\1\104\2\0\2\104\1\0\1\104"
"\3\133\16\0\1\134\1\135\1\134\1\135\7\0\15\134"
"\43\0\1\265\26\0\2\262\1\0\2\262\1\0\2\262"
"\3\124\3\0\15\173\3\0\3\173\12\0\1\172\1\0"
"\0\u0ce4\0\u0d10\0\u0898\0\u0d3c\0\u0d68\0\u0d94\0\u0dc0\0\u0dec"
"\2\0\1\204\7\0\1\134\1\135\1\134\1\135\7\0"
"\0\u03c8\0\u03f4\0\u0420\0\u044c\0\u0478\0\u01b8\0\u039c\0\u04a4"
"\1\13\1\14\1\10\1\15\1\16\1\15\1\17\1\20"
"\3\0\15\123\3\0\3\123\14\0\1\36\1\0\4\123"
"\1\236\60\0\4\40\6\0\1\225\15\40\3\0\3\40"
"\2\15\1\33\1\40\1\34\1\50\1\41\4\15\1\42\1\35\1\51"
"\1\13\1\14\5\13\1\15\1\13\1\16\3\13\1\17"
"\53\0\1\253\54\0\1\254\61\0\1\255\11\0\1\256"
"\2\0\5\262\7\0\15\262\3\0\4\262\27\0\1\266"
"\1\0\1\102\2\77\1\0\1\100\3\0\1\100\4\22"
"\3\137\3\0\1\176\1\0\1\102\2\176\6\0\4\140"
"\0\u0948\0\u0974\0\u09a0\0\u09cc\0\u09f8\0\u0a24\0\u0a50\0\u0a7c"
"\73\0\1\110\16\0\1\67\4\0\4\70\7\0\15\70"
"\33\0\1\227\32\0\1\172\1\0\1\36\1\0\4\123"
"\1\201\3\0\1\201\1\131\1\130\1\131\1\132\7\0"
"\14\0\2\1\2\11\1\1\1\0\2\1\1\0\1\1"
"\3\0\3\34\27\0\1\112\42\0\4\40\7\0\15\40"
"\0\u0160\0\u018c\0\u01b8\0\u01e4\0\u0210\0\u023c\0\u0268\0\u0294"
INTERNAL_LINK_STATE
"\3\0\3\40\16\0\4\40\7\0\2\40\1\113\12\40"
"\53\0\1\246\32\0\1\36\1\0\4\173\1\0\3\124"
numBalanced
"\1\203\1\0\1\102\2\176\6\0\1\155\2\130\1\132"
"\3\36\3\44\10\36\1\37\5\36\4\47\1\36\1\45"
"\3\0\1\106\1\0\1\102\2\77\1\0\1\100\3\0"
"\1\36\3\54\1\36\1\56\1\57\5\56\1\60\1\56"
"\3\0\3\70\16\0\4\31\7\0\15\31\3\0\3\31"
TWO_SINGLE_QUOTES_STATE
"\15\17\1\26\2\13\3\17\1\13\7\27\1\30\5\27"
"\2\40\1\161\12\40\3\0\3\40\2\0\1\162\101\0"
"\1\102\2\176\6\0\1\127\1\130\1\131\1\132\7\0"
"\7\0\15\262\3\0\3\262\40\0\1\263\53\0\1\264"
"\0\u02c0\0\u02ec\0\u01b8\0\u0318\0\u0344\0\u0370\0\u01b8\0\u039c"
"\0\u0f78\0\u0fa4\0\u0fd0\0\u0ffc\0\u1028\0\u1054\0\u1080\0\u10ac"
"\1\147\7\0\15\146\3\0\3\146\3\0\1\77\1\0"
numLinkToks
"\1\201\3\0\1\201\3\137\1\140\7\0\15\137\3\0"
"\1\0\1\102\2\103\1\0\1\104\3\0\1\104\1\154"
"\1\41\21\15\1\25\1\0\1\26\1\0\1\6\1\0\1\31\1\43"
"\3\0\3\136\16\0\1\100\2\141\10\0\15\100\3\0"
numWikiTokensSeen
"\20\21\u0100\0\200\21\200\0\u19c0\21\100\0\u5200\21\u0c00\0\u2bb0\20\u2150\0"
"\1\21\1\22\7\0\15\21\3\0\3\21\3\0\1\106"
"\2\36\1\46\2\36\15\47\3\36\3\47\10\36\1\37"
"\3\0\1\40\1\41\2\42\1\41\2\43\2\0\1\42"
"\3\147\16\0\4\152\7\0\15\152\3\0\3\152\16\0"
"\0\u11e0\0\u120c\0\u1238\0\u1264\0\u1290\0\u12bc\0\u12e8\0\u1314"
"\1\20\1\21\1\22\1\23\1\24\2\13\1\25\2\13"
"\3\0\3\130\3\0\1\202\1\0\1\102\2\200\1\0"
"\0\u1ad0\0\u1afc\0\u1b28\0\u1b54\0\u01b8\0\u01b8\0\u01b8"
"\16\0\1\127\1\130\1\131\1\132\7\0\15\127\3\0"
"\24\0\1\27\56\0\1\111\42\0\4\34\7\0\15\34"
"\1\0\1\1\3\0\7\1\2\0\1\1\1\0\15\1"
DOUBLE_EQUALS_STATE
"\33\0\4\251\7\0\15\251\3\0\3\251\36\0\1\252"
"\0\u0814\0\u01b8\0\u0840\0\u086c\0\u0898\0\u08c4\0\u08f0\0\u091c"
"\1\220\1\132\1\220\1\132\7\0\15\220\3\0\3\220"
"\3\141\3\0\1\106\1\0\1\102\2\77\1\0\1\100"
"\1\100\1\221\1\144\1\221\1\144\7\0\15\221\3\0"
"\1\163\1\164\40\0\4\70\7\0\6\70\1\165\6\70"
"\3\221\3\0\1\204\2\0\1\204\7\0\4\222\7\0"
"\1\53\2\0\1\3\1\1\4\0\1\14\1\5\1\2\1\10\12\16"
"\4\0\4\70\7\0\15\70\3\0\3\70\16\0\4\54"
"\2\0\1\231\104\0\1\232\36\0\4\233\7\0\15\233"
"\7\0\15\47\3\0\3\47\26\0\1\116\37\0\1\111"
"\1\36\1\45\5\36\15\52\3\36\3\52\10\36\1\53"
"\3\70\12\0\1\72\43\0\1\73\1\74\1\75\1\76"
EXTERNAL_LINK_STATE
"\u0200\21\u0465\0\73\21\75\15\43\0"
"\7\0\15\222\3\0\3\222\33\0\1\223\61\0\1\224"
"\0\u1448\0\u1474\0\u14a0\0\u14cc\0\u14f8\0\u1524\0\u01b8\0\u1550"
"\3\0\1\104\3\143\1\144\7\0\15\143\3\0\3\143"
"\1\15\1\36\1\52\1\32\3\15\1\44\1\37\1\15\1\45\1\47"
"\16\0\1\126\1\0\1\126\10\0\15\126\3\0\3\126"
"\1\0\1\201\3\0\1\201\3\141\10\0\15\141\3\0"
"\1\0\1\102\2\77\1\0\1\100\3\0\1\100\1\145"
"\24\0\1\56\55\0\1\122\43\0\4\70\7\0\15\70"
"\1\0\1\201\3\0\1\201\3\130\1\132\7\0\15\130"
CATEGORY_STATE
"\1\221\1\144\1\221\1\144\7\0\15\221\3\0\3\221"
"\12\0\1\11\7\1\1\11\3\1\1\11\6\1\1\11"
"\166\15\12\17\u0166\15\12\17\266\15\u0100\15\u0e00\15\u1040\0\u0150\21\140\0"
"\2\0\1\51\30\0\1\52\2\0\1\53\1\54\1\55"
"\3\0\3\233\3\0\1\175\1\0\1\102\2\176\6\0"
"\2\146\1\147\7\0\15\145\3\0\3\145\3\0\1\103"
"\1\100\4\144\7\0\15\144\3\0\3\144\3\0\1\77"
"\3\0\1\1\1\11\3\0\1\1\1\11\5\0\1\1"
"\0\u01b8\0\u04d0\0\u04fc\0\u0528\0\u0554\0\u0580\0\u05ac\0\u05d8"
"\15\144\3\0\3\144\16\0\4\147\7\0\15\147\3\0"
"\15\222\3\0\3\222\34\0\1\234\55\0\1\235\26\0"
"\1\126\10\0\15\126\3\0\3\126\3\0\1\175\1\0"
"\30\0\1\156\1\157\64\0\1\160\27\0\4\40\7\0"
"\12\0\4\1\4\2\1\3\1\1\1\4\1\1\2\5"
"\63\0\1\121\57\0\4\62\7\0\15\62\3\0\3\62"
"\1\102\2\176\6\0\1\136\2\137\1\140\7\0\15\136"
"\3\124\3\0\15\123\3\0\3\123\16\0\4\173\1\0"
"\1\22\7\0\15\17\3\0\3\17\3\0\1\101\1\0"
"\1\143\1\154\1\144\7\0\15\154\3\0\3\154\3\0"
"\3\0\3\70\2\0\1\166\63\0\1\167\71\0\1\170"
"\2\1\1\11\14\1\1\11\6\1\2\11\3\0\1\11"
"\3\0\3\134\16\0\1\136\2\137\1\140\7\0\15\136"
"\3\142\3\0\1\101\1\0\1\102\2\103\1\0\1\104"
"\3\0\3\123\2\0\1\214\102\0\1\171\54\0\1\215"
"\1\150\2\151\1\152\7\0\15\150\3\0\3\150\16\0"
"\1\35\1\27\15\34\3\27\3\34\1\27\7\36\1\37"
"\7\0\15\155\3\0\3\155\31\0\1\157\54\0\1\207"
"\3\100\16\0\1\142\2\143\1\144\7\0\15\142\3\0"
"\5\36\4\47\1\36\1\45\2\36\1\50\2\36\15\47"
"\3\0\15\173\3\0\3\173\36\0\1\247\53\0\1\250"
"\30\0\4\40\6\0\1\225\15\40\3\0\2\40\1\226"
"\3\0\3\136\3\0\1\200\1\0\1\102\2\200\1\0"
"\7\0\15\140\3\0\3\140\3\0\1\201\2\0\2\201"
"\34\0\1\216\52\0\1\172\3\0\4\173\7\0\15\173"
"\11\0\1\24\1\23\1\0\1\24\1\22\22\0\1\24\1\0\1\12"
currentTokType
"\1\6\2\5\1\7\1\5\2\10\1\11\1\12\1\11"
"\15\131\3\0\3\131\3\0\1\203\1\0\1\102\2\176"
"\12\0\4\251\7\0\15\251\3\0\3\251\37\0\1\257"
"\6\0\4\132\7\0\15\132\3\0\3\132\3\0\1\204"
"\15\147\3\0\3\147\3\0\1\100\2\0\2\100\1\0"
"\53\0\1\267\24\0"
"\1\100\3\0\1\100\1\150\2\151\1\152\7\0\15\150"
"\1\11\1\0\1\34\1\35\1\0\1\36\1\0\1\37"
"\7\0\15\22\3\0\3\22\24\0\1\13\55\0\1\107"
"\3\127\16\0\1\133\1\0\1\133\10\0\15\133\3\0"
"\1\102\2\77\1\0\1\100\3\0\1\100\1\153\1\143"
"\3\0\1\104\3\151\1\152\7\0\15\151\3\0\3\151"
"\53\0\1\260\54\0\1\261\22\0\1\13\62\0\4\262"
FIVE_SINGLE_QUOTES_STATE
"\3\0\1\205\1\0\1\102\2\77\1\0\1\100\3\0"
"\1\0\1\102\2\103\1\0\1\104\3\0\1\104\3\146"
"\3\142\16\0\4\135\7\0\15\135\3\0\3\135\16\0"
"\57\0\4\52\7\0\15\52\3\0\3\52\11\0\1\117"
"\4\123\3\0\3\123\2\0\1\73\13\0\1\126\1\0"
"\3\11"
"\0\u0604\0\u0630\0\u065c\0\u0688\0\u06b4\0\u01b8\0\u06e0\0\u039c"
"\0\u0b84\0\u0bb0\0\u0bdc\0\u0c08\0\u0c34\0\u0c60\0\u0c8c\0\u0cb8"
"\1\46\5\0\1\47\4\0\1\47\2\0\2\47\2\0"
"https://lucene.apache.org/java/docs/index.html?b=c"
"and2"
here]]
"==heading==
"http://lucene.apache.org/java/docs/index.html#news"
testLinkPhrases
[[:Category:bar
here]"
"goes"
"italics
here]
[[link
click
head===
goes
WikipediaTokenizerTest
$3.25
numCitation"
[http://lucene.apache.org
followed
===sub
LINK_PHRASES
numBoldItalics
[[Category:foo]]"
[https://lucene.apache.org/java/docs/index.html?b=c
untoks
none
testLinks
something
'''bold'''
"quotes"
[http://foo.boo.com/test/test/test.html?g=b&c=d
[[Category:foo]]
[http://lucene.apache.org]
"martian"
[[link]]
italics'',
"Johnny"
closed."
132
testBoth
g]]
"[[link]]
"ital"
"Test"
"Here's"
d]]"
"withstanding"
"Category"
"display"
"3.50"
"parens"
here''
Test]
equals:
"period"
numItalics
[[Category:h
withstanding]]
linked
there]]
[[Category:a
tokens:
"[http://lucene.apache.org/java/docs/index.html#news
d]]
"http://foo.boo.com/test/test/"
<ref>Citation</ref>
numBoldItalics"
italics"
italics''
"heading"
"'''same
"''[[Category:ital_cat]]''
]]
Johnny.
expectedType
numCitation
j]]"
<span
info]]
[http://lucene.apache.org/java/docs/index.html?b=c
"Here
'''''and2
[[link|display
again]]
numItalics"
<sup>martian</sup>
"followed"
''more
(parens)
Test]"
numCategory"
"http://foo.boo.com/test/test/test.html?g=b&c=d"
"italics"
''italics''
"http://foo.boo.com/test/test/test.html"
"link"
[[Category:blah|
"closed"
tcm
testHandwritten
"click
class=\"glue\">code</span>"
"3.25"
''italics
'''''five
3.50.
period.
Here's
"Category
"Citation"
"code"
"URL"
"[[Category:a
quotes'''''
[http://foo.boo.com/test/test/
checkLinkPhrases
tokText
[[Category:e
"click"
[http://foo.boo.com/test/test/test.html
"expectedType
"http://lucene.apache.org/java/docs/index.html?b=c"
again]
testLucene1133
"linked"
numCategory
MutableInteger
logName
getSentences
\\t\\x0B\\f]*){2,}"
AnalyzerUtil
getParagraphs
"maxTokens
maxTokens
"([\\r\\n\\u0085\\u2028\\u2029][
getMaxTokenAnalyzer
"logStream
getTokenCachingAnalyzer
isSentenceSeparator
":EOS:"
getPorterStemmerAnalyzer
0xA1
getMostFrequentTerms
getLoggingAnalyzer
"]\n"
getSynonymAnalyzer
PARAGRAPHS
<query>"
org.apache.lucene.wordnet.SynExpand
"\":"
org.apache.lucene.wordnet.SynLookup
SynLookup
"Synonyms
<word>"
"before
f0="
lastNum
synonyms="
lastGroup
f3="
groups
internedWords
88022
group2Words
WORDS
"\n\nkeys="
f2="
gc"
allSynonyms
analyze
allWords
f1="
GROUPS
0.7
word2Groups
76401
word2Syns
randomize
1234567
0x278DDE6D
SYNONYM_TOKEN_TYPE
doc=
dir>\n\n"
ndecent
isDecent
Building
"OUCH:
prologFilename
"syn"
org.apache.lucene.wordnet.Syns2Index
ndecent="
num2Words
Prolog
synonyms,
non-existent
"[2/2]
"[1/2]
<prolog
word2Nums
"\trow="
"Optimizing.."
SynonymWhitespaceAnalyzer
wolfish
testSynonyms
"went"
"ravenous"
testSynonymsSingleQuote
"lost"
"Lost
TestSynonymTokenFilter
testSynonymsLimitedAmount
woods"
dog
"wolfish"
forest"
TestWordnet
"testLuceneWordnet"
storePathName
assertExpandsTo
testExpansionSingleQuote
expandedQuery
expectedQuery
commandLineArgs
testExpansion
ServletConfig
queryTemplateManager
getServletContext
openExampleIndex
xslFile
"salary"
dispatcher
defaultStandardQueryParserField
xmlQuery
xmlParser
"/WEB-INF/"
RequestDispatcher
"/index.jsp"
pNames
template"
FormBasedXmlQueryDemo
"xslFile"
"defaultStandardQueryParserField"
"/WEB-INF/data.tsv"
webdemo
dataIn
completedFormFields
"UserQuery"
"SpanNot"
stream:"
sft
maxNumCachedFilters
"SpanFirst"
btb
parseXML
addQueryBuilder
sots
"SpanTerm"
sot
sqof
"SpanNear"
"CachedFilter"
"SpanOrTerms"
pXmlFile
snb
"SpanOr"
"FilteredQuery"
snt
addFilterBuilder
"MatchAllDocsQuery"
"BooleanFilter"
"LikeThisQuery"
"FuzzyLikeThisQuery"
"DuplicateFilter"
"TermsFilter"
"BoostingQuery"
getChildTextByTagName
loadXML
getChildByTagName
deflt
getTagName
getOwnerDocument
getTextBuffer
createTextNode
getFilterBuilder
QueryObjectBuilder
xslIs
defaultCompiledTemplates
addDefaultQueryTemplate
compiledTemplatesCache
keysEnum
transformCriteria
Result
formProperties
xslDoc
getQueryAsXmlString
DOMResult
queryTemplateName
DOMSource
clauseFilter
clauseQuery
clause:"
occs
\"occurs\"
"mustNot"
"occurs"
defaultBoost
boostQuery
mainQueryElem
"BoostQuery"
boostQueryElem
btq
qb
childElement
filterElem
DuplicateFilter:"
"processingMode"
"fast"
"keepMode"
queryElement
"Filter"
filterElement
fbq
fieldElem
"Field"
"minSimilarity"
defaultMaxNumTerms
"maxNumTerms"
defaultIgnoreTF
"ignoreTF"
"prefixLength"
defaultPercentTermsToMatch
fieldsList
"minTermFrequency"
"fieldNames"
defaultFieldNames
"minDocFreq"
"IoException
defaultMaxQueryTerms
"percentTermsToMatch"
"maxQueryTerms"
"stopWords"
defaultMinTermFrequency
"lowerTerm"
"includeLower"
"includeUpper"
"upperTerm"
slopString
"inOrder"
"slop"
excludeElem
"Include"
includeElem
"Exclude"
SpanQueryObjectBuilder
unSafeParser
createQueryParser
testBoostingTermQueryXML
"like
"BooleanQuery.xml"
"TermQuery.xml"
"ConstantScoreQuery.xml"
"MatchAllDocsQuery.xml"
testCachedFilterXML
testDuplicateFilterQueryXML
"NestedBooleanQuery.xml"
"UserInputQuery.xml"
testUserQueryXML
xmlFileName
testBooleanFilterXML
"TermsFilterQuery.xml"
"MatchAllDocsQuery
testBoostingQueryXML
TestParser
"reuters21578.txt"
endOfDate
"BoostingQuery.xml"
testTermsFilterXML
"boosting
testMatchAllDocsPlusFilterXML
testSpanTermXML
testFuzzyLikeThisQueryXML
"Boolean
"FuzzyLikeThis"
testRangeFilterQueryXML
testBooleanQueryXML
"CachedFilter.xml"
"FuzzyLikeThisQuery.xml"
"RangeFilterQuery.xml"
"========="
"BoostingTermQuery.xml"
testNestedBooleanQuery
testConstantScoreQueryXML
"UserInputQueryCustomField
"============"
testLikeThisQueryXML
Filter"
"LikeThisQuery.xml"
testCustomFieldUserQueryXML
"SpanQuery.xml"
"UserInput
"UserInputQueryCustomField.xml"
"BooleanFilter.xml"
"TermsQuery.xml"
"DuplicateFilterQuery.xml"
"Span
"DuplicateFilterQuery
dumpResults
testSimpleXML
testSimpleTermsQueryXML
"artist=Fugazi
genre:electronic
\talbum=Grace
docFieldValues
"albumBooleanQuery.xsl"
"albumLuceneClassicQuery.xsl"
\tgenre=rock"
Medicine
\talbum=Repeater
qtm
"queryString=artist:buckly~
Blimey
\ttemplate=albumFilteredQuery"
Snapper
\texpectedMatches=2
\texpectedMatches=1
\texpectedMatches=0
\ttemplate=albumBooleanQuery"
"albumFilteredQuery"
queryForms
testFormTransforms
getDocumentFromString
Tom
"artist=Peeping
"albumLuceneClassicQuery"
TestQueryTemplateManager
\treleaseDate=2006
\treleaseDate=1999
"\t="
\tgenre=electronic
\talbum=Red
\ttemplate=albumLuceneClassicQuery"
"artist=Buckley
\talbum=Prince
"artist=Jeff
"albumFilteredQuery.xsl"
\tgenre=alternative"
expectedHits
"albumBooleanQuery"
\tgenre=electronic"
"expectedMatches"
\tgenre=rock
getPropsFromString
"artist=Red
Buckley
\treleaseDate=1990
\treleaseDate=1996
\treleaseDate=1995
\talbum=Peeping
queryFormProperties
DeleteFiles
<unique_term>"
org.apache.lucene.demo.DeleteFiles
dirSep
docDir
readable,
org.apache.lucene.demo.IndexFiles
"'..."
IndexFiles
IndexHTML
"IndexHTML
".htm"
"-create"
<index>]
uidIter
[-create]
"Specify
"Optimizing
"-field"
[-field
"Time:
field]
"(q)uit
Title:
[-paging
"-paging"
SearchFiles
"-norms"
jump
streaming
"\n\tSpecify
streamingHitCollector
page"
org.apache.lucene.demo.SearchFiles
"-repeat"
dir]
interactive
'false'
hitsPerPage]"
[-raw]
collected."
quit
search."
"-h"
"(p)revious
doPagingSearch
file]
[-repeat
n]
"(n)ext
page,
page."
[-queries
numTotalHits
hitsPerPage
[-norms
"-raw"
OneNormsReader
doStreamingSearch
"Usage:\tjava
"Collect
"Enter
"Press
f]
"-queries"
"&Omega"
"&pi"
"&Atilde"
"&nu"
"&or"
"&Zeta"
"&Oacute"
"&lowast"
"&infin"
"&middot"
"&rceil"
"&Iacute"
"&notin"
"&shy"
"&Yuml"
"&Agrave"
"&rdquo"
"&upsilon"
"&Dagger"
"&rarr"
"&Mu"
"&cent"
"&Sigma"
"&yacute"
"&lsquo"
"&ni"
"&iquest"
"&Upsilon"
"&rho"
"&bull"
"&Ecirc"
"&Egrave"
"&euml"
"&Iota"
"&trade"
"&lt"
"&ne"
"&hearts"
"&sdot"
"&Eta"
"&Eacute"
"&Aring"
"&tilde"
"&otilde"
"&Ucirc"
"&frac12"
"&ndash"
"&theta"
"&tau"
"&phi"
"&harr"
"&ocirc"
"&Uacute"
"&zwnj"
"&Ouml"
"&Beta"
"&Ugrave"
"&acirc"
"&ensp"
"&iuml"
"&Igrave"
"&sube"
"&dArr"
"&sect"
"&brvbar"
"&prod"
"&sup1"
"&psi"
"&frac34"
"&Omicron"
"&bdquo"
"&rsquo"
"&thinsp"
"&asymp"
"&piv"
"&szlig"
"&chi"
"&Ograve"
"&aelig"
"&mu"
"&Oslash"
"&lceil"
"&oacute"
"&part"
"&Tau"
"&acute"
"&lang"
"&sigmaf"
"&Chi"
"&Alpha"
"&thorn"
"&Kappa"
"&alpha"
"&epsilon"
"&and"
"&eacute"
"&lambda"
"&darr"
"&nsub"
"&times"
"&iacute"
"&Prime"
"&macr"
"&beta"
"&rlm"
"&egrave"
"&iota"
"&mdash"
"&para"
"&Auml"
"&cedil"
"&weierp"
"&eth"
"&rfloor"
"&rang"
"&sbquo"
"&minus"
"&cup"
"&kappa"
"&Otilde"
decoder
"&permil"
"&oslash"
"&gt"
"&Aacute"
"&euro"
"&not"
"&diams"
"&zeta"
"&uacute"
"&larr"
"&supe"
"&Ntilde"
"&agrave"
"&ang"
"&amp"
"&sup2"
"&clubs"
"&rArr"
"&equiv"
"&sigma"
"&laquo"
"&uuml"
"&reg"
"&int"
"&ordm"
"&divide"
"&zwj"
"&Uuml"
"&Gamma"
"&scaron"
"&THORN"
"&forall"
"&uml"
"&ouml"
"&ucirc"
"&crarr"
"&gamma"
"&deg"
"&emsp"
"&AElig"
"&ntilde"
"&lfloor"
"&Icirc"
"&Ccedil"
"&Xi"
"&Yacute"
"&exist"
"&icirc"
"&nabla"
"&Acirc"
"&oline"
"&Rho"
"&aring"
"&spades"
"&ge"
"&uarr"
"&Ocirc"
"&sum"
"&plusmn"
"&rsaquo"
"&isin"
"&curren"
"&sim"
"&upsih"
"&cong"
"&real"
"&raquo"
"&atilde"
"&circ"
"&Theta"
"&iexcl"
"&quot"
"&omicron"
"&thetasym"
"&Pi"
"&ETH"
"&Nu"
"&ecirc"
"&Psi"
"&alefsym"
"&hArr"
"&loz"
"&delta"
"&Lambda"
"&omega"
"&OElig"
"&empty"
"&sup3"
"&lrm"
"&le"
"&aacute"
"&ordf"
"&prop"
"&frac14"
"&Epsilon"
"&dagger"
"&yuml"
"&sup"
"&radic"
"&ldquo"
"&sub"
"&oelig"
"&cap"
"&lArr"
"&Iuml"
"&micro"
"&ograve"
"&auml"
"&Phi"
"&oplus"
"&pound"
"&Delta"
"&perp"
"&prime"
"&image"
"&nbsp"
"&fnof"
"&ccedil"
"&Scaron"
"&xi"
"&eta"
"&copy"
"&uArr"
"&frasl"
"&Euml"
"&there4"
"&lsaquo"
"&yen"
"&ugrave"
"&igrave"
"&hellip"
"&otimes"
addMetaTag
pipeOutStream
0x3b0000
"<title"
inTitle
afterTag
Decl
Tag
"<STYLE"
metaTags
"<META"
pipeIn
CommentTag
PIPE_SIZE
addToSummary
"<img"
jj_2_2
0x200000
PipedOutputStream
0x380000
currentMetaTag
inStyle
PipedInputStream
currentMetaContent
"UTF-16BE"
afterSpace
jj_3_2
pipeInStream
0x2c7e
0x4000
"HTTP-EQUIV"
"alt"
ScriptTag
addText
inMetaTag
inImg
MyPipedInputStream
tit
addSpace
"<TagEnd>"
"\"<!\""
"<Space>"
"<Punct>"
AfterEquals
22>"
"<Entity>"
"<Quote2Text>"
WithinTag
"\"<script\""
WithinScript
"<ArgValue>"
"<ScriptText>"
WithinComment2
WithinComment1
"<ArgName>"
"<LET>"
"<Quote1Text>"
"<Word>"
WithinQuote1
WithinQuote2
"\"\\\'\""
"<CloseQuote2>"
"<DeclName>"
"\"<!--\""
"\"-->\""
HEX
"\"=\""
"<HEX>"
SP
"<CloseQuote1>"
LET
"\">\""
"<TagName>"
"\"\\\"\""
"<SP>"
"<CommentText2>"
"<CommentText1>"
"<ScriptEnd>"
"\74\163\143\162\151\160\164"
"WithinComment2"
0x9ffffffeffffd9ffL
0x9fffff7affffd9ffL
0x500000000000L
"WithinQuote1"
jjMoveNfa_6
jjStopStringLiteralDfa_0
jjMoveNfa_5
jjStopStringLiteralDfa_6
0x20L
0x2L
jjStartNfa_6
jjStartNfa_7
jjStartNfa_0
0xbffffffeffffd9ffL
0x208000000000L
jjStopStringLiteralDfa_7
"AfterEquals"
jjStartNfaWithStates_0
"\74\41\55\55"
"\47"
115
0x5000000000000000L
"\75"
0xafffffffffffffffL
0x10L
"\76"
"WithinTag"
0x7e0000007eL
0x100000001000000L
0x400000L
jjMoveStringLiteralDfa5_0
jjMoveStringLiteralDfa3_0
0x7fffffe07fffffeL
"\74\41"
jjMoveStringLiteralDfa0_4
"\55\55\76"
jjMoveStringLiteralDfa0_5
"WithinComment1"
"\42"
jjMoveNfa_7
jjMoveNfa_4
old0
jjMoveStringLiteralDfa1_6
jjMoveStringLiteralDfa1_0
0x880000000000L
0x30L
0x7fbfec7fL
"WithinScript"
jjMoveStringLiteralDfa4_0
jjMoveStringLiteralDfa2_0
jjMoveStringLiteralDfa2_6
0xbfffffffffffffffL
0xffffff7fffffffffL
jjMoveStringLiteralDfa0_6
jjMoveStringLiteralDfa0_7
0x32L
0xffffdfffffffffffL
"WithinQuote2"
jjMoveStringLiteralDfa6_0
Aborted:
"<h6"
"<br"
"</td"
"<li"
"<div"
"</h3"
"<h2"
"</h5"
"<q"
"<td"
"</p"
"</h6"
"</div"
"</dt"
"<dt"
"<h4"
"<blockquote"
"<br/"
"</h2"
"</q"
"</blockquote"
"</h4"
"<p"
"<h3"
"</li"
"<hr"
"<h1"
"<h5"
"</h1"
"<hr/"
"Summary:
"Title:
"Content:"
"-dir"
LineNumberReader
tokenStreamMethod
reusableTokenStreamMethod
"reusableTokenStream"
setOverridesTokenStreamMethod
"tokenStream"
tokenStreams
'\u249C'
'\u0217'
'\u2461'
'\u1E9D'
'\u1D63'
'\u1D18'
'\u24CB'
'\u1E32'
'\u0288'
'\uFF28'
'\uFF11'
'\u1E92'
'\u24FD'
'\u1E69'
'\u0281'
'\uFF57'
'\u0208'
'\u01D7'
'\u1E21'
'\u1E85'
'\uA762'
'\u1E58'
'\u01CB'
'\uFF37'
'\u0255'
'\u1D85'
'\u1EEB'
'\u1E07'
'\u2C68'
'\uFF02'
'\u01E0'
'\u1E45'
'\u01F9'
'\u1D64'
'\u029A'
'\u0158'
'\u2771'
'\u1E7E'
'\u2788'
'\u248E'
'\u00BB'
'\u24A9'
'\u014B'
'\u1D0A'
'\u1E8D'
'\u2087'
'\u0121'
'\u01AD'
'\u0103'
'\u02A6'
'\u24C7'
'\u017B'
'\u0145'
'\uA731'
'\u024A'
'\u1D7E'
'\u24E7'
'\u1EB6'
'\uFF4C'
'\u0231'
'\u0114'
'\u1EE4'
'\u011A'
'\u24BA'
'\u24D0'
'\uA782'
'\u1EF6'
'\u247C'
'\u018F'
'\u1ED6'
'\u2047'
'\u027D'
'\u2475'
'\u0225'
'\u1ED8'
'\u277D'
'\u1EFC'
'\u2018'
'\uA748'
'\u0216'
'\u1E9C'
'\u01B6'
'\u0276'
'\u1D62'
'\u1D19'
'\u017C'
'\uFF29'
'\uFF10'
'\u1E91'
'\u24FE'
'\u1E66'
'\u025F'
'\uFF56'
'\u1E4A'
'\uFF20'
'\u0209'
'\u01D6'
'\u1E26'
'\uA755'
'\u1E86'
'\uA763'
'\u01CC'
'\u016C'
'\u0254'
'\uFF58'
'\uA7FC'
'\u1EEA'
'\uA740'
'\uFF03'
'\u01E1'
'\u1E46'
'\u1D99'
'\u01F8'
'\u246A'
'\u2C78'
'\u1E48'
'\uFF0D'
'\u1E71'
'\uFF45'
'\u028B'
'\u0243'
'\u1E7D'
'\u2787'
'\u248F'
'\u1EBD'
'\u24A8'
'\u0222'
'\u014A'
'\u1E8E'
'\u2088'
'\u0120'
'\u01AE'
'\u2C65'
'\u0102'
'\u02A5'
'\u24C6'
'\u24EC'
'\u0144'
'\uA730'
'\u2769'
'\u24DA'
'\uFF2B'
'\u207F'
'\u24E6'
'\u1EB7'
'\u0115'
'\u023E'
'\uA73F'
'\u1E5F'
'\u1E10'
'\u24BF'
'\u24D1'
'\u247B'
'\uA783'
'\u011B'
'\u1EF7'
'\u020F'
'\u018E'
'\u2498'
'\u2474'
'\u2496'
'\u24D9'
'\u277E'
'\u0215'
'\u1E9B'
'\uFF35'
'\u249A'
'\uA749'
'\u1D05'
'\u1E65'
'\uFF5B'
'\u1D14'
'\u276A'
'\u1E90'
'\u24FB'
'\u1D1C'
'\u0233'
'\u0284'
'\u1E4B'
'\u01D5'
'\u1E27'
'\uA754'
'\u025E'
'\u016B'
'\u1D87'
'\u1E01'
'\u1E3E'
'\u1E87'
'\u01E2'
'\u1E47'
'\u0289'
'\uFF13'
'\uA745'
'\u1E49'
'\uFF0E'
'\u1E70'
'\u01A4'
'\u0244'
'\uFF25'
'\uA779'
'\u2786'
'\uA74C'
'\u1EBE'
'\u24A7'
'\u1D0C'
'\u1E8F'
'\u2089'
'\u0123'
'\u01AF'
'\u1D8E'
'\u2C64'
'\u24C1'
'\u0183'
'\u24EB'
'\u0130'
'\uA737'
'\u2768'
'\u024C'
'\uFF2C'
'\u207A'
'\u1EB4'
'\u0149'
'\uA73E'
'\u0116'
'\u023D'
'\u24E5'
'\u24B2'
'\u013E'
'\u0154'
'\u0101'
'\u2C6E'
'\u1E5E'
'\u24D2'
'\u247E'
'\uA780'
'\u011C'
'\u1EF4'
'\u1EC9'
'\uFF4A'
'\u0227'
'\u027F'
'\u020C'
'\u2477'
'\u2495'
'\u1EFA'
'\u0226'
'\u021F'
'\u2488'
'\u1E9A'
'\u01B0'
'\u0214'
'\u1D04'
'\uA75E'
'\u017A'
'\uFF5E'
'\u2184'
'\u24FC'
'\u1E64'
'\u1E39'
'\u0232'
'\uFF34'
'\u01D4'
'\u1E24'
'\uA757'
'\u025D'
'\u1E4C'
'\u2479'
'\u016E'
'\u1D86'
'\u1E3D'
'\uFF01'
'\u1E80'
'\u01CA'
'\uFF12'
'\u01E3'
'\u1D89'
'\u1E73'
'\u01A5'
'\u0245'
'\u2785'
'\uA77F'
'\u1E1F'
'\u24A6'
'\u1E7F'
'\u1D0B'
'\u0122'
'\u2C67'
'\uA742'
'\u24C0'
'\u0182'
'\u24EA'
'\u0179'
'\uA736'
'\u024D'
'\u2093'
'\u1EB5'
'\u0195'
'\u0148'
'\uFF47'
'\uA73D'
'\u1D6F'
'\u0117'
'\u24E4'
'\u24B3'
'\u277B'
'\u0155'
'\u0100'
'\u02A3'
'\u1E5D'
'\u278F'
'\u2079'
'\u24D3'
'\u247D'
'\u018C'
'\uA781'
'\u24BD'
'\u1EC8'
'\u1EF5'
'\uFF2A'
'\u2476'
'\u2494'
'\u021E'
'\u2489'
'\u0202'
'\u01B3'
'\u0213'
'\u1D07'
'\uFF5D'
'\u0204'
'\u276C'
'\u1E38'
'\u0268'
'\u1D70'
'\uFF33'
'\u0291'
'\uA74D'
'\u1E25'
'\uA756'
'\u1E4D'
'\u2478'
'\u016D'
'\u0259'
'\u1D81'
'\uFF06'
'\u1E81'
'\u01D9'
'\u01CF'
'\uFF48'
'\u1D1A'
'\u01E4'
'\u0246'
'\uFF46'
'\u1E72'
'\u1E6B'
'\u1EEF'
'\u1E03'
'\u0266'
'\u248A'
'\uA77E'
'\u1ED7'
'\u1E1E'
'\u24A5'
'\u204E'
'\u2784'
'\uA768'
'\u2778'
'\u2C66'
'\u014F'
'\u0181'
'\u0176'
'\u024E'
'\u0228'
'\u2092'
'\u207C'
'\u019D'
'\u0136'
'\uA735'
'\u0196'
'\u0110'
'\u023F'
'\u24B0'
'\u0156'
'\u24C3'
'\u278E'
'\u2078'
'\u24D4'
'\u018B'
'\uFF1E'
'\u24BE'
'\u026A'
'\u0161'
'\u1EF2'
'\u1D84'
'\u1ECE'
'\u021D'
'\u2493'
'\uA7FD'
'\u01FD'
'\u1EE3'
'\u1EB8'
'\u01B2'
'\u2032'
'\u1EDA'
'\u012E'
'\u0212'
'\u2484'
'\u1D6C'
'\u1D06'
'\u0205'
'\u276B'
'\u1D71'
'\u0287'
'\u022B'
'\u0221'
'\uFF32'
'\u0290'
'\uA751'
'\u1E4E'
'\u2499'
'\u1D80'
'\u0260'
'\uFF07'
'\u1E82'
'\u01D8'
'\u1E53'
'\u1E28'
'\u0258'
'\uFF49'
'\u015A'
'\u24FA'
'\u1E62'
'\u01E5'
'\u01FA'
'\u0247'
'\u1D93'
'\u1E75'
'\u01DC'
'\u1EEE'
'\u1E3F'
'\u020D'
'\u1E1D'
'\u1E8A'
'\u204F'
'\u2783'
'\u248B'
'\uA77D'
'\u00B2'
'\u2C61'
'\u014E'
'\u0180'
'\u0177'
'\u02AF'
'\u2091'
'\u24A4'
'\u026F'
'\u207B'
'\u019E'
'\u0137'
'\uA734'
'\u2497'
'\u024F'
'\u2C72'
'\u278D'
'\u0111'
'\u023A'
'\u24B1'
'\u1E2B'
'\u0197'
'\u0157'
'\u2C6F'
'\u24C2'
'\u24D5'
'\u247F'
'\u018A'
'\u0159'
'\u0160'
'\u1EF3'
'\u1ECD'
'\u2013'
'\u021C'
'\u1E0D'
'\u2492'
'\uA7FE'
'\u1EE2'
'\u1EB9'
'\u277A'
'\u249B'
'\u2035'
'\u1EDB'
'\u012D'
'\u0211'
'\u2485'
'\u1D6B'
'\uA75F'
'\u1D01'
'\u0206'
'\u276E'
'\u01C6'
'\u1D72'
'\u0280'
'\u1D8D'
'\u1D65'
'\u1E6F'
'\uA750'
'\u1E4F'
'\uFF36'
'\u1EAF'
'\u022A'
'\u010F'
'\u1E02'
'\u1E83'
'\u1E52'
'\u01CD'
'\u1E29'
'\u016F'
'\u1D83'
'\u015B'
'\u1E63'
'\u01E6'
'\uA743'
'\u1D10'
'\uFF44'
'\u1E74'
'\u01DB'
'\u01E8'
'\u1EED'
'\u01A0'
'\u1E3A'
'\uFF04'
'\u1E1C'
'\uFF4E'
'\u01AB'
'\u2782'
'\u2779'
'\uA77C'
'\uA74B'
'\u2C60'
'\u014D'
'\u2E29'
'\u00B3'
'\u0174'
'\u0129'
'\u02AE'
'\u2090'
'\u24A3'
'\u019F'
'\u0134'
'\u2C73'
'\u0230'
'\u278C'
'\u24E9'
'\u24B6'
'\u1E2C'
'\u0190'
'\u02A0'
'\u2C6A'
'\u24F8'
'\u026C'
'\u1EA9'
'\u24F6'
'\u0163'
'\u24D6'
'\u0112'
'\u1EF0'
'\u1EC5'
'\u208A'
'\u2012'
'\u021B'
'\u1D1B'
'\u1E0E'
'\uA7FF'
'\u203A'
'\u1EE1'
'\u0210'
'\u2468'
'\u2486'
'\uFF3E'
'\uA75A'
'\u2491'
'\u1EA7'
'\u1D00'
'\u276D'
'\u01C7'
'\uA753'
'\u1D21'
'\u1D22'
'\u0207'
'\uFF30'
'\u1EDC'
'\u201C'
'\u1EAE'
'\u010E'
'\u24CE'
'\u1E51'
'\u01CE'
'\u1D82'
'\uFF19'
'\u1E60'
'\u1E35'
'\uFF27'
'\u01E7'
'\u1E19'
'\u01FC'
'\u015C'
'\u2466'
'\u0249'
'\u1D91'
'\u1E99'
'\u1EF8'
'\u01DA'
'\uFF42'
'\u01E9'
'\u028C'
'\u1E17'
'\u01A1'
'\uFF05'
'\u0265'
'\u1E1B'
'\u1E8C'
'\u0223'
'\u01AC'
'\u2C63'
'\u2781'
'\u2776'
'\uA77B'
'\u01F0'
'\u1E77'
'\u2C7B'
'\u1E00'
'\u2E28'
'\u0128'
'\u24A2'
'\u026D'
'\u0135'
'\u278B'
'\u24B7'
'\u0191'
'\u24F9'
'\u24EE'
'\u0175'
'\u2080'
'\u1EA8'
'\uFF1F'
'\u24F7'
'\u0162'
'\u2075'
'\u24D7'
'\u0113'
'\u023C'
'\u24E8'
'\u1EF1'
'\u1EC4'
'\u208B'
'\u2011'
'\u021A'
'\u1E0F'
'\uFF3F'
'\u1ECF'
'\u1EE0'
'\u2469'
'\u2487'
'\u2490'
'\u1EA6'
'\u1E6A'
'\u1D03'
'\uFF3D'
'\u01C4'
'\uA752'
'\u1D8F'
'\u1D7A'
'\u0200'
'\u1E6D'
'\u1E06'
'\u1EDD'
'\u012F'
'\u201B'
'\u1EAD'
'\u010D'
'\u0271'
'\u24CD'
'\uFF51'
'\u1E61'
'\u1E34'
'\u0239'
'\u0282'
'\u1E18'
'\u01FB'
'\u015D'
'\u2467'
'\u1D96'
'\u1E98'
'\u029F'
'\u24DB'
'\u1E16'
'\u1E3C'
'\uFF26'
'\u1E50'
'\u0262'
'\u2C62'
'\u2780'
'\u2777'
'\uA77A'
'\u1E1A'
'\u24AF'
'\u01F3'
'\u1E76'
'\u2C7C'
'\u246F'
'\u0253'
'\u24E3'
'\uFF31'
'\u24A1'
'\uA739'
inputPos
'\u2C71'
'\u278A'
'\u0192'
'\u0143'
'\u0138'
'\u24ED'
'\u0172'
'\u2081'
'\u2C6C'
'\u24F4'
'\uFF59'
'\u0165'
'\u2074'
'\u24D8'
'\u023B'
'\u24B4'
'\u1E2A'
'\u1EC7'
'\u208C'
'\u203C'
'\u1ECA'
'\u2010'
'\u1EE7'
'\u1D77'
'\u2480'
'\u2036'
'\u1ED0'
'\u1EA5'
'\u1D02'
'\uFF3C'
'\u01C5'
'\u1D8A'
'\u0201'
'\u276F'
'\u275B'
'\u1EDE'
'\u012A'
'\u0224'
'\u201A'
'\u1EAC'
'\u1E6E'
'\u010C'
'\u0270'
'\uFF50'
foldToASCII
'\u1E37'
'\u1D75'
'\u1D20'
'\u1D88'
'\u01FE'
'\uFF18'
'\u2464'
'\uFF43'
'\u1E97'
'\u029E'
'\u01F5'
'\u24DC'
'\u1E40'
'\u1E15'
'\u1E3B'
'\u1E57'
'\u1E08'
'\u022C'
'\u2774'
'\u24AE'
'\uA74A'
'\u1E79'
'\u1E88'
'\u019A'
'\uA738'
'\uFF0B'
'\u2790'
'\u24A0'
'\u026B'
'\u0248'
'\u2C76'
'\u02AB'
'\uFF3A'
'\u1EBB'
'\u277C'
'\u0193'
'\u0142'
'\u0139'
'\u0173'
'\u2082'
'\u024B'
'\u2C6B'
'\u013D'
'\u24F5'
'\u1EC6'
'\u0164'
'\u2077'
'\u01EA'
'\u013B'
'\uFF2D'
'\u24B5'
'\u1E2F'
'\u208D'
'\u2033'
'\uFF4F'
'\u1EE6'
'\u011D'
'\u2481'
'\uFF1A'
'\u1EFF'
'\u1ED1'
'\u1EA4'
'\u2048'
'\uFF3B'
'\u020E'
'\uA75B'
'\u24EF'
'\uA74E'
'\u1EDF'
'\u1EAB'
'\u1D16'
'\u275C'
'\u010B'
'\u0273'
'\u24CF'
'\uFF53'
'\u1E8B'
'\u1D76'
'\u017F'
'\uA747'
'\u015F'
'\u2465'
'\u1E36'
'\u1E96'
'\u029D'
'\u020A'
'\u0235'
'\u1D17'
'\u24DD'
'\u1E41'
'\u1E14'
'\u1D94'
'\uFF15'
'\uFF08'
'\u01D3'
'\u1E56'
'\u01C8'
'\u2039'
'\u025C'
'\u1E09'
'\u248C'
'\uA74F'
'\u1E78'
'\u2C7A'
'\u01DF'
'\u246D'
'\u1E89'
'\uA766'
'\u24AD'
'\uFF0C'
'\u1D0E'
'\u2791'
'\u00B9'
'\u2775'
'\u02AA'
'\u2094'
'\u1EBC'
'\uFF24'
'\u1E7A'
'\u0141'
'\u01F1'
'\u0170'
'\u2083'
'\u0125'
'\u0109'
'\uA728'
'\u24C9'
'\u24F2'
'\u1EC1'
'\u0189'
'\u028D'
'\u01EB'
'\u013C'
'\uFF1C'
'\u0107'
'\uFF2E'
'\u208E'
'\u00AB'
'\u1E0A'
'\u1D8C'
'\u1ECC'
'\u1E5C'
'\u0167'
'\u2076'
'\u1EE5'
'\u1EB2'
'\uA73C'
'\uA786'
'\u011E'
'\u2038'
'\u1ED2'
'\u1EA3'
'\uA7FB'
'\u2049'
'\u1D6E'
'\u2471'
'\u1EAA'
'\u0203'
'\u020B'
'\u012C'
'\u2482'
'\u1E6C'
'\u275D'
'\u0261'
'\u24CA'
'\u1D79'
'\u249F'
'\uA744'
'\u010A'
'\u0272'
'\uFF23'
'\u022D'
'\u2462'
'\u1E31'
'\u1E95'
'\u029C'
'\u0234'
'\u24DE'
'\u1E42'
'\u1E13'
'\u1D95'
'\uFF14'
'\uFF41'
'\u01D2'
'\u1E55'
'\u01C9'
'\u1E22'
'\u025B'
'\u0229'
'\uFF09'
'\uA767'
'\u1E04'
'\u01F4'
'\u01DE'
'\u246E'
'\u0250'
'\u24AC'
'\u019C'
'\u2C74'
'\u1D0D'
'\uFF52'
'\u1ED9'
'\u2772'
'\u0171'
'\u2084'
'\u0124'
'\u0108'
'\uA729'
'\u24C8'
'\u24F3'
'\u1EC0'
'\u0188'
'\u028E'
'\uFF1B'
'\u0106'
'\u1D7B'
'\u1E2D'
'\u0140'
'\u24E2'
'\u1ECB'
'\uFF1D'
'\uFF4D'
'\u1D92'
'\u1E0B'
'\u1E5B'
'\u0166'
'\u2071'
'\u01EC'
'\u1EB3'
'\uA73B'
'\u2053'
'\u011F'
'\u24BB'
'\u1EA2'
'\u1D6D'
'\u1ED3'
'\u2470'
'\u2044'
'\u1D09'
'\u012B'
'\u2483'
'\u027C'
'\u022F'
'\u0299'
'\u275E'
'\u249E'
'\u01FF'
'\u0219'
'\u0275'
'\uFF39'
'\u0297'
'\u017D'
'\u022E'
'\u2463'
'\u01B5'
'\u1E30'
'\u029B'
'\u0237'
'\u1E12'
'\uFF17'
'\u1E94'
'\u24FF'
'\u01D1'
'\u1E54'
'\u1E23'
'\u025A'
'\uFF55'
'\uA741'
'\uA760'
'\u1E05'
'\u01DD'
'\u246B'
'\u24DF'
'\u1E43'
'\u0257'
'\u24AB'
'\u01F7'
'\uFF0A'
'\u2C75'
'\u2793'
'\u2773'
'\u0240'
'\u1E7C'
'\u1EBA'
'\u2085'
'\u0127'
'\u1D74'
'\u1EC3'
'\u0187'
'\u028F'
'\u0150'
'\u0105'
'\u02A8'
'\u24F0'
'\u24C5'
'\u24B8'
'\u1E2E'
'\u207E'
'\u0147'
'\uA733'
'\u1E67'
'\u0118'
'\u1D7C'
'\u24E1'
'\u2014'
'\uFF22'
'\u1E0C'
'\u1E5A'
'\u0169'
'\u2070'
'\u01ED'
'\u013A'
'\u1EB0'
'\u0198'
'\uA73A'
'\u2052'
'\uA784'
'\u24BC'
'\u1EE9'
'\u0238'
'\u247A'
'\u1ED4'
'\u1EA1'
'\u2473'
'\u201E'
'\u2045'
'\u1D08'
'\uA759'
'\u277F'
'\u1EFE'
'\u1D15'
'\uA746'
'\uA785'
'\u249D'
'\u0218'
'\u2460'
'\u015E'
'\u0274'
'\uFF38'
'\u24CC'
'\uFF5A'
'\u1E9E'
'\u01B4'
'\u1E33'
'\u017E'
'\u1D97'
'\u0236'
'\uFF16'
'\u1E93'
'\u1E68'
'\u1E20'
'\uFF54'
'\u0220'
'\u01D0'
'\u1E84'
'\uA761'
'\u1E59'
'\u2792'
'\u016A'
'\u1EEC'
'\uFB04'
'\u2C69'
'\u246C'
'\uFB03'
'\u1E44'
'\u1E11'
'\u0256'
'\u24AA'
'\u01F6'
'\uFF0F'
'\u2770'
'\u1E7B'
'\u2789'
'\u248D'
'\u1EBF'
'\u014C'
'\u1D0F'
'\u2086'
'\u0126'
'\u1EC2'
'\u0186'
'\u0151'
'\u0104'
'\u24F1'
'\u24C4'
'\u207D'
'\u0146'
'\uA732'
'\u0119'
'\u1D7D'
'\u24B9'
'\u24E0'
'\u1EB1'
'\uFF4B'
'\u0168'
'\u013F'
'\u1D73'
'\u0199'
'\u1EE8'
'\u01BF'
'\u01F2'
'\u1EF9'
'\u1ED5'
'\u1EA0'
'\u201D'
'\uFF2F'
'\u2046'
'\u027E'
'\u0080'
'\uA758'
'\uFF21'
'\u2472'
'\u1EFB'
OffCorrectMap
pcmList
cumulativeDiff
EmptyCharArrayMap
createEntrySet
allowModify
Map)"
oldkeys
"(this
oldvalues
nextCharArray
"normalize"
useOldAPI
isTokenCharMethod
normalizeMethod
"since
normalize(char)."
oldIoBuffer
incrementTokenOld
subclasses
LUCENE_31,
LUCENE_3_1
"isTokenChar"
isTokenChar(int)"
isTokenChar(char)
'\uFB05'
'\u00c0'
removeAccents
keywordSet
pushChar
firstChar
charPointer
chr
nm
"MappingCharFilter:
"lowerPrecNumeric"
"call
"(numeric,valSize="
set???Value()
usage"
",precisionStep="
"fullPrecNumeric"
"PerFieldAnalyzerWrapper("
analyzerMap
default="
fieldAnalyzers
doublec
"logi"
setto
wordBuffer
wordLen
bufferLen
cvc
INITIAL_SIZE
k0
vowelinstem
i0
streamChain
stopwordsFile
getEnablePositionIncrementsVersionDefault
skippedPositions
wordSet
getStopwordSet
consumed."
cachedStates
supplied
ACCEPT_ALL_FILTER
compatible
tee"
addState
consumed
setFinalState
clearNoTermBuffer
newType
",posIncr="
0x0a45aa31
newTermOffset
startTermBuffer
newEndOffset
typ
TokenAttributeFactory
newStartOffset
newTermBuffer
termBufferOffset
",type="
newTermLength
stopwordResource
wordstem
wordstemfile
"wordstemfile
ACRONYM_TYPE
APOSTROPHE_TYPE
"<ACRONYM_DEP>"
"\4\12\2\0\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0"
"\2\0\1\5\1\0\1\5\3\4\6\5\1\6\1\4"
"\0\142\0\160\0\176\0\214\0\232\0\250\0\266\0\304"
"\2\12\14\1\7\12\11\1\12\2\47\0\2\12\1\0\1\12\2\0"
"\2\24\7\0\1\63\4\0\2\62\1\0\1\56\2\0"
"\1\0\11\12\1\0\1\12\2\0\7\12\71\0\1\1\60\12\1\1"
"\1\42\4\0\1\42\1\43\1\0\1\16\2\0\3\16"
"\4\0\2\34\1\0\1\57\1\0\1\11\2\12\1\13"
"\1\1\1\0\17\1\1\0\1\1\3\0\5\1"
"\10\12\2\0\2\12\2\0\26\12\1\0\7\12\1\0\1\12\3\0"
"\1\0\1\54\1\0\1\11\2\54\1\55\1\31\4\0"
"\2\0\6\12\2\0\6\12\2\0\3\12\43\0"
"\1\17\1\10\1\21\1\22\2\12\1\13\1\23\4\0"
"\1\30\1\31\1\0\1\52\1\0\1\11\2\52\1\0"
"\32\12\5\0\113\12\225\0\64\12\54\0\12\2\46\0\12\2\6\0"
"\2\24\1\0\1\51\1\0\1\11\2\52\1\0\1\24"
"\1\0\1\11\2\52\1\0\1\30\4\0\1\30\1\31"
"\1\0\20\12\46\0\2\12\4\0\12\2\25\0\22\12\3\0\30\12"
"\23\12\16\0\11\2\56\0\125\12\14\0\u026c\12\2\0\10\12\12\0"
"\2\0\1\56\2\0\1\27\4\0\2\30\1\0\1\52"
"\1\34\4\0\1\34\1\35\1\0\1\60\1\0\1\11"
"\1\40\1\0\1\15\1\0\1\11\2\15\1\16\1\40"
"\4\0\1\10\1\0\32\12\57\0\1\12\12\0\1\12\4\0\1\12"
"\1\13\3\13\6\13\50\13\3\13\1\0\136\12\21\0\30\12\70\0"
"\4\0\12\2\25\0\6\12\3\0\3\12\1\0\4\12\3\0\2\12"
"\1\13\4\0\1\34\1\35\7\0\1\36\4\0\1\37"
"\1\11\2\52\1\0\1\24\3\0"
"\1\43\4\0\1\42\1\43\1\0\1\13\2\0\3\13"
"\50\0\14\12\164\0\3\12\1\0\1\12\1\0\207\12\23\0\12\2"
"\0\u0142\0\u0150\0\u015e\0\u016c\0\u017a\0\u0188\0\u0196\0\u01a4"
"\1\56\2\0\1\62\4\0\2\24\1\0\1\61\1\0"
"\1\32\5\0\1\33\1\0\1\55\2\0\3\55\1\33"
"\1\0\1\11\2\12\1\13\1\36\4\0\2\37\1\0"
"\1\0\2\12\1\0\5\12\3\0\1\12\22\0\1\12\17\0\1\12"
"\1\0\3\12\55\0\11\2\25\0\10\12\1\0\3\12\1\0\27\12"
"\1\2\1\3\1\4\7\2\1\5\1\6\1\7\1\2"
"\65\12\3\0\1\12\22\0\1\12\7\0\12\12\4\0\12\2\25\0"
"\6\12\2\0\46\12\2\0\6\12\2\0\10\12\1\0\1\12\1\0"
"\4\0\1\37\1\40\1\0\1\12\1\0\1\11\2\12"
"\20\0\7\12\1\0\1\12\1\0\3\12\1\0\26\12\1\0\7\12"
"\5\0\12\2\25\0\10\12\2\0\2\12\2\0\26\12\1\0\7\12"
"\2\0\11\12\2\0\7\12\16\0\2\12\16\0\5\12\11\0\1\12"
"\1\21\1\22\2\12\1\13\1\23\20\0\1\2\1\0"
"\1\12\1\0\4\12\2\0\7\12\1\0\7\12\1\0\27\12\1\0"
"\2\15\1\16\1\4\4\0\1\3\1\4\1\17\1\20"
"\1\0\2\12\37\0\4\12\1\0\1\12\7\0\12\2\2\0\3\12"
"\130\12\10\0\51\12\u0557\0\234\12\4\0\132\12\6\0\26\12\2\0"
"\1\0\12\12\1\0\5\12\46\0\2\12\4\0\12\2\25\0\10\12"
"\5\0\27\12\1\0\37\12\1\0\u0128\12\2\0\22\12\34\0\136\12"
"\7\12\1\0\1\12\1\0\4\12\2\0\47\12\1\0\1\12\1\0"
"\2\12\1\0\1\12\2\0\1\12\6\0\4\12\1\0\7\12\1\0"
"\7\0\32\12\6\0\32\12\12\0\1\13\72\13\37\12\3\0\6\12"
"\1\24\1\25\7\0\1\26\4\0\2\27\7\0\1\27"
"\11\0\1\0\1\15\1\0\1\0\1\14\22\0\1\0\5\0\1\5"
"\1\40\7\0\1\41\4\0\1\42\1\43\7\0\1\44"
"\2\0\1\12\7\0\47\12\110\0\33\12\5\0\3\12\56\0\32\12"
"\2\12\11\0\1\12\2\0\5\12\1\0\1\12\11\0\12\2\2\0"
"\213\0\1\12\13\0\1\12\1\0\3\12\1\0\1\12\1\0\24\12"
"\1\12\1\0\1\11\2\12\1\13\1\37\4\0\1\37"
"\1\0\2\12\2\0\4\12\3\0\1\12\36\0\2\12\1\0\3\12"
"\1\12\2\0\12\12\1\0\1\12\3\0\5\12\6\0\1\12\1\0"
"\1\13\1\41\4\0\2\42\1\0\1\13\2\0\3\13"
"\1\55\1\25\4\0\1\24\1\25\1\0\1\51\1\0"
"\17\0\2\3\1\0\1\10\1\0\1\11\2\12\1\13"
"\2\24\7\0\1\24\4\0\2\30\7\0\1\30\4\0"
"\0\u01b2\0\u01c0\0\u01ce\0\u01dc\0\u01ea\0\u01f8\0\322\0\u0206"
"\0\322\0\340\0\356\0\374\0\u010a\0\u0118\0\u0126\0\u0134"
658
"\2\34\7\0\1\34\4\0\2\37\7\0\1\37\4\0"
"\1\0\2\12\1\0\154\12\41\0\u016b\12\22\0\100\12\2\0\66\12"
"\15\12\5\0\3\12\1\0\7\12\202\0\1\12\202\0\1\12\4\0"
"\4\0\2\12\2\0\26\12\1\0\7\12\1\0\2\12\1\0\2\12"
"\4\12\164\0\42\12\1\0\5\12\1\0\2\12\25\0\12\2\6\0"
"\1\0\12\12\1\0\15\12\1\0\5\12\1\0\1\12\1\0\2\12"
"\20\13\u0100\0\200\13\200\0\u19b6\13\12\13\100\0\u51a6\13\132\13\u048d\12"
"\6\12\112\0\46\12\12\0\47\12\11\0\132\12\5\0\104\12\5\0"
"\1\12\1\0\1\12\1\0\37\12\2\0\65\12\1\0\7\12\1\0"
"\1\4"
"\1\11\2\52\1\0\1\26\4\0\2\27\1\0\1\56"
"\1\12\1\0\1\12\1\0\4\12\1\0\3\12\1\0\7\12\u0ecb\0"
"\u0773\0\u2ba4\12\u215c\0\u012e\13\322\13\7\12\14\0\5\12\5\0\1\12"
"\1\3\4\0\1\11\1\7\1\4\1\11\12\2\6\0\1\6\32\12"
"\2\7\1\10\1\0\1\10\3\0\2\10\1\11\1\12"
"\37\12\1\0\1\12\1\0\4\12\2\0\7\12\1\0\47\12\1\0"
"\2\0\2\12\2\0\2\12\3\0\46\12\2\0\2\12\67\0\46\12"
"\1\0\54\12\1\0\10\12\2\0\32\12\14\0\202\12\12\0\71\12"
"\4\12\42\0\2\12\1\0\3\12\4\0\12\2\2\12\23\0\6\12"
"\2\15\1\16\1\35\4\0\1\34\1\35\1\0\1\57"
"\2\42\7\0\1\42\4\0\2\62\7\0\1\62\4\0"
"\1\0\3\12\1\0\27\12\1\0\12\12\1\0\5\12\44\0\1\12"
"\3\12\1\0\1\12\1\0\1\12\2\0\2\12\1\0\4\12\1\0"
"\5\0\13\12\25\0\12\2\7\0\143\12\1\0\1\12\17\0\2\12"
"\1\47\4\0\2\27\7\0\1\50\4\0\1\3\1\4"
"\4\0\1\30\1\31\7\0\1\32\5\0\1\33\7\0"
"\1\0\1\12\1\0\2\12\3\0\2\12\3\0\3\12\3\0\10\12"
"\11\0\12\2\3\12\23\0\1\12\1\0\33\12\123\0\46\12\u015f\0"
"\1\0\1\1\3\2\1\3\1\1\13\0\1\2\3\4"
"\0\u0214\0\u0222\0\u0230\0\u023e\0\u024c\0\u025a\0\124\0\214"
"\1\0\1\61\1\0\1\11\2\52\1\0\1\26\4\0"
"\2\12\52\0\5\12\12\0\1\13\124\13\10\13\2\13\2\13\132\13"
"\2\27\1\0\1\56\2\0\1\56\2\0\1\50\4\0"
"\1\0\2\12\4\0\12\2\25\0\10\12\1\0\3\12\1\0\27\12"
"\122\12\6\0\7\12\1\0\77\12\1\0\1\12\1\0\4\12\2\0"
"\1\3\4\0\1\3\1\4\1\0\1\14\1\0\1\11"
"\2\12\42\0\1\12\37\0\12\2\26\0\10\12\1\0\42\12\35\0"
"\0\u0268\0\u0276\0\u0284"
"\1\44\6\0\1\17\6\0\1\45\4\0\1\24\1\25"
"\15\0\1\45\4\0\1\24\1\25\7\0\1\46\15\0"
"\1\12\3\0\3\12\1\0\7\12\3\0\4\12\2\0\6\12\4\0"
1154
"\1\0\1\11\3\1\1\11\1\1\13\0\4\1\2\0"
"\4\0\1\24\1\25\1\0\1\53\1\0\1\11\2\54"
"\0\0\0\16\0\34\0\52\0\70\0\16\0\106\0\124"
keyword
newCharBuffer
MIN_BUFFER_SIZE
initTermBuffer
growTermBuffer
"term="
collationKey
"termVectorOffsets"
",lazy"
",omitTermFreqAndPositions"
",omitNorms"
"termVectorPosition"
BEST_COMPRESSION
Deflater
compressor
deflate
inflate
decompressor
Inflater
compressionLevel
setInput
early,
DATE_LEN
365
"representation
MAX_DATE_STRING
MIN_DATE_STRING
forDigit
"time
late,
"yyyyMMdd"
MONTH_FORMAT
SECOND_FORMAT
calInstance
"yyyyMM"
MILLISECOND_FORMAT
MINUTE_FORMAT
YEAR_FORMAT
"minute"
"millisecond"
"yyyyMMddHHmm"
"yyyyMMddHHmmss"
"day"
"yyyyMMddHH"
DAY_FORMAT
HOUR_FORMAT
"yyyyMMddHHmmssSSS"
NO_FIELDS
NO_FIELDABLES
NO_STRINGS
"Document<"
NO_BYTES
neither
"tokenStream
unstored"
byte[]
internName
non-binary
"binary
sense
selection
fieldSelections
POSITIVE_PREFIX
"0000000000000"
NEGATIVE_PREFIX
prefix"
RADIX
"1y2p0ij32e8e7"
padLen
numericTS
getNumericValue
lazyFieldsToLoad
newDeleteTerms
doTermSort
newDeleteDocIDs
newDeleteQueries
newLevel
allocator
newUpto
nextSlice
nextIndex
firstSize
offset0
offsetEnd
numBuffer
userDataString
LAST
permanently
deletedDocs.count()="
index.\n"
exception:"
segInfoStat
Y]\n"
exits
segment,
-segment"
"FORMAT
testAsserts
terms,
terms;
indexPath
undeleted
index\n"
any\n"
prior]"
"FORMAT_LOCKLESS
many)
detected"
problematic
0.\n"
(containing
-fix:
"SegmentReader.maxDoc()
count;
-fix,
documents)
"FORMAT_USER_DATA
docStoreIsCompoundFile="
fields.............."
tokens]"
"FORMAT_SINGLE_NORM_FILE
YOUR
(perhaps
testFieldNorms
pairs;
"docCount="
Always
org.apache.lucene.index.CheckIndex
fields]"
(MB)="
2.4]"
[Lucene
cantOpenSegments
pathToIndex
newSegments
"specified.
fix
actually
fixIndex
sizeMB
"corruption,
_2
"\nERROR:
"documents
actively
2.9]"
information\n"
Pre-2.1]"
2.2]"
"\nChecking
MySegmentTermDocs
[-fix]
[-segment
"FORMAT_DEL_COUNT
SegmentInfos.docCount
make\n"
X:
exitCode
CTRL+C!"
2.3]"
totLoseDocCount
"fixIndex()
numFiles="
on;
userData="
to.
"-segment"
option\n"
'-segment
thorough
'-ea:org.apache.lucene...',
2.1]"
testStoredFields
X]
docs:
"too
emergency
doFix
"FORMAT_CHECKSUM
freq0
freq,
maxDoc()="
were\n"
compiled
THIS
"FORMAT_SHARED_DOC_STORE
assertsOn
"int="
toLoseDocCount
exiting"
deletionsFileName
norms........."
deletions
segments.
"FORMAT_HAS_PROX
cause\n"
missingSegments
missingSegmentVersion
"**WARNING**:
affected\n"
docFreq="
toolOutOfDate
removed.\n"
"Stored
"\nNOTE:
myTermDocs
"FORMAT_DIAGNOSTICS
basis
this!
[delFileName="
fields......."
segmentFormat
lost,
"FAILED"
"OK
re-compile
numFiles
"-fix"
numBadSegments
vectors........"
saw
segmentsChecked
TermVectorStatus
segments:"
IS
doc]"
-segment
test:
TermIndexStatus
warned!\n"
seconds;
lost\n"
issues
Lucene;
FieldNormStatus
StoredFieldStatus
segments\n"
assertions
CHANCE
-fix
broken
sFormat
reader........."
"Writing..."
times,
docStoreCompoundFile
"Segments
prox..."
[newer
tool]"
_a'.\n"
term/freq
hasProx="
testTermIndex
"\nOpening
terms/docs
docs]"
problems
specified\n"
multiple\n"
fileOffset
sub-file
"Already
renameFile
(id:
"Difference
finalLength
directoryOffset
"Non-zero
startPtr
endPtr
copying:
dataOffset
performed"
consider
"priority
stalled
startMerge
addMyself
allInstances
CompareByMergeDocCount
maxMergeCount
"unpause
setMergeThreadPriority
getRunningMerge
MIN_PRIORITY
merges;
mergeThreadCount
pending;
initMergeThreadPriority
getMaxMergeCount
"setTestMode()
LuceneTestCase"
maxThreadCount
mergeThreads
threads;
called;
mergeThread
"CMS:
mergeThreadPriority
setRunningMerge
tWriter
Merge
return"
updateMergeThreads
launch
suppressExceptions
"count
"pause
stalling..."
case's
getCurrentMerge
runningMerge
startStallTime
anyExceptions
super.setUp
lastProxPointer
storesPayloads
lastFreqPointer
curStorePayloads
curFreqPointer
curProxPointer
lastSkipPayloadLength
lastSkipProxPointer
proxOutput
freqOutput
curPayloadLength
lastSkipFreqPointer
lastSkipDoc
doReopenFromWriter
"IndexReader
oldBytes
segmentInfosStart
IndexWriter.getReader()
readerTermDocs
operations"
doReopenNoWriter
false)"
readerShared
numSubReaders
ReaderCommit
openReadOnly=true
delete,
undelete,
segmentReaders
numMatchingSegments
matchingSegmentPos
oldReaderIndex
matchingSegments
twoFields
twoThreadsAndFields
oneFields
oneThreadsAndFields
twoDoc
oneDoc
hashPos2
(longer
fieldHash
fieldGen
hashMask
newHashMask
totalFieldCount
thisFieldGen
newHashSize
lastPerField
newHashArray
immense
"...'"
numDocFields
skipped.
fp0
nextFP0
endChildFields
endChildThreadsAndFields
"maxFieldLength
startLength
valueLength
doInvert
anyToken
SingleTokenAttributeSource
newFlushedSize="
trigger="
initFlushState
"docWriter:
docs/MB="
perDocAllocator
RAM:
flushPending
nextWriteLoc
minThreadState
"closeDocStore:
skipDocWriter
lastDeleteTerm
waitingBytes
nextDocID
charBlockFree="
doBalanceRAM
free:
allocMB="
BYTES_PER_DEL_QUERY
numBytesAlloc
addDeleteQuery
waitQueueResumeBytes
"apply
docIDStart
deletesRAMUsed
freeTrigger
nextWriteDocID
1.05
perDocFree="
BYTES_PER_DEL_DOCID
numDocsInRAM
waitReady
segments."
allThreadsIdle
deletesInRAM
docEnd
timeToFlushDeletes
freeIntBlocks
onlyDocStore
oldRAMSize="
numWaiting
byteBlockFree="
waitQueue
numBytesUsed
nothing
100.0
waitQueuePauseBytes
freeCharBlocks
deletesMB="
bufferIsFull"
flushState
newSegmentSize
flushTrigger
bufferIsFull
docStart
startBytesAlloc
success2
BYTES_PER_DEL_TERM
deletesFull
new/old="
threadStates
threadBindings
freeLevel
pauseThreads
docIDUpto
docIdInt
"DW:
setFlushPending
deletesFlushed
freedMB="
freqProxWriter
toMB
allocations:
CHAR_NUM_BYTE
WaitQueue
PER_DOC_BLOCK_SIZE
usedMB="
addDeleteTerm
addDeleteDocID
newFiles
numBlocks
free;
blockCount
free"
checkDeleteTerm
initSegmentName
infosEnd
triggerMB="
SkipDocWriter
delTerm
"lastTerm="
delTerm"
flushedFile
writeDocument
doResume
balanceRAM
waitForWaitQueue
nu
FORMAT_START
STORE_PAYLOADS
FORMAT_PRE
byNumber
STORE_TERMVECTOR
IS_INDEXED
addInternal
storePositionsWithTermVector
getAttributeSource
bytesize
getFieldStream
localFieldsStream
acceptField
getPointer
"indexSize="
fieldsStreamTL
getToRead
isCompressed
setToRead
uncompress
cloneableIndexStream
formatSize
"compressed
isOriginal
lower"
seekIndex
setPointer
cloneableFieldsStream
addFieldSize
addFieldLazy
2.9"
LazyField
skipField
storedCount
ioe2
FilterTermDocs
"FilterReader("
totalNumDocs="
"docID="
docsWriter
postingUpto
minState
termStates
mergeStates
numAllFields
compareText
allFields
numToMerge
termsUTF8
"FreqProxTermsWriterPerField.newTerm
proxCode
"FreqProxTermsWriterPerField.addTerm
payloadAttribute
writeProx
otherCommit
"IFD
currentCommitPoint
initDone
DecRef
"refresh
re-try
[prefix="
"setInfoStream
pre-incr
pre-decrement
RefCount
"RefCount
deletionPolicy="
CommitPoint
commitsToDelete
lastFiles
oldDeletable
pre-increment
pre-decr
deletable
refCounts
"forced
deleteCommits
Will
segmentPrefix2
segmentPrefix1
deleteFiles
readFrom
point"
"deleteCommits:
VERBOSE_REF_COUNTS
docWriterFiles
IncRef
"f\\d+"
extensionsInCFS
isCFSFile
"s\\d+"
"fnm"
GEN_EXTENSION
"tii"
"tvf"
"fdx"
"nrm"
"prx"
"tis"
"cfx"
"tvx"
"frq"
"fdt"
"tvd"
reopen()."
"INDEXED_NO_TERMVECTOR"
reopen(IndexCommit)."
cfr
"INDEXED_WITH_TERMVECTOR"
<cfsfile>"
"TERMVECTOR_WITH_POSITION"
clone()"
dirname
"-extract"
[-extract]
"TERMVECTOR_WITH_OFFSET"
directory..."
"STORES_PAYLOADS"
"INDEXED"
directory2
org.apache.lucene.index.IndexReader
"UNINDEXED"
fldOption
"TERMVECTOR"
getUniqueTermCount()"
bufLen
option
getAndDecrement
"OMIT_TERM_FREQ_AND_POSITIONS"
"TERMVECTOR_WITH_POSITION_OFFSET"
mergeExceptions
"startCommitMerge"
dss
getLogMergePolicy
numDocsInRAM="
flushDeletes="
docStore
"commitMergeDeletes
pendingMerges
pooled
pendingCommitChangeCount
newDelFileName
"setMergeScheduler
sReader
null;
selected
defaultInfoStream
setMessageID
"midStartCommitSuccess"
mergeSuccess
deleteAll"
registerMerge
newMerge
"rollback()
numBufDelTerms="
"deleteAll"
numRamDocs
ramSizeInBytes
setDefaultInfoStream
oom
rollbackInternal
"startCommitMergeDeletes"
CFS"
handle"
setRollbackSegmentInfos
addIndexesNoOptimize"
poolReaders
"merge:
expungeDeletes"
flushDocStores="
LogDocMergePolicy"
"startApplyDeletes"
runningMerges
ensureContiguousMerge
flushDeletesCount
"mergeFactor"
doFlushDocStore
finishSync
(currently)
startCommit():
drop
toSync
handleOOM
resolveExternalSegments
waitForAllSynced
pushMaxBufferedDocs
"IW
deletes"
"optimize:
docUpto
flushing
"done
"expungeDeletes:
syncs"
totDocCount
haveReadLock
mergeMiddle"
currentDocStoreSegment
sourceSegments
flushDocs="
triggerMerge
commitMergedDeletes
OutOfMemoryError;
acquireRead
"Directory
resetMergeExceptions
_mergeInit
getIfExists
numMerges
startSync
"setMergePolicy
non-null"
"midStartCommit2"
getDefaultInfoStream
syncing
transaction"
pendingMerges:
rollback"
infoIsLive
1;
localFlushedDocCount
"midStartCommit"
"mergePolicy="
infos"
superseded
dups
mergeMiddle
"rollbackInternal"
commitLock
resumeAddIndexes
rollbackTransaction
"addIndexesNoOptimize"
"setMaxBufferedDeleteTerms
waitForMerges
messageID
merge\n
stopMerges
wrote
"deleteDocuments(Term)"
startTransaction
"dup
localRollbackSegmentInfos
"setMaxFieldLength
includePendingClose
"create
"updateDocument"
exc="
"mergeDocStores"
writeThread
non-contiguous
"User-specified"
upgradeCount
"handleMergeException:
"\ndir="
lastDir
getNextExternalMerge
"maxNumSegments
numBufferedDeleteTerms="
mergingSegments
corresponding
"addDocument"
MESSAGE_ID_LOCK
"prepareCommit:
[total
addMergeException
commitTransaction
"setRAMBufferSizeMB
changeCount="
readerMap
pending]"
"IndexCommit's
commitMerge
ReaderPool
closing"
"finishStartCommit"
"startCommit():
numSegmentsToMerge
MERGE_READ_BUFFER_SIZE
"startCommit
"merging
lastCommitChangeCount
"background
flushDocs
"commit:
"startStartCommit"
flushDeletes
optimizeMergesPending
addIndexes*
"sync
"UNLIMITED"
mfl
maybeMerge
mergedName
flushDocStores
upgradeReadToWrite
releaseRead
commit()"
dir="
doFlushInternal
"abort
hitOOM
sizeInBytes="
myChangeCount
"dss="
mergeDocStoreSegment
updatePendingMerges
"deleteDocuments(Term..)"
"closeInternal"
releaseWrite
setPending
index="
oldInfos
committing
"calling
"LIMITED"
rollbackSegments
blockAddIndexes
lastDocStoreSegment
decrefMergeSegments
"write.lock"
LogMergePolicy"
"addIndexes(IndexReader...)"
flushed
finishAddIndexes
mapToLive
prepared"
merge="
noDupDirs
messageState
closeInternal
"commitMerge:
changeCount
pendingCommit
mergeReader
prepare"
"startCommit"
doFlush
"version="
shouldClose
getReader"
compoundFileName
"MergePolicy
"MergeScheduler
"setMaxBufferedDocs
acquireWrite
itself"
finishMerges
"delPolicy="
"termIndexInterval="
"matchVersion="
"maxBufferedDocs="
"openMode="
mergeSegmentWarmer
"maxFieldLength="
"mergedSegmentWarmer="
"maxBufferedDeleteTerms="
"ramBufferSizeMB="
DEFAULT_MAX_THREAD_STATES
"mergeScheduler="
WRITE_LOCK_TIMEOUT="
"similarity="
openMode
"commit="
"writeLockTimeout="
"maxThreadStates="
"analyzer="
1.6
DEFAULT_MAX_MERGE_MB
setMaxMergeMB
getMinMergeMB
DEFAULT_MIN_MERGE_MB
minMergeDocs
DEFAULT_MIN_MERGE_DOCS
getUseCompoundDocStore
"LMP:
calibrateSizeByDeletes
"findMergesToExpungeDeletes:
"findMerges:
maxMergeDocs;
maxLevel
segments"
levelBottom
sumSize
getCalibrateSizeByDeletes
skipping"
levelFloor
bestSize
finalMergeSize
setCalibrateSizeByDeletes
"mergeFactor
bestStart
levels
firstSegmentWithDeletions
LEVEL_LOG_SPAN
anyTooLarge
newStarts
oldDocID
maxDocID
lastDocCount
firstSegment
"MergeSpec:\n"
[mergeDocStores]"
[optimize]"
maxSegmentCount
paused
inputIsBuffered
skipDoc
maxNumberOfSkipLevels
numSkipped
toBuffer
loadNextSkip
loadSkipLevels
numberOfLevelsToBuffer
lastChildPointer
SkipBuffer
numLevels
newChildPointer
TermPositionsQueue
_index
_freq
_array
_arraySize
_doc
growArray
_lastIndex
_posList
IntQueue
_termPositionsQueue
"MultiReader
newSubReaders
uptos[0]="
actual="
normsFileName
normsOut
expected="
normCount
minLoc
byField
".nrm
defaultNorm
toMerge
ParallelTermEnum
incRefReaders
"ParallelReader("
ParallelTermPositions
firstKey
fieldIterator
SortedMap
fieldToReader
readerToFields
tailMap
oldReader
maxDoc:
ignoreStoredFields
storedFieldReaders
"ParallelReader
ParallelTermDocs
numDocs:
currentPositions
Mapper"
posVal
(it
patternLength
CHECK_DIR
"->"
pendingDelCount
delGen
hasSingleNormFile
".s"
dss="
clearFiles
setDocStoreOffset
"delCount="
"dso="
isCompoundFile
addIfExists
listAll()
fieldIndex
numNormGen
normGen
preLockless
defaultGenFileRetryPauseMsec
nextGeneration
genLookaheadCount
genOutput
genA="
listing
"fallback
segnOutput
checksumThen
prevSegmentFileName
genInput
lastGeneration
"success
setDefaultGenFileRetryCount
gen0
gen1
err2
getDefaultGenLookahedCount
pendingSegnOutput
segments*
getDefaultGenFileRetryPauseMsec
defaultGenFileRetryCount
setDefaultGenLookaheadCount
ahead
check:
getLastGeneration
checksumNow
includeSegmentsFile
"primary
genB
defaultGenLookaheadCount
genA
"checksum
retry"
"segments.gen
prevExists
setDefaultGenFileRetryPauseMsec
"fileName
genB="
"secondary
"SIS
"look
getDefaultGenFileRetryCount
getNextSegmentFileName
"';
retry="
retry:
stiB
stiA
comparison
segmentFieldInfos
readerFieldInfos
tvxSize
10000.0
numFIs
corruption"
matchingFieldsReader
rawDocLengths
numReaderFieldInfos
mergeTerms
produced
mergeVectors
fInfos
4192
copyVectorsNoDeletions
numFieldInfos
matchingSegmentReaders
mergeFields
fdxFileLength
copyFieldsNoDeletions
matchSize
mergedDocs
smis
workCount
vectorsReader
copyFieldsWithDeletions
omitTFAndPositions
normBuffer
"mergeVectors
matchingVectorsReader
MAX_RAW_MERGE_DOCS
readerCount
rawDocLengths2
result:
matchingSegmentReader
"mergeFields
".fnm"
copyVectorsWithDeletions
mergeNorms
segmentReader
setMatchingSegmentReaders
mergeTermInfos
checkDeletedCounts
storesSegment
normInput
loadDeletedDocs
recomputedCount
tvReader
rollbackDeletedDocsDirty
tisNoIndex
termsIndexIsLoaded
reWrite
cnse
singleNormStream
fieldsReaderOrig
termVectorsReader
deletedDocsDirty
cloneDeletedDocs
getNorms
"refCount="
commit:
dir0
storeCFSReader
normsUpToDate
cfsDir
getCFSReader
normsDirty
differ
exactly
BitVector="
segmentInfo
singleNormRef
setSegmentInfo
closeInput
recomputed
cloneNormBytes
origNorm="
termVectorsLocal
origInstance
normSeek
singleNormFile
openNorms
pendingDeleteCount
getFieldsReaderOrig
DirectoryReader"
fieldsReaderLocal
cloneBytes
single-segment
CloneNotSupportedException"
usesCompoundFile
fieldNormsChanged
oldRef
bytesOut
termVectorsReaderOrig
FieldsReaderLocal
storeDir
CoreReaders
getTermVectorsReader
deletionsUpToDate
nextNormSeek
rollbackPendingDeleteCount
rollbackNormsDirty
rollbackDirty
copyOnWrite
segmentTermEnum
readNoTf
docCode
skipListReader
formatM1SkipInterval
"indexInterval="
negative;
version:"
"skipInterval="
prevBuffer
scanBuffer
higher"
retOffset
"TermPositions
skipPositions
lazySkip
readDeltaPosition
skipPayload
lazySkipProxCount
needToLoadPayload
call.
proxCount
lazySkipPointer
EMPTY_TERM_POS
release()
MyCommitPoint
"snapshot
wrapCommits
snapshot()
set;
myCommits
newPositions
"_ALL_"
existingOffsets
newOffsets
termToTVE
existingPositions
fieldsIdxName
"StoredFieldsWriter.finishDocument
initFieldsWriter
localFieldsWriter
storedFieldsWriter
"doc.numStoredFields="
"StoredFieldsWriterPerThread.processFields.writeField"
termLen
compareChars
chars2
chars1
enumOffset
termsCache
"indexDivisor
totalIndexInterval
termOrd
-1
indexInfos
indexPos
tiOrd
TermInfoAndOrd
index)
indexPointers
ensureIndexIsRead
seekEnum
indexOffset
indexTerms
mustSeekEnum
sameTermInfo
DEFAULT_CACHE_SIZE
indexEnum
numScans
ThreadResources
getIndexOffset
threadResources
ti1
ti2
getThreadResources
writeTerm
lastText="
lastTi
"freqPointer
lastFieldNumber
".tis"
termBytesLength
utf16Result1
".tii"
utf16Result2
compareToLastTerm
lastTermBytesLength
initUTF16Results
(number
"proxPointer
lastIndexPointer
lastField="
text="
nextThreadsAndFields
shrinkFreePostings
"TermsHash.getPostings
postingsFreeChunk
consumer="
postingsFreeCount="
postingsFreeList
nextChildFields
"postingsFreeCount="
postingsFreeCount
postingsAllocCount="
numToFree
create"
postingsAllocCount
newPostingsAllocCount
downto
postingEquals
textUpto
intUptos
doNextCall
postingsHashSize
postingsHashHalfSize
comparePostings
postingsHash
rehashPostings
streamCount
intUptoStart
numPostingInt
newMask
tokenTextLen
postingsCompacted
doCall
compactPostings
tokenPos
newHash
textLen1
postingsHashMask
"postings["
"consumer="
noNullPostings
null:
"TermVectorEntry{"
frequency="
"field='"
entry1
checkValidFormat
formats"
seekTvx
storingPositions
readFields
FORMAT_VERSION
preUTF8
tvfFormat
tvdFormat
readTermVectors
lastTvdPosition
currentPosition
lastTvfPosition
byteBuffer
prevOffset
FORMAT_VERSION2
16L
FORMAT_UTF8_LENGTH_IN_BYTES
tvfPointer
readTvfPointers
FORMAT_SIZE
"numTotalDocs="
less"
prevPosition
deltaLength
tvfPointers
readTermVector
storingOffsets
fieldNumbers
"TermVectorsTermsWriter.finishDocument
initTermVectorsWriter
fldName
doVectors
"TermVectorsTermsWriterPerField.finish
charBuffers
doVectorPositions
lastTermBytesCount
doVectorOffsets
maxNumPostings
"TermVectorsTermsWriterPerField.newTerm
encoderUpto
"TermVectorsTermsWriterPerField.addTerm
termBytesCount
clearLastVectorFieldName
lastVectorFieldName
fieldPointer
tvfStart
lastFieldPointer
utf8Upto
tvdStart
tpVector
arguments
3077643314630884523L
makeAccessible
MissingResourceException
messageKey
validateMessage
isFieldAccessible
FINAL
bundleName
loadfieldValue
bundles
MOD_MASK
STATIC
PUBLIC
PrivilegedAction
getResourceBundleObject
"WARN:
fieldArray
MOD_EXPECTED
resourceBundle
applySlop
"None-hex
0x3ed2000
newPrefixQuery
newBooleanClause
jj_la1_init_1
newBooleanQuery
newMultiPhraseQuery
0x3ed0000
character."
newWildcardQuery
jj_3R_3
jj_la1_1
WildcardQuery"
null."
sequence:
fieldToDateResolution
xsp
0x4000000
0x2690000
"'*'
jj_3R_2
0x3ed3f00
newFuzzyQuery
0x30000000
0x90000
PrefixQuery"
0x3ffffff01L
jjStartNfaWithStates_3
0x4000000L
"\52"
0x40000000L
"Failure
coordFactor
"coord("
shouldMatchCount
condition(s)
sumExpl
BooleanWeight
cIter
"maxClauseCount
wIter
meet
maxClauseCount
minimum
required/prohibited
numProhibited
clause(s)"
BooleanScorerCollector
buckets
nextMask
"boolean("
BucketScorer
MASK
newCollector
requiredMask
BucketTable
bucketTable
subScorerDocID
Bucket
SubScorer
prohibitedMask
SingleMatchScorer
dualConjunctionSumScorer
countingSumScorer
makeCountingSumScorerNoReq
Coordinator
lastScoredDoc
defaultSimilarity
countingConjunctionSumScorer
nrOptRequired
addProhibitedScorers
requiredNrMatchers
countingDisjunctionSumScorer
coordinator
requiredScorers
allReq
requiredCountingSumScorer
makeCountingSumScorerSomeReq
makeCountingSumScorer
req1
req2
lastDocScore
CachingSpanFilter
getCachedResult
"CachingSpanFilter("
docIdSetToCache
"CachingWrapperFilter("
"(MATCH)
"(NON-MATCH)
firstScorer
getDiscountOverlaps
setDiscountOverlaps
numDisjunctions
disjunctQuery
currentWeight
DisjunctionMaxWeight
heapify
rchild
rscorer
lscorer
lchild
scoreAll
rdoc
heapAdjust
heapRemoveRoot
numScorers
matchers
subScorers"
scorerDocQueue
advanceAfterCurrent
minimumNrMatchers
initScorerDocQueue
nrScorers
toHtml
"</ul>\n"
"<br
"<li>"
"</li>\n"
"<ul>\n"
=~
".DEFAULT_DOUBLE_PARSER"
".NUMERIC_UTILS_FLOAT_PARSER"
".NUMERIC_UTILS_DOUBLE_PARSER"
(size
".DEFAULT_INT_PARSER"
".DEFAULT_BYTE_PARSER"
setEstimatedSize
"'=>"
".DEFAULT_LONG_PARSER"
".NUMERIC_UTILS_INT_PARSER"
".NUMERIC_UTILS_LONG_PARSER"
"0.#"
".DEFAULT_SHORT_PARSER"
STRING_INDEX
".DEFAULT_FLOAT_PARSER"
"\nStack:\n"
created\nDetails:
ByteCache
entryKey
StringCache
ShortCache
termval
FloatCache
DoubleCache
mterms
insanities
CacheEntryImpl
StringIndexCache
IntCache
LongCache
printNewInsanity
readerCacheEntry
ne
cacheType
FieldCacheDocIdSet
upperPoint
shortValue
inclusiveUpperPoint
mayUseTermDocs
1948649653
byteValue
1721088258
getLowerVal
365038026
1572457324
1674416163
inclusiveLowerPoint
550356204
lowerPoint
getUpperVal
1549299360
openBitSet
FieldCacheTermsFilterDocIdSet
FieldCacheTermsFilterDocIdSetIterator
midVal
hasCollators
collators
numComparators
"slot:"
OneComparatorFieldValueHitQueue
MultiComparatorsFieldValueHitQueue
oneReverseMul
_innerSet
iterator"
_innerIter
preBoost
filter:
"filtered("
advanceToCommon
")->"
manager
fia
setCleanThreadSleepTime
fcThread
sortedFilterItems
setCacheSize
FilterCleaner
cleanSleepTime
filterCleaner
cacheCleanSize
DEFAULT_CACHE_SLEEP_TIME
DEFAULT_CACHE_CLEAN_SIZE
FilterItem
"maxExpansions
maxExpansions
defaultMaxExpansions
termLongEnough
calculateMaxDistance
maxDistance
scale_factor
fullSearchTermLength
bestPossibleEditDistance
realPrefixLength
prePopulate
fieldSortDoMaxScore
filterDocIdSet
"nDocs
fieldSortDoTrackScores
deBasedDoc
filterIter
filterDoc
doTrackScores
searchWithFilter
MatchAllDocsWeight
"MatchAllDocsQuery,
MatchAllScorer
0x1AA71190
termArraysEquals
termArray1
boq
termArray2
MultiPhraseWeight
termArrays2
iterator1
0x4AC65113
termArrays1
termArraysHashCode
aggregatedDfs
CachedDfSource
subDoc
cacheSim
fieldDoc
fd
getSearchables
BooleanQueryRewrite
setDocCountPercent
docVisitCount
hasCutOff
CutOffTermCollector
collectTerms
stQueue
termCountLimit
ConstantScoreAutoRewrite
numberOfTerms
100.
ConstantScoreBooleanQueryRewrite
ScoringBooleanQueryRewrite
docCountPercent
setTermCountCutoff
getTermCountCutoff
ConstantScoreFilterRewrite
getDocCountPercent
TermCollector
docCountCutoff
pendingTerms
1279
350
TopTermsBooleanQueryRewrite
DEFAULT_TERM_COUNT_CUTOFF
DEFAULT_DOC_COUNT_PERCENT
termCountCutoff
0x14fa55fb
rangeBounds
0x733fa5fe
termTemplate
currentUpperBound
0x64365465
0x4565fd66
NumericRangeTermEnum
CountTotalHits
aggregatedDocFreqs
searchThreads
DocumentFrequencyCallable
AggregateDocFrequency
foreach
CountDocFreq
PhraseWeight
pieces
pp1
getPrefixTerm
uniques
allClauses
mergeBooleanQueries
splittable
clause2
coordDisabled
processTerms
integer
tmpList
tmpFreqs
tmpSet
"QueryWrapperFilter("
toNonExcluded
exclDisi
reqDoc
exclDoc
optScorer
reqScore
optScorerDoc
curScore
getNormDecoder
maxDocs="
NORM_TABLE
defaultImpl
decodeNorm
NO_DOC_ID_PROVIDED
"idf(docFreq="
fIdf
delegee
tpPos2
tmpPos
termPositionsDiffer
initPhrasePositions
tpPos
checkedRepeats
pp3
tpsDiffer
0x45aaf665
"<short:
"<double:
DOC"
"<long:
0x08150815
"<???:
numeric
"<int:
"<string_val:
0xff5685dd
0x346565dd
0x3aaf56ff
"<score>"
"<float:
"<string:
"<custom:\""
initFieldType
0xaf5998bb
"<byte:
StartEnd
"SpanQueryFilter("
currentInfo
"tf(termFreq("
TermWeight
More
Less
checkLower
upperTermText
startTermText
lowerTermText
scoreCache
weightValue
SCORE_CACHE_SIZE
pointerMax
"Elapsed
thread"
newResolution
isGreedy
ms."
TimerThread
"TimeLimitedCollector
getMilliseconds
DEFAULT_GREEDY
TIMER_THREAD
OneComparatorScoringNoMaxScoreCollector
trackMaxScore
updateBottom
MultiComparatorScoringMaxScoreCollector
MultiComparatorNonScoringCollector
OneComparatorScoringMaxScoreCollector
OneComparatorNonScoringCollector
OutOfOrderOneComparatorScoringNoMaxScoreCollector
MultiComparatorScoringNoMaxScoreCollector
OutOfOrderOneComparatorNonScoringCollector
OutOfOrderMultiComparatorScoringNoMaxScoreCollector
OutOfOrderMultiComparatorNonScoringCollector
trackDocScores
OutOfOrderMultiComparatorScoringMaxScoreCollector
EMPTY_SCOREDOCS
queueFull
OutOfOrderOneComparatorScoringMaxScoreCollector
OutOfOrderTopScoreDocCollector
InOrderTopScoreDocCollector
pqTop
termContainsWildcard
termIsPrefix
pEnd
stringIdx
preLen
wildcardEquals
justWildcardsLeft
searchTermText
patternIdx
sEnd
WILDCARD_CHAR
WILDCARD_STRING
wildchar
cidx
wildcardSearchPos
valSrcQuery
valSrcQueries
provider
STRICT"
valSrcScorers
valSrcWeights
subQueryWeight
isStrict
strict
"queryBoost"
CustomWeight
vScores
qStrict
doExplain
"<subquery>
customExp
purposes
computed
avgVal
compute
Score
"::"
Type!"
ValueSourceWeight
MinPayloadFunction
avgPayloadScore
"payloadNear(["
processPayloads
boostingNearQuery
"bnq,
spansArr
PayloadNearSpanScorer
PayloadNearSpanWeight
payLoads
queryToSpanQuery
PayloadTermSpanScorer
getPayloadScore
"btq,
similarity1
getSpanScore
processPayload
PayloadTermWeight
includeSpanScore
maskedQuery
"mask("
maskedField
start1
prevStart
allowedSlop
ppStart
end1
end2
advanceAfterOrdered
matchStart
lastEnd
toSameDoc
matchSlop
possiblePayload
NearSpansOrdered.toSameDoc()
stretchToOrder
prevEnd
shrinkToAfterShortestMatch
matchEnd
ppEnd
lastStart
possibleMatchPayloads
inSameDoc
spanDocComparator
subSpansByDoc
prevSpans
"Less
listToQueue
cell
SpansCell
atMatch
queueStale
initList
queueToList
addToList
CellQueue
"spanFirst("
spanFirstQuery
"spanNear(["
0x99AFD3BD
includeSpans
moreExclude
"spanNot("
excludeSpans
rewrittenExclude
moreInclude
rewrittenInclude
theTop
"])"
SpanQueue
initSpanQueue
"spanOr(["
skipCalled
expDoc
tfExpl
checkBufferSize
getBufferSize
leftInBuffer
"bufferSize
buffer.length="
"buffer="
bufferSize="
pieceLength
closeDirSrc
primaryExtensions
getPrimaryDir
getSecondaryDir
primaryFiles
secondaryFiles
listed:
DIGESTER
createDir
setReadChunkSize
list()
"chunkSize
DEFAULT_READ_CHUNK_SIZE
overwrite:
0x7FL
readModifiedUTF8String
0xFFFFFFFFL
COPY_BUFFER_SIZE
"numBytes="
non-negative
sleepCount
out:
maxSleepCount
"lockWaitTimeout
LOCK_OBTAIN_WAIT_FOREVER
pointing
locking
verifierHost
use\n"
obtain/release\n"
process)\n"
sleepTime\n"
verify\n"
lockFactoryClassName
listening
0..255"
LockStressTest
LockClass
"InstantiationException
correctly.\n"
on\n"
"test.lock"
betweeen
own\n"
(only
running\n"
"myID
verifierPort
LockFactory"
LockVerifyServer.\n"
"IllegalAccessException
org.apache.lucene.store.LockStressTest
ID,
verifyLF
myID
verifierHostOrIP
IP
sleepTimeMS
Simple/NativeFSLockFactory\n"
org.apache.lucene.store.LockVerifyServer
lockedID
port\n"
"\nReady
released
"s]
holds
setReuseAddress
lock,
maxBBuf
hack
getMaxChunkSize
bufNr
"Unmap
nrBuffers
getUseUnmap
mapped
PrivilegedExceptionAction
useUnmapHack
PrivilegedActionException
setMaxChunkSize
MapMode
setUseUnmap
MMapIndexInput
cleanMapping
maxBufSize
rafc
maxBufSize:
buffer"
UNMAP_SUPPORTED
BufferUnderflowException
platform!"
mmap
bufSize
curBuf
curAvail
"java.nio.DirectByteBuffer"
unmap
"Maximum
raf
cleaner
MultiMMapIndexInput
bufSizes
"RandomAccessFile
getCleanerMethod
>0"
"cleaner"
"sun.misc.Cleaner"
bufOffset
curBufIndex
"Non
acquireTestLock
LOCK_HELD
"-test.lock"
markedHeld
randomLockName
FileLock
forcefully
acquire
lockExists
NativeFSLock
"NativeFSLock@"
canonicalPath
component:
lock;
filesystem
otherByteBuf
otherBuffer
readOffset
NoLock
"NoLock"
singletonLock
enforceEOF
RAMFile!
buflen
setFileLength
chunks
Descriptor
"SimpleFSLock@"
SimpleFSLock
createNewFile
SingleInstanceLock
CheckedLock
acquired"
MIN_RADIX
"chars
0x7ffffff8
0x7ffffffe
0x7ffffffc
bytesPerElement
parse"
"=null"
attImpl
getClassForInterface
curInterface
evicted"
interfaces,
currentState
fulfil
curInterfaceRef
knownImplClasses
initState
target"
implementing
strong
targetImpl
accepts
thisState
DefaultAttributeFactory
foundInterfaces
otherState
"addAttribute()
attributeImpls
actClazz
contract."
Attribute,
attClassImplMap
"Impl"
hasAttributes
AttributeSource"
computeCurrentState
isPowerOfTwo
ntzTable
0x5555555555555555L
0x0F0F0F0F0F0F0F0FL
twosB
twosA
tot8
lowByte
fours
wordOffset
0x3333333333333333L
foursA
twos
0x0000000F
eights
0x00000003
foursB
BYTE_COUNTS
copyBits
"bit
writeBits
readBits
bitsToClear
"bit="
writeDgaps
0.."
readDgaps
isSparse
"offset
"buffersize
Java5CharacterUtils
JAVA_5
JAVA_4
lastTrailingHighSurrogate
Java4CharacterUtils
iv
hardRefs
weakRef
"1.2."
"os.name"
JAVA_1_1
JAVA_1_3
JAVA_1_2
"1.3."
pkg
"SunOS"
"1.1."
SUN_OS
"Windows"
"Linux"
"-dev
ident
"sun.arch.data.model"
"3.1"
"64"
LINUX
getBitSet
DocIdBitSetIterator
lockInterruptibly
Condition
newCondition
"thirteen
"eight
"twelve
1000000000000l
"quintillion,
"quadrillion,
"eleven
1000000000000000l
"nineteen
"sixteen
"seven
"ten
"fifteen
"trillion,
"minus
1000000000000000000l
"eighteen
"fourteen
"million,
1000000000
"billion,
"seventeen
"thousand,
"twenty"
rfMap
badEntries
valIdToItems
valId
readerFieldToValIds
"SUBREADER"
InsanityType"
checkValueMismatch
readerFields
non-null/non-empty
getMsg
cacheEntries
badChildren
ReaderField
badness
kidKey
kids
checkSubreaders
getAllDecendentReaderKeys
valMismatchKeys
sanityChecker
CacheEntry[]"
viToItemSets
EXPECTED
decendents
"EXPECTED"
rfToValIdSets
setRamUsageEstimator
badKids
"Insanity
"VALUEMISMATCH"
valMap
numInputChars
outputOffset
outputLength
middleShift
"Arguments
initialShift
numOutputBytes
numFullBytesInFinalChar
CodingCase
finalShift
CODING_CASES
outputArray
finalMask
inputByteNum
codingCase
backing
numEncodedChars
caseNum
outputCharNum
inputArray
0x7FFF
arrays"
inputLength
"original
advanceBytes
inputChar
7L
14L
outputByteNum
middleMask
inputCharNum
15L
inputOffset
theSet
theMap
"%s-%d"
threadNumber
SecurityManager
threadPoolNumber
getThreadGroup
"%s-%d-thread"
ThreadFactory
ThreadGroup
threadNamePrefix
checkPrefix
NORM_PRIORITY
NAME_PATTERN
0..31"
prefixCoded
LONG?)"
INT?)"
(char
splitRange
0..63"
sortableBits
0x60
value,
hasUpper
nextMaxBound
floatToPrefixCoded
nextMinBound
invalid)"
prefixCodedToFloat
numerical
64."
hasLower
representation
wlen
setBits
setNumWords
0x98761234
otherArr
bits2words
ensureCapacityWords
bitmask
nWords
endWord
intersects
expandingWordNum
startWord
startmask
thisArr
wordNum
0x03f
trimTrailingZeros
endmask
getBit
bitSetDoc
curDocId
Parameter
par
"Parameter
allParameters
used!"
makeKey
StreamCorruptedException
ObjectStreamException
sentinel
sizeOfArray
classSize
arrayElementClazz
checkInterned
isPrimitive
NUM_BYTES_FLOAT
NUM_BYTES_DOUBLE
refSize
NUM_BYTES_SHORT
memoryModel
checkAdjustElsePop
adjustTop
topHSD
HeapedScorerDoc
popNoResult
cond
tableSize
nextToLast
maxChainLength
chainLength
numMantissaBits
fzero
zeroExp
smallfloat
initBytes
inputSize
sortedInts
lastInt
bytePos
resizeBytes
SortedVIntListBuilder
MAX_BYTES_PER_INT
newBytes
lastBytePos
BITS2VINTLIST_SIZE
negative."
interner
bytes1
bytes2
0xFFFD
0xf8
0xf0
outUpto
HALF_SHIFT
0x3FF
0xEF
0xBD
0xDFFF
0xc0
0xD7C0
0xDC00
UNI_MAX_BMP
HALF_BASE
chHalf
0xDBFF
HALF_MASK
LUCENE_22
LUCENE_21
subclazz
singletonSet
"assigned
singletons
"VirtualMethod
class,
reflectImplementationDistance
overridden
SynchronizedCache
synchronizedCache
swapped
cache1
countdown
cache2
secondary
LOADFACTOR
SynchronizedSimpleMapCache
"assertions
enabled!"
TestAssertions
TestDemo
isearcher
testDemo
indexed."
"fieldname"
25000
hitDoc
excCalled
testSubclassConcurrentMergeScheduler
MyMergeThread
mergeThreadCreated
"doMerge"
"MyMergeThread"
mergeCalled
TestMergeSchedulerExternal
FailOnlyOnMerge
TestSearch
e\""
"medium"
useCompoundFiles
testRun
MAX_DOCS
"priority"
HIGH_PRIORITY
MED_PRIORITY
PRIORITY_FIELD
LOW_PRIORITY
"low"
"high"
results\n"
TestSearchForDuplicates
numToRead
testSnapshotDeletionPolicy
backupIndex
dp
INDEX_PATH
"addDocument
testReuseAcrossWriters
"test.snapshots"
readFile
TestSnapshotDeletionPolicy
been"
checkClearAtt
76137213
getAndResetClearCalled
chain"
CheckClearAttributesAttributeImpl
"finalOffset
"clearAttributes()
clearCalled
"abac\uDC16adaba"
testLowerCaseFilter
"BAR"
"foo.bar.FOO.BAR"
highSurEndingUpper
testLowerCaseFilterBWComp
"abaca\ud801\udc3edaba"
PayloadSetter
verifyPayload
"BogustermBogusterm\udc16"
testStop
"AbaC\uDC16AdaBa"
testSubclassOverridingOnlyTokenStream
"bogustermboguster\ud801"
"<>"
"BogustermBoguster\ud801"
"\ud801\udc3e\ud801\udc3e\ud801\udc3e\ud801\udc3e"
"AbaCaDabA"
THESE
"FOO"
testLowerCaseFilterLowSurrogateLeftover
highSurEndingLower
testNull
'\udc3e'
"abaca\ud801\udc16daba"
"bogustermbogusterm\udc16"
"abac\uD801adaba"
"abacadaba"
LowerCaseWhitespaceAnalyzerBWComp
LowerCaseWhitespaceAnalyzer
testPayloadCopy
MyStandardAnalyzer
"AbaCa\ud801\udc16DabA"
_testStandardConstants
"AbaC\uD801AdaBa"
TestAnalyzers
"\"QUOTED\""
"\ud801\udc16\ud801\udc16\ud801\udc16\ud801\udc16"
"Ù"
"ẃ"
"ū"
"″"
"₆"
"❲"
"⒙"
"hv"
"Ⱬ"
"ǋ"
"Ṛ"
"ᴀ"
"Ṕ"
"Ç"
"Ꞇ"
"ʫ"
"⓮"
"ȼ"
"ʥ"
"⓸"
"Č"
"Ꝕ"
"ｌ"
"ẍ"
"Ǘ"
"Ỽ"
"⒦"
"ǅ"
"Œ"
"Ȣ"
"ʗ"
"Ṇ"
"ɡ"
"õ"
"！"
"ã"
"db"
"⑯"
"Ɲ"
"３"
"ᶒ"
"(j)"
"Ợ"
"Ⓤ"
"ḵ"
"ĺ"
"Ꝛ"
"ｂ"
"ɓ"
"ｐ"
"(13)"
"ḣ"
"Ẵ"
"ờ"
"Ꝉ"
"➋"
"ɝ"
"⑽"
"ń"
"Ȑ"
"ʁ"
"⒴"
"ǻ"
"ⓔ"
"Ả"
"Ȇ"
"Ⓔ"
"ṿ"
"Ꝿ"
"¹"
"ᶄ"
"Ố"
"Ə"
"Ɐ"
"Ꜵ"
"ǩ"
"ꜷ"
"Ŷ"
"❭"
"Ệ"
"(r)"
"Ɓ"
"⒈"
"ꜹ"
"Ğ"
"ₑ"
"ɏ"
"ḑ"
"Ầ"
"＿"
"Ƴ"
"ṩ"
"Ẑ"
"«"
"ḏ"
"Ø"
"Ẃ"
"₇"
"❱"
"⒞"
"Ū"
"(c)"
"ṛ"
"Ƥ"
"ᴇ"
"ğ"
"ṕ"
"ɶ"
"Æ"
"ƥ"
"Ẋ"
"LJ"
"ⓩ"
"ʦ"
"Ƚ"
"Ƙ"
"ｍ"
"Ẍ"
"(e)"
"17."
"ǖ"
"⒧"
"Ǆ"
"ʐ"
"ȣ"
"ɠ"
"â"
"⑮"
"2."
"２"
"ᶕ"
"ƞ"
"Ⓟ"
"ǹ"
"ｃ"
"Ḵ"
"Ḣ"
"ặ"
"ĩ"
"ｑ"
"ɜ"
"➈"
"❞"
"ay"
"⒵"
"ᵤ"
"Ň"
"Ǻ"
"4."
"ʂ"
"ȑ"
"ⓗ"
"6."
"ȇ"
"Ⓕ"
"ŉ"
"Ǩ"
"Ṹ"
"⑼"
"Ɱ"
"ᶇ"
"ố"
"❬"
"ệ"
"(2)"
"ⱼ"
"ᶉ"
"Ż"
"Ꜻ"
"“"
"⒉"
"‒"
"Ḑ"
"ạ"
"Ᵹ"
"Ɏ"
"➆"
"(x)"
"ı"
"ƴ"
"Ṫ"
"⁺"
"Ḏ"
"ẓ"
"ꝣ"
"Ṥ"
"Ɗ"
"ẝ"
"⒟"
"ŭ"
"₄"
"❰"
"‸"
"Ｆ"
"8."
"ᴐ"
"₊"
"Ĝ"
"Ṗ"
"ɱ"
"Å"
"Ꞅ"
"ꝓ"
"ⓨ"
"ᵱ"
"Ǖ"
"Ȳ"
"⓺"
"ẏ"
"18."
"16."
"＃"
"Ỳ"
"14."
"Ƞ"
"ʑ"
"⒤"
"ᵳ"
"Ċ"
"Ṁ"
"ó"
"10."
"ɭ"
"⑭"
"á"
"ᶔ"
"ⱻ"
"Ỡ"
"Ɵ"
"＝"
"Ⓞ"
"ḳ"
"ĸ"
"Ꝙ"
"ⓖ"
"Ꝏ"
"ｖ"
"ɟ"
"ḡ"
"Ķ"
"➉"
"ᵥ"
"ņ"
"❝"
"Ȗ"
"⒪"
"Ⓒ"
"ň"
"Ȅ"
"ṹ"
"⁻"
"⑻"
"ï"
"Ｏ"
"ᶆ"
"Ỗ"
"Ƒ"
"Ạ"
"Ｙ"
"ᶈ"
"Ễ"
"ƃ"
"⒎"
"ꜻ"
"ź"
"Ĥ"
"ꝼ"
"»"
"ɉ"
"ḟ"
"➇"
"Ǧ"
"(8)"
"(11)"
"ĳ"
"ᴂ"
"Ȋ"
"ṫ"
"ḍ"
"Ẓ"
"Ꝣ"
"⁹"
"ṥ"
"Ö"
"ǚ"
"ẜ"
"⁷"
"Ị"
"Ŭ"
"₅"
"❷"
"⒜"
"ᴗ"
"₋"
"⓫"
"ɰ"
"ᴅ"
"ĝ"
"ṗ"
"ts"
"ǔ"
"Ặ"
"ʠ"
"ȳ"
"(18)"
"⓵"
"ꞅ"
"Ä"
"Ẏ"
"＂"
"ỳ"
"Ƶ"
"ừ"
"⒥"
"ᵴ"
"ŗ"
"Ǌ"
"ċ"
"ṁ"
"ɢ"
"ò"
"ɬ"
"ᵹ"
"Ĩ"
"⑬"
"＜"
"ᶗ"
"ȡ"
"Ⓡ"
"Ⱳ"
"Ḳ"
"ꝟ"
"ａ"
"ⓑ"
"ｗ"
"Nj"
"Ḡ"
"ằ"
"ķ"
"ɞ"
"ỡ"
"❜"
"ȗ"
"⒫"
"ř"
"Ǹ"
"ʌ"
"ŋ"
"–"
"13."
"ȅ"
"Ⓓ"
"Ṻ"
"⑺"
"Ꝗ"
"î"
"ƒ"
"Ｎ"
"ᶙ"
"Ｂ"
"S"
"ⱺ"
"ễ"
"ᵣ"
"⒏"
"Ž"
"Ꜽ"
"ₔ"
"Ɉ"
"Ḟ"
"ả"
"ĥ"
"➄"
"!!"
"Ḍ"
"ậ"
"➒"
"ⱨ"
"ȋ"
"Ṵ"
"ꝡ"
"⁸"
"Ṧ"
"Õ"
"⁶"
"Ǒ"
"ị"
"❶"
"(z)"
"⒝"
"ů"
"ᴖ"
"₈"
"⓪"
"ᴄ"
"Ṑ"
"ɳ"
"(u)"
testAllFoldings
"(b)"
"Ｄ"
"Ȱ"
"Ǜ"
"⓴"
"Ã"
"Ꞃ"
"ẉ"
"Ǉ"
"Ự"
"Ő"
"ǉ"
"Ŗ"
"Ĉ"
"(d)"
"Ṃ"
"ɽ"
"ñ"
"ɯ"
"ḱ"
"Ć"
"⑫"
"ÿ"
"？"
"ᶖ"
"Ｍ"
"Ȧ"
"Ⓠ"
"Ꝟ"
"ｆ"
"ⓐ"
"Ằ"
"Ꝍ"
"ｔ"
"ə"
"ḯ"
"Ủ"
"(20)"
"⒨"
"ǿ"
"Ř"
"Ȕ"
"ʍ"
"P"
"Ⓘ"
"ǭ"
"Ŋ"
"Ț"
"ṻ"
"í"
"⑹"
"Ｉ"
"Ổ"
"Ɠ"
"⑷"
"［"
"ᶊ"
"Ớ"
"ꜽ"
"ḝ"
"Ģ"
"⁉"
"ɋ"
"➅"
"Ÿ"
"ḋ"
"Ậ"
"Ꝡ"
"⁇"
"➓"
"tc"
"Ȉ"
"⒌"
"ǣ"
"ṵ"
"ᴕ"
"ṧ"
"⁵"
"(m)"
"Ô"
"ẞ"
"Ỉ"
"Ħ"
"Ｕ"
"Ᵽ"
"Ů"
"❵"
"⒒"
"₉"
"ⓥ"
"ě"
"ṑ"
"ɲ"
"ᴋ"
"ls"
"ĵ"
"(g)"
"ȱ"
"⓷"
"ꞃ"
"Â"
"Ẉ"
"，"
"(o)"
"ự"
"ĉ"
"＋"
"ṃ"
"ɼ"
"ð"
"Ḱ"
"ć"
"⑪"
"þ"
"ȧ"
"ǈ"
"ʜ"
"Ⓛ"
"ｇ"
"ⓓ"
"ｕ"
"ɘ"
"Ḯ"
"ẳ"
"ꝃ"
"ủ"
"＞"
"ś"
"Ǿ"
"ʎ"
"ȕ"
"⒩"
"Ⓙ"
"ō"
"Ǭ"
"”"
"ț"
"⑸"
"ì"
"Ｈ"
"ổ"
"⑶"
"Ｚ"
"ᶍ"
"Ɔ"
"ⱴ"
"ộ"
"ⓝ"
"ģ"
"⁈"
"Ɋ"
"Ḝ"
"ｈ"
"➂"
"Ḋ"
"ắ"
"ꝧ"
"⁆"
"AO"
"➐"
"⒍"
"ſ"
"Ǣ"
"ȉ"
"Ṷ"
"OO"
"ᴔ"
"Ṡ"
"Ó"
"ẙ"
"⁴"
"ⱪ"
"Ｔ"
ĳ"
"ỉ"
"‹"
"❴"
"⒓"
"š"
"ż"
"₎"
"ⓤ"
"ᴊ"
"AY"
"(l)"
"Ǚ"
"ȶ"
"ʣ"
"ᵲ"
"Ṓ"
"Á"
"Ꞁ"
"ẋ"
"ⱸ"
"ᵶ"
"ᵻ"
"Ï"
"／"
"Ỷ"
"(19)"
"⓶"
"ﬂ"
"ɿ"
"Ė"
"ớ"
"(5)"
"Ṍ"
"Ą"
"ｄ"
"ḿ"
"ý"
"ꟽ"
"ꜰ"
"‼"
"⑩"
"ᵷ"
"Ẻ"
"Ȥ"
"ʝ"
"Ⓚ"
"Ȫ"
"ʏ"
"ⓒ"
"Ꜷ"
"ḭ"
"Ĳ"
"Ꝃ"
expectedOutputTokens
"ｚ"
"９"
"Ụ"
"⒮"
"ǽ"
"Ś"
"Ừ"
"Ō"
"av"
"Ș"
"Ⓖ"
"ǳ"
"ɛ"
"Ƃ"
"Ŵ"
"⑧"
"ƕ"
"Ｋ"
"⑵"
"ⱡ"
"ᶌ"
"ẽ"
"Ộ"
"Ƈ"
"Ⱨ"
"L"
"ⓜ"
"ḛ"
"Ġ"
"➃"
"Ʌ"
"Ꝧ"
"⁅"
"ﬄ"
"ḉ"
"Ắ"
"➑"
"ǡ"
"ꜿ"
"ž"
"Ȏ"
"⒂"
"ṷ"
"ṡ"
"ᴛ"
"Ò"
"ẘ"
"Ｗ"
"Ĺ"
"Ỏ"
"⒐"
"Š"
"❻"
"ƫ"
"ⓧ"
"ᴉ"
"ę"
"ḇ"
"TZ"
"ȷ"
"ǘ"
"ｏ"
"Ꜿ"
"ṓ"
"ꞁ"
"À"
"ꝍ"
"(f)"
"Î"
"ỷ"
"．"
"⓱"
"‚"
"NJ"
"ė"
"ṍ"
"ɾ"
"ｅ"
"ɨ"
"Ḿ"
"ą"
"～"
"⑨"
"15."
"ü"
"ǎ"
"ʞ"
"ȥ"
"11."
"）"
"Ⓝ"
"(a)"
"ʈ"
"ȫ"
"(i)"
"ᴠ"
"Ⓧ"
"ꝁ"
"｛"
"８"
"1."
"ụ"
"⒯"
"ŝ"
"Ǽ"
"Ɩ"
"Ⓗ"
"ŏ"
"ǲ"
"ș"
"ɚ"
"Ḭ"
"ê"
"⑦"
"(14)"
"Ｊ"
"➀"
"⑴"
"ⱦ"
"ᶏ"
"ƈ"
"ⓟ"
"Ḛ"
"ế"
"ġ"
"Ʉ"
"Ḉ"
"ẩ"
"⁄"
"!?"
"ⱶ"
"ȏ"
"⒃"
"ű"
"Ǡ"
"Ṱ"
"⒑"
"ᴚ"
"Ṣ"
"Ñ"
"ỏ"
"))"
"Ｖ"
"ţ"
"❺"
"Ƭ"
"‵"
"₌"
"ⓦ"
"Ḇ"
"ẛ"
"ŕ"
"AU"
"ǟ"
"ȴ"
"ỗ"
"ᴈ"
"ß"
"ẅ"
"Í"
"ⱬ"
"Ĭ"
"Ỵ"
"⓰"
"⁓"
"č"
"Ĕ"
"Ṏ"
"??"
"Ｘ"
"SS"
"(n)"
"ḽ"
"Ă"
"Ꝓ"
"ｊ"
"ɫ"
"ṇ"
"û"
"Ę"
"Ǎ"
"Ⱥ"
"ʟ"
"Ⓜ"
"(t)"
"Ȩ"
"ʉ"
"⒬"
"ᵫ"
"⒘"
"Ⓦ"
"ｘ"
"′"
"；"
"Ỻ"
"Ŝ"
"Ứ"
"Ɨ"
"５"
"Ǳ"
"Ŏ"
"Ȟ"
"⒲"
"ḫ"
"İ"
"Ꝁ"
"ɕ"
"⑥"
"➁"
"⑳"
"Ｇ"
"ᶎ"
"Ở"
"Ɖ"
"ⓞ"
"ɇ"
"ḙ"
"Ế"
"Ẩ"
"ﬆ"
"ḗ"
"Ṝ"
"ợ"
"ǧ"
"ꜱ"
"Ű"
"❫"
"Ȍ"
"ṱ"
"⸨"
"⒖"
"Ţ"
"ᴙ"
"—"
"ṣ"
"Ð"
"Ｑ"
"Ọ"
"❹"
"ƭ"
"₍"
"ⓡ"
"ḅ"
"ẚ"
"ⁱ"
"Ļ"
"ƀ"
TestASCIIFoldingFilter
"Ǟ"
"ꟻ"
"ʮ"
"ȵ"
"ᴏ"
"ṝ"
"Þ"
"Ẅ"
"ṏ"
"Ì"
"（"
"ỵ"
"(w)"
"⓳"
"ĕ"
"ꝏ"
"⓽"
"ă"
"ｋ"
"ɪ"
"Ḽ"
"ú"
"ᵺ"
"ǌ"
"‘"
"Ȼ"
"(4)"
"⒭"
"ᵬ"
"ş"
"ȩ"
"Ⓩ"
"："
"ỻ"
"ⱱ"
"４"
"ứ"
"’"
"ȟ"
"⒳"
"Ł"
"ǰ"
"Ḫ"
"ᴡ"
"ꝇ"
"ｙ"
"ɔ"
"è"
"ffi"
"⑤"
"Ɇ"
"⑲"
"➎"
"ở"
"Ɽ"
"ᶁ"
"ʄ"
"ⓙ"
"ď"
"qp"
"Ḙ"
"ẹ"
"²"
"į"
"Ṳ"
"ﬁ"
"Ḗ"
"ẫ"
"Ꝼ"
"(12)"
"ų"
"Ꜳ"
"❪"
"Dz"
"ȍ"
"⒁"
"⸩"
"⒗"
"ť"
"❸"
"ᴘ"
"Ṭ"
"Ｐ"
"ọ"
"tz"
"Ʈ"
"ⓠ"
"ƶ"
"₂"
"(("
"⁰"
"ⱥ"
"Ḅ"
"ẕ"
"lz"
"ǝ"
"ᴎ"
"Ṟ"
"Ý"
"ẇ"
"Ṉ"
"Ë"
"dz"
"(7)"
"Ơ"
"ʯ"
"⓲"
"Ē"
"⓼"
"Ｌ"
"ḻ"
"Ā"
"Ꝑ"
"ɥ"
"ȸ"
"ʙ"
"(17)"
"Ǔ"
"ᵭ"
"Ş"
"Ɛ"
"Ȯ"
"ʋ"
"Ǐ"
"⒢"
"‑"
"3."
"Ⓨ"
"Ⱶ"
"ù"
"Ě"
"Ỹ"
"Į"
"％"
"７"
"Ữ"
"ƙ"
"⒰"
"Ƿ"
"ŀ"
"❛"
"Ꝇ"
"5."
"＾"
"ɗ"
"ḩ"
"ľ"
"7."
"④"
"ç"
"ḧ"
"➏"
"⑱"
"Ａ"
"ᶀ"
"ꝙ"
"Ȝ"
"ⓘ"
"(15)"
"Ẹ"
"³"
"ṳ"
"ﬀ"
"ḕ"
"Ẫ"
"ꝺ"
"nj"
"Ờ"
"Ƌ"
"❩"
"⒆"
"ǥ"
"ꜳ"
"Ų"
"Ȃ"
"Ť"
"❿"
"⒔"
"(y)"
"ṭ"
"ħ"
"Ɯ"
"Ｓ"
"⒀"
"Ể"
"Ư"
"₃"
"ⓣ"
"ḃ"
"Ẕ"
"Ꝩ"
"ⁿ"
"9."
"Ẇ"
"AV"
"ǜ"
"⒚"
"ᴍ"
"ṟ"
"Ü"
"ṉ"
"ᴃ"
"Ê"
"＊"
"ʨ"
"ơ"
"HV"
"⓭"
"ē"
"⓿"
"Ľ"
"Ḻ"
"ā"
"ｉ"
"‐"
"ᵼ"
"ǒ"
"ʚ"
"ȹ"
"ꟼ"
expectedIter
"ī"
"ȯ"
"⒣"
"ᵮ"
"ő"
"(q)"
"(k)"
"Lj"
"Ⓣ"
"ø"
"(16)"
"＄"
"„"
"ỹ"
"ƚ"
"６"
"ᶑ"
testLatin1Accents
"Ń"
"Ƕ"
"‛"
"Ḩ"
"ꝅ"
"ɖ"
"③"
"æ"
"ɀ"
"ⱳ"
"Ḧ"
"ẻ"
"ĭ"
"➌"
"⑰"
"Ɫ"
"ᶃ"
"ȝ"
"⒱"
"ⓛ"
"⁒"
"(s)"
"Ṽ"
"Ꝺ"
"ﬃ"
"ᴢ"
"Ḕ"
"ấ"
"ƌ"
"⒇"
"ŵ"
"Ǥ"
"❨"
"ȃ"
"❾"
"⒕"
"ŧ"
"ₒ"
"ᵵ"
"⓻"
"(9)"
"Ṯ"
"Ｒ"
"ể"
"ư"
"₀"
"ⓢ"
"‶"
"Ḃ"
"ẗ"
"⁾"
"Ⱪ"
"lj"
"ẁ"
"⒛"
"ũ"
"Ꜩ"
"ᴌ"
"Ṙ"
"Û"
"Đ"
"Ṋ"
"ɵ"
"É"
"⓬"
"Ｅ"
"⓾"
"(v)"
"ｎ"
"?!"
"ḹ"
"Ď"
"ᵽ"
"Ⱦ"
"ʛ"
"VY"
"⒠"
"ᵯ"
"ꟿ"
"(1)"
"Ȭ"
"Ⓢ"
"Ṅ"
"ↄ"
"(h)"
"＇"
"Ỿ"
"②"
"Ǝ"
"１"
"ꝗ"
"Ử"
"ļ"
"Ꝅ"
"ᴆ"
"ḷ"
"å"
"Ẽ"
"ffl"
"ḥ"
"Ī"
"Ꝋ"
"12."
"ｒ"
"Ƀ"
"(p)"
"➍"
"⒅"
"⑿"
"Ⓐ"
"ǵ"
"ł"
"Ȓ"
"ʇ"
"ⓚ"
"(3)"
"ṽ"
foldings
"ḓ"
"Ấ"
"⁏"
"Ｃ"
"ᶂ"
"Ồ"
"ꜵ"
"V"
"❯"
"Ȁ"
"⒄"
"ǫ"
"Ŧ"
"ₓ"
"❽"
"⒊"
"ɍ"
"Ề"
"ƿ"
"］"
"－"
"ṯ"
"⁽"
"ữ"
"ḁ"
"ẖ"
"Ẁ"
"Ĵ"
"ꜩ"
"Ũ"
"₁"
"❳"
"ṙ"
"Ú"
"ᴁ"
"đ"
"Ẳ"
"ṋ"
"ɴ"
"È"
"ʪ"
"LL"
"⓯"
"⓹"
"(6)"
"Ḹ"
"ꝕ"
"＼"
"ȿ"
"ᵾ"
"ǐ"
"œ"
"ǆ"
"ꟾ"
"Ŀ"
"DZ"
"ȭ"
"⒡"
"ᵰ"
"ṅ"
"ɦ"
"ö"
"ỿ"
"＆"
"①"
"ä"
"０"
"ᶓ"
"ử"
"Ⓥ"
"｝"
"ɐ"
"Ḷ"
"ꝛ"
"ꝉ"
"ｓ"
"Ḥ"
"ẵ"
"➊"
"⑾"
"Ⓑ"
"ᵢ"
"Ņ"
"Ǵ"
"ʀ"
"ꝋ"
"ȓ"
"ⓕ"
"ȁ"
"Ṿ"
"Ḓ"
"ầ"
"ꝿ"
"⁎"
"Ⱡ"
"ᶅ"
Ñ"
"ồ"
"❮"
"(10)"
"ŷ"
"Ǫ"
"ề"
"ₐ"
"❼"
"＠"
"⒋"
"Ź"
"Ꜹ"
"ᴜ"
"Ɍ"
"ꝑ"
"Ʋ"
"Ṩ"
"Ŕ"
"OU"
"›"
"Ḁ"
"ẑ"
"⁼"
checkTokens
testCaching
TestCachingTokenFilter
NOT_IN_MAP
keys"
"keySet()
"{test=1}"
"[test=1]"
"[1]"
map"
testCopyJDKSet
testRehash
NPE"
lowerArr
setIngoreCase
"xthisy"
copyCaseSens
"abc\uD800efg"
stopwordsUpper
NOT_IN_SET
newWords
"abc\ud801\udc44"
testEmptySet
"\uD800\ud801\udc44b"
"ABC\uD800"
testObjectContains
upperArr
"abc\uD800"
%s
testCopyCharArraySetBWCompat
"deprecated"
TestCharArraySet
"\ud801\udc44\ud801\udc44cde"
"remove()
"\ud801\udc1c\ud801\udc1cCDE"
testContainsWithNull
testSingleHighSurrogateBWComapt
raise
setCaseSensitive
testSupplementaryChars
version,
"\uD800\ud801\udc1cB"
"ABC\uD800EfG"
testCopyEmptySet
cas
testSupplementaryCharsBWCompat
testUnmodifiableSet
"A\ud801\udc1cB"
testNonZeroOffset
CharArraySetIterator"
testCopyCharArraySet
"\uD800efg"
falsePos
"\uD800EfG"
"_1"
testSingleHighSurrogate
unmodifiable"
findme
"a\ud801\udc44b"
TEST_STOP_WORDS
CharFilter1
TestCharFilter
testCharFilter1
"corrected
testCharFilter2
testCharFilter11
CharFilter2
testCharFilter12
TestCharTokenizers
TestingCharTokenizerNormalizeIsTokenChar
testMaxWordLength
3.1
testLowerCaseTokenizerBWCompat
testWhitespaceTokenizerBWCompat
"Tokenizer
permitted
testExtendCharBuffer
TestingCharTokenizerNormalize
"\ud801\udc44test"
testNormalizeCharInSubclass
TestingCharTokenizer
1023
testIsTokenCharCharInSubclass
"\ud801\udc1cabc"
testNormalizeAndIsTokenCharCharInSubclass
testMaxWordLengthWithSupplementary
testReadSupplementaryChars
testU
TestISOLatin1AccentFilter
Ñ
ĳ
"partnum:Q36
+space"
"Q36
TestKeywordAnalyzer
"Q36"
"+partnum:Q36
"Illidium
testPerFieldAnalyzer
"partnum"
Modulator"
"Q37"
SPACE"
kept
testMutipleDocument
as-is"
LowerCaseFilterMock
browN
"lucenefox"
testIncrementToken
jdkSet
Jumps"
TestKeywordMarkerTokenFilter
"jumps"
"LuceneFox"
quIck
LuceneFox
"short
TestLengthFilter
evenmuchlongertext
testReaderReset
"llllllll"
test5to0
test3to1
"llll"
testChained
"kkk"
aa"
cccc
"aaaa
test4to2
test2to4
test2to1
TestMappingCharFilter
test1to2
test1to3
test1to1
h"
succeed."
testIntStream
"incrementToken()
testNotInitialized
testLongStream
TestNumericTokenStream
"Type
4573245871874382L
lvalue
encoded"
"reset()
"qwerty"
"WhitespaceAnalyzer
testPerField
"SimpleAnalyzer
"special"
lowercase"
"Qwerty"
TestPerFieldAnalzyerWrapper
lowercases"
"output.txt"
TestPorterStemFilter
"voc.txt"
yours"
"yourselves
testPorterStemFilter
"porterTestData.zip"
"toolong"
testComplianceFileName
testTSADash
testComplianceNumericLong
testNumeric
"first_lastname@example.com"
testEMailAddresses
"mosque.jpg"
testLucene1140
"Excite@Home"
"David
"/money.cnn.com/magazines/fortune/fortune_archive/2007/03/19/8402357/index.htm
testComplianceNumericFile
did"
bones"
"www.nutch.org"
"archive/2007/03/19/8402357"
"21.35"
"word_with_underscore_and_stopwords"
"don't"
"a1-b-c3"
testComplianceManyTokens
"R2D2
"having"
"dashed"
"embedded"
"AT&T"
"magazines"
"cats"
"index.htm"
testVariousText
testApostrophes
"1-2-3"
"some-dashed-phrase"
"s-class"
"b2b"
"2006-03-11t082958z_01_ban130523_rtridst_0_ozabs"
"t-com"
"wanted"
"zayed"
"bones"
testCompanyNames
"at&t"
"she's"
testComplianceNumericWithDash
"fortune"
"mid-20th"
"excite@home"
"david"
"jim"
"C#"
"safari-0-sheikh-zayed-grand-mosque.jpg"
testUnderscores
"o'reilly"
"a-class"
한글입니다"
"한글입니다"
"O'Reilly"
"r2d2"
"you're"
"78academyawards/rules/rule02.html"
"grand"
"안녕하세요"
"안녕하세요
"ac/dc"
testComplianceNumericWithUnderscores
"216.239.63.104"
"underscore"
testComplianceNumericIncorrect
testAlphanumeric
"62.46"
C3PO"
testTextWithNumbers
"safari-0-sheikh"
"word_having_underscore"
"developers"
"2004.jpg"
"usa"
testDomainNames
"first.lastname@example.com"
"O'Reilly's"
developers
"chase"
"test@example.com"
testCPlusPlusHash
"money.cnn.com"
"a1-b2-c3"
"978-0-94045043-1"
"5000"
"Jim's"
testAcronyms
TestStandardAnalyzer
testMaxTermLength2
testMaxTermLength3
"dogs,chase,cats"
"www.nutch.org."
testMaxTermLength
wanted"
"wwwnutchorg"
"2b"
testKorean
TestStopAnalyzer
newStop
"good"
inValidTokens
testStopListPositions
expectedIncr
testStopList
english
analyzer"
TestStopFilter
"Stop0:
"--->
stpf
stopSet1
"Time"
"Stop1:
testExactCase
enable-increments-"
stpf0
"Now"
stpf01
"Stop:
doTestStopPositons
testIgnoreCase
enableIcrements
testStopPositons
testStopFilt
stopSet0
stopWords1
stopWords0
tfTok
theDetector
tokCount
modCount
TestTeeSinkTokenFilter
sink2
"-----Tokens:
"stay"
lowerCasing
sinkTok
"porch"
buffer2
sinkPos
teeStream
"Red"
Two
End
token:
tee2
tee1
buffer1
sink1
"Burgundy"
"ModCount:
theFilter
mc
Tee
ModuloTokenFilter
dogFilter
testMultipleSources
dogDetector
tfPos
Tokens:
tokens1
performance
"-----"
"-
ModuloSinkFilter
modCounts
lowerCaseTokens
SenselessAttributeImpl
"(hi
testCtor
"(hello,6,22,type=junk)"
testTermBufferEquals
"(hello,6,22)"
SenselessAttribute
testTokenAttributeFactory
"(aloha,0,5)"
there,0,5)"
SenselessAttributeImpl"
testKeywordAttribute
attrImpl
testOffsetAttribute
testTypeAttribute
"Copied
instance's
testPayloadAttribute
"flags=1234"
testPositionIncrementAttribute
att2
"Clone's
"positionIncrement=1234"
hashcode
testFlagsAttribute
"type=hallo"
"term=aloha"
"term=hi
TestTermAttributeImpl
firstEnd
secondBeg
firstBeg
sortData
keyBits
secondEnd
encodedBegArray
usResult
TestCollationKeyAnalyzer
TestCollationKeyFilter
binaryValCompressed
"binaryStored"
TestBinaryDocument
docFromReader
"stringStored"
testBinaryFieldInIndex
binaryFldStored
binaryValStored
binaryFldCompressedTest
binaryFldStoredTest
binaryFldCompressed
stringFldStoredTest
"stringCompressed"
testCompressionTools
"binaryCompressed"
stringFldCompressed
stringFldStored
"fail"
TestDateTools
testStringtoTime
dateDay
isoFormat
"200401011235009999"
testDateToolsUTC
"Europe/London"
22:00:00:000"
"20040203220856"
"200407050910"
22:08:56:333"
"1961-03-05
"20040705"
"200402032208"
09:10:00:000"
"197001010000"
testDateAndTimetoString
dateMillisecond
"1961030523"
"20040203"
444
"2004-07-05
dateYear
09:10:55:990"
dateYearLong
"2004"
"2004020322"
"2004-01-01
23:09:51:444"
22:08:56:000"
"midnight"
23:00:00:000"
22:08:00:000"
dateHour
"200402"
"2004-02-01
"2004-02-03
HH:mm:ss:SSS"
"19700101010203000"
"198002021105"
dateSecond
1130630400
"97"
"19610305230951444"
"20040705091055990"
1961
"later"
dateMonth
dateMinute
"19700101000000000"
testStringToDate
"20040203220856333"
00:00:00:000"
sdf
1970
dateMillisecondLong
binaryTest
doAssert
testConstructorExceptions
binaryTest2
testRemoveForNewDocument
makeDocumentWithFields
textFieldValues
keywordFieldValues
stringFld
TestDocument
binaryTests
unindexedFieldValues
"doesnotexists"
binaryFld2
testGetValuesForNewDocument
testFieldSetValueChangeBinary
IDs"
testFieldSetValue
binaryVal2
binaryFld
binaryVal
stringTest
testBinaryField
testGetValuesForIndexedDocument
unstoredFieldValues
TestNumberTools
subtestTwoLongs
testMin
testMax
testNearZero
NO_TF_TEXT
textField3
textField2
textField1
noTFField
"textField1Utf8"
UNSTORED_FIELD_1_KEY
\u4e00two
laughing
bbbNoNorms"
"unStoredField1"
aaaNoNorms
"UTF8"
"analyzed
KEYWORD_FIELD_KEY
lazyField
"Keyword"
"unIndField"
UNINDEXED_FIELD_KEY
"omitNormsText"
noNormsField
UNINDEXED_FIELD_TEXT
\u4e00text"
lieu
UNSTORED_FIELD_2_KEY
LARGE_LAZY_FIELD_TEXT
largeLazyField
"unstored
"unindexed
lazyFieldBinary
"Lazily
"textField2Utf8"
unIndField
UNSTORED_2_FIELD_TEXT
"lazyFieldBinary"
unStoredField2
unStoredField1
UNSTORED_1_FIELD_TEXT
textUtfField1
textUtfField2
FIELD_UTF2_FREQS
"aaaNoNorms
"largeLazyField"
"lazyField"
"unStoredField2"
destOffset
bytesInBuffer
testNoMergeAfterCopy
setUpDirs
testWithPendingDeletes2
testWithPendingDeletes3
1060
testHangOnClose
testWithPendingDeletes
testMoreMerges
TestAddIndexesNoOptimize
aux2
aux3
testNoCopySegments
aux4
1039
1032
1030
addDocs2
1025
1020
verifyNumDocs
verifyTermDocs
testMergeAfterCopy
testAddSelf
newWriter
testTargetCFS
1040
testNoTailSegments
indexer"
testAtomicUpdates
"lucene.test.atomic"
TestAtomicUpdate
search1"
search2"
indexer2"
unzip
"fie\u2C77ld"
"20.nocfs"
actual:\n
"testindex.nocfs"
shouldStillBeCompressed
"size
testIndexOldIndex
"22.cfs"
trunk
"29.cfs"
"src/test/org/apache/lucene/index/index."
BINARY_TO_COMPRESS
"compressed"
TestBackwardsCompatibility
hasTested29
testCreateNoCFS
"_0_1.s"
\u2620
"testindex.cfs"
"autf8"
TEXT_TO_COMPRESS
ab\ud917\udc17cd"
addNoProxDoc
uncompressed
changeIndexNoAdds
FieldsReader.canReadRawDocs()
"compressedSize"
"Lu\uD834\uDD1Ece\uD834\uDD60ne
"utf8"
"23.nocfs"
"22.nocfs"
"20.cfs"
zipName
"correct
testSearchOldIndex
doCFS
"29.nocfs"
filenames
shouldSize
fullDir
origDirName
"24.nocfs"
binary"
changeIndexWithAdds
"19.nocfs"
"incorrectly
ZipEntry
testOptimizeOldIndex
oldNames
"21."
"29."
2.9
BINARY_PLAIN_LENGTH
\u0000
"24.cfs"
"19.cfs"
actualSize
decompressed
destDirName
assertCompressedFields29
"lucene.backwardscompat0.index"
testExactFileNames
testCreateCFS
v0
"23.cfs"
testHits
".zip"
TEXT_PLAIN_LENGTH
"21.nocfs"
compressedSize
testIndexOldIndexNoAdds
oldName
recorded
"uncompressed
"21.cfs"
non-ascii
expected:\n
"ab\ud917\udc17cd"
fullPath
"22."
"incorrect
counters
stream="
NUM_STREAM
TestByteSlices
numValue
testDeletedDocs
TestCheckIndex
testLuceneConstantVersion
".big6"
TestCompoundFile
1911
assertSameSeekBehavior
"f.comp"
ba1
position"
".big7"
1099
testFileNotFound
testReadAfterClose
seek(end)"
ba2
testSingleFile
seekTo
testRandomAccessClones
isCSIndexInputOpen
two/2"
assertEqualArrays
".big4"
testRandomAccess
"testBufferStart.txt"
seek(end-1)"
".notIn2"
".hundred"
".ten"
assertSameStreams
be1
".big5"
demo_FSIndexInputBug
fsdir
testRandomFiles
testBuffer
".big1"
"test.cfs"
isCSIndexInput
largeBuf
".big2"
expectedBuffer
seek(0)"
seek(end+1)"
testLargeWrites
cis
readByte()
".big3"
1910
be2
testReadPastEOF
1027
"d.csf"
1028
testClonedStreamsClosing
currentPos
readLen
"f11"
".notIn"
seek(end-2)"
createRandomFile
"Single
createSequenceFile
csw
csr
testTwoFiles
"onetwothree"
seek(mid)"
setUp_2
".one"
".zero"
testNoExtraFiles
hitExc
TestConcurrentMergeScheduler
"testNoExtraFiles"
450
testFlushExceptions
extraCount
testDeleteMerging
testCrashAfterReopen
testCrashAfterCloseNoWait
testCrashReaderDeletesAfterClose
314
TestCrash
testCrashReaderDeletes
testCrashAfterClose
testCrashWhileIndexing
testWriterAfterCrash
"commit
nowVersion
KeepLastNDeletionPolicy
numOnCommit
"SegmentInfos
lastVersion
testKeepAllDeletionPolicy
"onCommit
TestDeletionPolicy
lastDeleteTime
numToKeep
testKeepLastNDeletionPolicy
seconds
testKeepLastNDeletionPolicyWithReader
msec)
timestamps
testExpirationTimeDeletionPolicy
lastTimestamp
modTime
now="
testKeepNoneOnInitDeletionPolicy
testOpenPriorSnapshot
testKeepLastNDeletionPolicyWithCreates
ExpirationTimeDeletionPolicy
numDelete
preCount
out-of-order"
IndexReader.isOptimized="
verifyCommitOrder
out-of-order:
2.0F
expirationTimeSeconds
KeepNoneOnInitDeletionPolicy
postCount
firstCommit
numOnInit
point:
expireTime
"lastCommit.isOptimized()="
doDeletes
nowTimestamp
wow"
readers1
readers2
te3
testMultiTermDocs
"even
ramDir3
doTestDocument
doTestUndeleteAll
td2
mr2
mr3
DF="
siMerge
"test2.txt"
siMerge2
siMerge3
"TestDoc"
printSegment
fileToDelete
TF="
testIndexAndMerge
"merge2"
TestDoc
"merge3"
fw
si2
si1
filesToDelete
tfv2
testTokenReuse
f2"
testLUCENE_1590
testMixedTermVectorSettingsSameField
norms"
'with_tv'
testAddDocument
"repeated
TestDocumentWriter
"f2
testPositionIncrementGap
rule!"
testPreAnalyzedField
f1"
tfv1
"f1
"testFile"
TestFieldInfos
readIn
"byte["
Non-lazy
lazyFieldNames
Lazy
"sv
testLoadFirst
FaultyIndexInput
lazyTime
testLazyPerformance
testLoadSize
outage"
reads"
simOutage
assertSizeEquals
FaultyFSDirectory
mismatched"
"user.name"
TEST_SEGMENT_NAME
zero):
"stringValue
"Simulated
"testfieldswriterexceptions"
TestFieldsReader
regularTime
testLazyFieldsAfterClose
testLazyFields
loadFieldNames
userName
"lazyDir"
TestTermEnum
TestTermPositions
TestReader
TestFilterIndexReader
testFilterIndexReader
"_1_1."
"_2.fnm"
"_2_1."
"_3.cfs"
"_188_1.del"
"_1_1.del"
testDeleteLeftoverFiles
"_2_2.f"
"_0_2.del"
files2
normSuffix
files1
"_188.cfs"
"segments_2"
"\ndif:
"IndexFileDeleter
difFiles
TestIndexFileDeleter
filesPre
"_1_1.f"
"_2_2."
0xE2
"\uD834\uDD1E\uD834\uDD60"
"\uD834\uDD1E"
testSkipChars
16383
16385
"Lu\u2620ce\u2620ne"
"\u2620"
utf8Bytes
"Lu\u00BFce\u00BFne"
testRead
0xA0
theBytes
utf8Str
tmpStr
TestIndexInput
0x9E
0x9D
0xC2
"\u00BF"
charsToRead
0x0C
"Lu\u0000ce\u0000ne"
"\u0634\u1ea1"
testBinaryFields
testDeleteReaderWriterConflictOptimized
testIndexReaderUnDeleteAll
testFieldCacheReuseAfterReopen
"deletetest"
"keyword2"
"Entry:
addDocumentWithDifferentFields
"deleteDocument
"lucenetestnormwriter"
norms2
names."
fb1
testDeleteReaderWriterConflictUnoptimized
"old
ise
testUndeleteAllAfterClose
fieldable2
"reopened
it2
it1
testBasicDelete
testLock
frequence
testNoTermsIndex
dictionary."
deleteReaderWriterConflict
addDocumentWithTermVectorFields
"reader.close()
"0
testReopenChangeReadonly
searchTerm1
searchTerm3
searchTerm2
testFieldCacheReuseAfterClone
"tvpositionoffset"
testPrepareCommitIsCurrent
testLockObtainFailed
"fighters"
"undeleteAll
addDocumentWithFields
tp2
tp1
data1
testGetFieldNames
assertTermDocsCount
"IndexReaders
curField2
curField1
"unindexed2"
generation"
testReadOnly
bytes):
testDocsOutOfOrderJIRA140
"bin1"
itField2
itField1
sel
TermDocs"
lazyFields
testNoDir
testDeleteReaderReaderConflictUnoptimized
successStr
testUndeleteAllAfterCloseThenReopen
mismatch"
testUndeleteAll
testDiskFull
fieldable1
testOpenReaderAfterDelete
LockObtainFailedException"
testWritingNormsNoReader
ints2
testUniqueTermCount
"text2"
"entry
testDeleteReaderReaderConflictOptimized
testExceptionReleaseWriteLockJIRA768
numDocs."
enum1
enum2
bin
FileNotFoundException"
deleteReaderReaderConflict
testNoDupCommitFileNames
"unstored2"
"termvector"
opened"
testIndexReader
"locked"
postinglist
"junk
reader"
"tvoffset"
termDocs"
version2
fields2
fields1
sub0
testWritingNorms
"setNorm
duplicated"
"reopened"
"_0_1.s0"
deletions."
testVersion
"Delete
testGetIndexCommit
"tvnot"
testFilesOpenClose
"Norm
"IOException"
out-of-bounds
maxDoc."
"tvposition"
assertDelDocsRefCountEquals
"Tried
TestIndexReaderClone
17.0f
clonedReader
assertDocDeleted
testCloneWriteToOrig
"yes
testCloneNoChangesStillReadOnly
origSegmentReader
testCloneWithDeletes
clones
testReopenSegmentReaderToMultiReader
testCloneReadOnlySegmentReader
testCloneReadOnlyToWriteable
testLucene1516Bug
deleteWorked
testCloneWithSetNorm
testReopenWriteableToReadOnly
testSegmentReaderUndeleteall
pr1Clone
isReadOnly
worked"
origReader
testCloseStoredFields
multiReader
reopenedReader
testReadOnlyCloneAfterOptimize
testCloneWriteToClone
testCloneWriteableToReadOnly
testSegmentReaderCloseReferencing
testParallelReader
cloneSegmentReader2
lbfe
testCloneReadOnlyDirectoryReader
testCloneSubreaders
pr1
cloned
"cloned
readOnlyReader
"readOnlyReader
testSegmentReaderDelDocsReferenceCounting
cloneReader2
clonedSegmentReader
testNormsClose
segmentReader3C
irc3
0.99f
reader3CCNorm
"lucenetestindex1"
reader5C
"lucenetestindex2"
irc
reader4C
reader5CCNorm
0.33f
r1norm
reader2CNorm
"lucenetestindex3"
"reader2CNorm.bytesRef()="
segmentReader4C
reader2C
reader3C
segmentReader2C
r1BytesRef
indexDir3
segmentReader5C
TestIndexReaderCloneNorms
reader4CCNorm
indexDir2
indexDir1
checkSubReaders
testReferenceCountingParallelReader
testCloseOrig
testCommitReopenFS
readersToClose
testMultiReaderReopen
refreshReader
refresh."
testCommitRecreateRAM
checkNormsClosed
testCommitReopenRAM
testDeletes2
index2_refreshed
multiReader2
multiReader1
dir5
dir4
rs2
ReaderThreadTask
testReopenOnCommit
readerToClose
prevItereationDoc
createReaderMutex
ReaderThread
KeepAllCommits
firstReader
though
testCommitRecreateFS
"fielda"
initReader2
doTestReopenWithCommit
refreshed
withReopen
delDocs
assertReaderClosed
"b30"
refreshedReader
"fieldb"
parallelReader2
parallelReader1
"IndexReaderReopen"
testReferenceCountingMultiReader
couple
ReaderCouple
testDeletes
reader4
reader5
testParallelReaderReopen
reader0
assertRefCountEquals
testReferenceCounting
subReaders1
subReaders0
subReaders3
subReaders2
segmentReader1
segmentReader3
performTestsWithExceptionInReopen
TestReopen
numDel
testNegativePositions
rollback()"
gg
"intentionally
"\ud917\ud917"
iii
unoptimized"
testSimulatedCorruptIndex1
cause"
"prepareCommit"
inputDiskUsage
finalWriter
"closeDocStore"
testBadSegment
problems"
experiencing
optimized"
CommitAndAddIndexes2
"\udc17\ud917\udc17\ud917"
fillUnicode
"ab\udc17\ud917cd"
"ab\udc17\ud917\udc17\ud917cd"
RunAddIndexesThreads
myLockFactory
"\udc17abcd"
OOME
testIOExceptionDuringCloseDocStoreOnlyOnce
"sync"
testPrepareCommitNoChanges
testExpungeDeletes3
testExpungeDeletes2
0xfffd
sawFlush
testAddIndexesWithThreads
testDoubleOffsetCounting2
testEnablingNorms
RUN_SEC
FAILED:
"doc1field1"
"doc2field2"
testCommitOnCloseOptimize
"\ud917abcd"
maxDiskUsage
"ab\ud917cd"
wicked
testRandomUnicodeStrings
bytes;
"deleteFile"
documents:
testCommitOnClose
testCommitThreadSafety
testIndexStoreCombos
remain
Throwable"
"org.apache.lucene.index.SegmentInfos"
"addIndexes(Directory[])
testDocumentsWriterExceptionThreads
"IndexReader.open
noErrors
onlyOnce
testDoubleOffsetCounting
_testSingleThreadFailure
testOptimizeMaxNumSegments2
"addIndexesNoOptimize(Directory[])"
testSimulatedCorruptIndex2
"\ufffd\ufffd"
writer"
FailOnlyInCommit
testDeadlock
testImmediateDiskFullWithThreads
termVectorField
testIOExceptionDuringWriteSegment
1470
"abcd
start="
testThreadInterruptDeadlock
testEndOffsetPositionStandardEmptyField
testEndOffsetPositionCharAnalyzer
testChangingRAMBuffer
space:
testAddIndexesWithCloseNoWait
writer.abort"
"saw
"ab\ufffd\ud917\udc17\ufffdcd"
lastNumFile
testChangingRAMBuffer2
incorrectly
2X
hung"
"crash"
"\udc17\udc17abcd"
"\ud917\ud917abcd"
testUnlimitedMaxFieldLength
startDiskUsage
testIOExceptionDuringCloseDocStoreWithThreadsOnlyOnce
FailOnlyInCloseDocStore
bx
midDiskUsage
testEmptyFieldName
savedWriteLockTimeout
"\ufffdabcd"
at"
MyRAMDirectory
testWickedLongTerm
optSegCount
testStoredFieldsOrder
"\udc17\udc17"
testExceptionDocumentsWriterInit
testDocCount
addDocument"
"tvtest"
doAbort
jjj"
testExceptionJustBeforeFlush
MockIndexWriter3
testPrepareCommit
"ab\udc17cd"
"optimized
bytePrefix
1X
testOptimizeTempSpaceUsage
testForceCommit
NUM_DIR
"org.apache.lucene.store.MockRAMDirectory"
testPrepareCommitRollback
"addIndexes(IndexReader[])"
testVariableSchema
fullCount
testCloseWithThreads
testAddIndexesWithClose
"crunch
allowInterrupt
testOutOfMemoryErrorCausesCloseToFail
interrupted
hitError
testIOExceptionDuringAbortWithThreadsOnlyOnce
testEmbeddedFFFF
mid="
FailOnlyInSync
endDiskUsage="
testAddIndexesWithRollback
testExceptionOnMergeInit
fileNameOut
testImmediateDiskFull
CrashingFilter
fileNameIn
testRollbackExceptionHang
CommitAndAddIndexes3
testNoTermVectorAfterTermVectorMerge
testSetMaxMergeDocs
testEndOffsetPositionWithTeeSinkTokenFilter
testHighFreqTerm
IndexerThreadInterrupt
cycle:
"unreferenced
didFail
testTermVectorCorruption3
testTermVectorCorruption2
"doc2field1"
testDeleteUnusedFiles
"\ufffd"
testCommitOnCloseAbort
FailOnlyOnAbortOrFlush
testExceptionDuringSync
"ab\ufffdcd"
success!"
testIOExceptionDuringAbortWithThreads
IndexReader:
testManyFields
testSimulatedCrashedWriter
testSegmentsChecksumError
"\udc17\ud917\udc17\ud917abcd"
testOtherFiles
kk"
"\ufffd\ufffdabcd"
numCopy
NUM_COPY
jj
testDiverseDocs
"org.apache.lucene.index.FreqProxTermsWriter"
fail1
"segmentInfos
testExceptionFromTokenStream
failing"
corrupt
"doc3field2"
hasIllegal
"otherfiles"
sees
testIOExceptionDuringCloseDocStore
"\ufffd\ud917\udc17\ufffdabcd"
testNullLockFactory
"FAILED;
fail2
testEndOffsetPositionStopFilter
testSmallRAMBuffer
_testMultipleThreadsFailure
testNoTermVectorAfterTermVector
"do
iter="
numFile
"\udc17\ud917"
testIOExceptionDuringCloseDocStoreWithThreads
MockIndexWriter4
testIncrementalUnicodeStrings
readers"
testNoDocsIndex
testIOExceptionDuringWriteSegmentWithThreadsOnlyOnce
current"
lastFlushCount
100000000
"\udc17\ud917abcd"
utf16a
"ab\ufffd\ufffdcd"
testAllUnicodeChars
pass="
"aborted
"appendPostings"
sync"
"TEST:
crashDoc
testTermVectorCorruption
isDelete
testEndOffsetPositionStandard
seems
testUserSpecifiedMaxFieldLength
"\udc17"
"optimize
testIOExceptionDuringWriteSegmentOnlyOnce
endDiskUsage
MockIndexWriter2
testCreateWithReader
"ab\ud917\ud917cd"
lmp2
testDoBeforeAfterFlush
"label"
sawAppend
testAddIndexOnDiskFull
addDoc"
0x0010FFFF
"org.apache.lucene.index.DocFieldProcessor"
testCommitOnCloseDiskUsage
diskFree="
segCount
diskFull
testFlushWithNoMerging
"\ufffd\ud917\udc17\ufffd"
utf16
correctly"
testDocumentsWriterExceptions
testIOExceptionDuringWriteSegmentWithThreads
testAddDocumentOnDiskFull
xyz
utf8Data
"doc3field1"
ldmp
thread.interrupted()
docFreq('aaa')
testDocumentsWriterAbort
startDiskUsage="
optimize()"
testEndOffsetPositionStandardEmptyField2
testInvalidUTF16
usage)"
CommitAndAddIndexes
"codepoint
testBinaryFieldOffsetLength
addDocWithIndex
testEndOffsetPositionWithCachingTokenFilter
idUpto
"I'm
testIndexNoDocuments
bigTerm
testEmptyDocAfterFlushingRealDoc
testIOExceptionDuringAbortOnlyOnce
"flushDocument"
a\uffffb"
IOException:"
Throwable:"
"term=f:"
"myrandomfile"
FailOnlyInWriteSegment
afterWasCalled
"crunch"
end="
"lucenetestindexwriter"
beforeWasCalled
"ab\udc17\udc17cd"
"\ud917"
testOptimizeExceptions
testMaxThreadPriority
testExceptionsDuringCommit
testBackgroundOptimize
finalI
testIOExceptionDuringAbort
intentional
"segment
"doc1field2"
"getMaxThreadStates"
MySimilarity
"getOpenMode"
modifiers
TestIndexWriterConfig
isFinal
"getIndexingChain"
toString"
"getMergedSegmentWarmer"
"getAnalyzer"
testSettersChaining
"getMergeScheduler"
"getMaxFieldLength"
"indexingChain"
IndexWriterConfig"
getters
"getMaxBufferedDocs"
testIndexWriterSetters
"getIndexDeletionPolicy"
"getSimilarity"
succeeded
MyIndexingChain
testInvalidValues
defaults"
"getTermIndexInterval"
testConstants
"getWriteLockTimeout"
"getDefaultWriteLockTimeout"
"getMaxBufferedDeleteTerms"
"getIndexCommit"
"getRAMBufferSizeMB"
"Amsterdam"
testNonRAMDelete
testErrorInDocsWriterAdd
TestIndexWriterDelete
testBothDeletes
testOperationsOnDiskFull
testDeleteAllNRT
updateDoc
"applyDeletes"
"fail
"docswriter
writer.close"
testDeletesOnDiskFull
lots
getHitCount
applyDeletes"
testDeleteAllRollback
testDeleteAll
testErrorAfterApplyDeletes
testMaxBufferedDeletes
"Venice"
testBatchDeletes
testRAMDeletes
sawMaybe
"Amsterdam
"Venice
abort()
canals"
testUpdatesOnDiskFull
bridges"
"city"
"content7"
"Indexer
exception3"
"content6"
close:"
exception2"
"EXC:
FAIL:
TestIndexWriterExceptions
testRandomExceptionsThreads
0.2
exception1"
testRandomExceptions
intentionally
failure"
testIndexWriterLockRelease
hand)"
"testIndexWriter"
(please
__test_dir
TestIndexWriterLockRelease
segmentCount
ramSegmentCount
segs="
"maxMergeDocs="
segmentCfsCount
testNormalCase
TestIndexWriterMergePolicy
upperBound="
checkInvariants
noOverMerge
testForceFlush
testNoOverMerge
testMergeDocCount0
testMaxBufferedDocsChange
testMergeFactorChange
testLucene
indexB
indexA
TestIndexWriterMerging
fillIndex
startAt
numAddIndexesNoOptimize
addDir
excs
id75
testDuringAddDelete
testAddIndexesAndDoDeletesThreads
testMergeWarmer
mainWriter
testIndexWriterReopenSegmentOptimize
numDirs
AddDirectoriesThreads
id50
"indexname"
"addindex"
r0
doTestIndexWriterReopenSegment
id10
iwr2
iwr1
warmCount
testDeleteFromIndexWriter
testAfterClose
testAddIndexes2
lastCount
testDuringAddIndexes
index2df
w2r1
addDirThreads
1000.
testDeletesNumDocs
testAddIndexes
doc150
8000
testIndexWriterReopenSegment
testUpdateDocument
testAfterCommit
SELECTOR
",VAL:"
country!"
TestLazyBug
lamb!"
399
aid"
\u1111
testLazyBroken
dataset
\u3333}"
testLazyAlsoWorks
testLazyWorks
"FIELD:"
mary
chars:{\u0111
\u0222
string,
\u2222
FIELD:
lamb,
WTF:
MAGIC_FIELD
\u0333
TestLazyProxSkipping
testLazySkipping
seeksCounter
testSeek
SeeksCountingStream
SeekCountingDirectory
testSimpleSkip
4800
CountingStream
maxCounter
TestMultiLevelSkipList
TestMultiReader
TestNorms
thrown:
RunThread
TestNRTReaderWithThreads
testIndexing
indexThreads
"OmitTermFreqAndPositions
"noTf"
notf"
testOmitTermFreqAndPositions
testMixedRAM
assertNoPrx
testMixedMerge
freqs"
Tf
TestOmitTf
"notf"
testNoPrxFile
singleHits
TestParallelReader
getDir1
getDir2
exptected
w1
testIsOptimized
queryTest
doc24
parallelHits
testQueries
testFieldNames
doc223
doc11
docParallel
testIncompatibleIndexes
iwOut
rdOut
testEmptyIndexWithVectors
TestParallelReaderEmptyIndex
jumps"
"field2:fox"
"field2:brown"
"field0"
"field1:quick"
"field3:fox"
"field1:brown"
"field2:jumps"
"field2:the"
TestParallelTermEnum
"field3:the"
"field3:dog"
"field3:lazy"
"field1:jumps"
"field2:quick"
"field3:jumps"
"field1:fox"
"field1:the"
"field3:over"
testPayload
bytesToString
setPayloadData
numFieldInstancesToSkip
TestPayloads
ingesters
generateTerms
payloads"
generateRandomData
portion
verifyPayloadData
testPayloadFieldBit
length."
uee
PayloadData
testPayloadsEncoding
"somedata"
assertByteArrayEquals
"Payload
maxDigits
test!"
PoolingPayloadTokenStream
arrays
ByteArrayPool
payloadData
fieldToData
payloadDataLength
"test_payloads"
thePositions
"extra"
"Bits
TestPositionBasedTermVectorMapper
"info.getTerms()
"thePositions
"info.getOffsets()
TestSegmentMerger
mergedDir
merge1Dir
docsMerged
mergedSegment
merge2Dir
testMerge
deleteReader
have:
testGetFieldNameVariations
vectors,
docToDelete
TestSegmentTermDocs
testIndexDivisor
"ccc
segTermDocs
testBadSeek
testPrevTermAtEnd
TestSegmentTermEnum
verifyDocFreq
runStressTest
TestStressIndexing
"lucene.test.stress"
testStressIndexAndSearching
"FAILED
offsets1
offsets2
terms1
terms2
indexRandomIWReader
r2r1
tvVal
TestStressIndexing2
sameFieldOrder
indexRandom
indexSerial
ff2
ff1
verifyEquals
"v1="
IndexingThread
id="
DocsAndWriter
tpv2
tpv1
testMultiConfig
getIdString
v2="
addUTF8Token
testRandomIWReader
id1="
id2="
1.25
bigFieldSize
id2
id1
tv1
tv2
nTokens
termDocs1
termDocs2
getUTF8String
d2="
d1="
termEnum2
termEnum1
idString
"AString"
"different"
differentField
differentType
differentText
TestTerm
RepeatingTokenStream
testTermDocPerf
"milliseconds
TestTermdocPerf
percentDocs
maxTF
iteration:
"f50"
TestTermVectorsReader
"sortedSet
"tve.getPositions()
MyTokenStream
testBadParams
MyAnalyzer
point!"
"tve
testFields
TERM_FREQ
DocNumAwareMapper
tokenUpto
testFieldsStoreOff
testOffsetReader
fsMapper
testMapper
"Documentnumber
testPositionReader
"tve.getOffsets()
freqVector
sortedSet
testFieldsStorePos
docNumAwareMapper
setFailed
"luceneTestThreadedOptimize"
iterFinal
iFinal
writerFinal
TestThreadedOptimize
NUM_ITER2
testThreadedOptimize
expectedLastRecordId
rollBackLast
"Rolled
RollbackDeletionPolicy
FIELD_RECORD_ID
checkExpecteds
rollbackPoint
sdp
"Did
currentRecordId
TestTransactionRollback
"record_id"
testRepeatedRollBacks
DeleteLastCommitPolicy
1-"
testRollbackDeletionPolicy
"records
differ:
TestTransactions
r2="
r1="
RandomFailure
testTransactions
randomly
"#comment"
"ONE\n
checkSet
\nthree\n#comment"
TestWordlistLoader
testWordlistLoading
"|comment\n"
wordSet2
testComments
comment\n"
wordSet1
\t\n"
|comment\n"
testSnowballListLoading
wordset
"ONE\n"
\nthree"
|comment
"XX"
testNLSLoading_xx_XX
invalidSyntax
testNLSLoading_ja
testMessageLoading
"構文エラー:
testMessageLoading_ja
JAPANESE
"切り捨てられたユニコード・エスケープ・シーケンス。"
TestNLS
XXX"
testMissingMessage
testNLSLoading
key:Q0005E_MESSAGE_NOT_IN_BUNDLE
bar\"~99"
DumbQueryWrapper
getSuperFieldQuery
DumbQueryParser
TestMultiAnalyzer
testMultiAnalyzerWithSubclassOfQueryParser
TestMultiFieldQueryParser
wizard
"foo:zoo*"
"用語\u0020用語\u0020用語"
ozzy"
"*:foo"
QueryParser(CharStream)
"用語\u3000用語\u3000用語"
protected"
"foo:*"
QueryParser(QueryParserTokenManager)
ozzy\""
testProtectedCtors
"\"wizard
"foo:*^2"
"foo:zoo*^2"
9999999
intLength
99560
TestIndex
testPad
allowNegativeRandomInts
"Cache
0x5525aacb
"CachingWrapperFilterHelper("
'max
"hits1"
description=\""
length1="
docnumbers
productOf
top="
verifyExplanation
sumOf
",hits2="
bag
totalHits="
'sum
hitcollector
maxOf
others'
SetCollector
ExplanationAssertingSearcher
checkDocIds
non-match:
dval
"\nfor
valued
"Hits
scoreTolerance
maxTimesOthers
"Hit
subDetails
"hit="
"\nunequal
of'"
combined
"hits2"
EXPLAIN_SCORE_TOLERANCE_DELTA
combined=="
"TopDocs
deep
explanationScore="
nrs
SCORE_TOLERANCE_DELTA
"Unequal
ExplanationAsserter
'product
and:
"times
"Simple:
Explanation:
here!"
",\t"
"]]
"\tscore="
"Wrap
[["
"\tlength2="
of:'
0.00005f
hits2str
hits1="
score(doc="
JustCompileFilter
JustCompileQuery
JustCompileWeight
JustCompileScorer
JustCompileCollector
JustCompileFilteredTermEnum
JustCompileTopDocsCollector
JustCompilePhraseScorer
JustCompileFilteredDocIdSetIterator
JustCompileDocIdSetIterator
JustCompileSearcher
JustCompileFilteredDocIdSet
JustCompileSimilarity
JustCompileExtendedFieldCacheLongParser
JustCompileSearch
JustCompileFieldComparator
JustCompileSpanFilter
JustCompileFieldComparatorSource
JustCompileDocIdSet
JustCompileExtendedFieldCacheDoubleParser
scorerDiff
opidx
"tscorer.more="
assigned
skipTo("
next_op
Whacky
maxDiff="
"scorerScore2="
"query's
Order="
7.21792348f
checkFirstSkipTo
"Serialization
">!"
docs:"
">
"unstable
whacky
maxDiff
non-deleted
skipToScore
ois
"\n\thitCollector.doc="
"scorerScore="
scorerScore
sbord
skip_op
lastReader
scorerScore2
Scorer="
scoreDiff="
scorerDiff="
scoreDiff
scorerDoc="
Op="
<"
skip()"
orders
docs!"
Query="
Searcher="
makeEmptyIndex
checkSerialization
hits4
+xx
-xx
cb
"+w3
mulFactor
xx"
bug"
bigSearcher
"big
"w3
"-w3
queriesTest
+xx"
testQueries01
testQueries02
testQueries05
testQueries08
"w*"
allowMust
-w5"
testQueries10
testQueries03
testQueries04
testQueries07
testQueries06
testQueries09
+w2
NUM_EXTRA_DOCS
testOneReqAndSomeOptional
testSomeReqOneProhibAndSomeOptional
testSomeProhibAndSomeOptional
testNoOptionalButMin2
testOneOptionalEqualToMin
testOneReqSomeProhibAndSomeOptional
"0.000000"
testAllOptional
subset:\n"
top2
maxLev
otherScore
testMinEqualToNumOptional
testSomeReqSomeProhibAndSomeOptional
testOneReqOneProhibAndSomeOptional
"Constrained
testNoOptionalButMin
found\n"
minNrCB
testMinHigherThenNumOptional
top1
testOneProhibAndSomeOptional
TestBooleanMinShouldMatch
testSomeReqAndSomeOptional
verifyNrHits
testParenthesisMust2
testElements
production
TestBooleanOr
FIELD_C
FIELD_T
When
testFlat
testParenthesisShould
"deleting"
testParenthesisMust
"production"
environment."
"Deleted
rw1
rw2
"foodanddrink"
"food"
drink"
"foodanddrinkandgoodtimes"
"food
"unepxected
Clauses
Mismatch"
TestBooleanPrefixQuery
testNullOrSubScorer
"nestedvalue2"
testEquality
"nestedvalue1"
nested1
nested2
TestBooleanQuery
testEmptyBucketWithMoreDocs
TestBooleanScorer
received
3000"
booleanQuery1
NO_MORE_DOCS"
shouldCacheable
cachedSet
TestCachingWrapperFilter
uncached,
cacher
testIsCacheAble
testCachingWorks
assertDocIdSetCacheable
originalSet
-w1"
testMA3
testST6
testST3
"(+w1^0.0
testFQ5
"+w1^0.0
testBQ12
testBQ13
testBQ18
w2)^0.0"
20.0f
testMPQ7
testSF7
testSF3
0.2f
testDMQ10
"w1^0.0"
testT3
w2^0.0"
testSNot3
dm2
testSNot6
"-xx
testSNot8
-w5^0.0"
createQnorm1Similarity
testSNot9
w2"
testBQ21
testBQ22
-zz"
testCSQ4
"(+w1
TestComplexExplanationsOfNonMatches
"mandant"
idMap
luceneId
switcher
testFieldSortSingleSearcher
testFieldSortCustomSearcher
TestCustomSearcherSort
testFieldSortMultiCustomSearcher
idHitDate
matchHits
getLuceneDate
hits."
rank:
custSort
INDEX_SIZE
duplicate."
hitsByRank
resultSort
Possibliy
resultMap
CustomSearcher
hitid
"publicationDate_"
RandomGen
criteria:
"sunny"
York
sunny
df2
testBefore
"NoMatchForThis"
999999
"Today
testAfter
"datefield"
TestDateFilter
888888
actualOrder
DATE_TIME_FIELD
dateTimeString
textField
testReverseDateSort
1192104129000L
1192209943000L
"dateTime"
TestDateSort
1192001126000L
1192101133000L
expectedOrder
1192001122000L
dateTimeField
testBooleanOptionalWithTiebreakerAndBoost
scores"
docid"
testSimpleTiebreaker
d1:
"testSimpleTiebreaker"
"testSimpleEqualScores1"
"dek"
d3:
"d4"
worse
testSimpleEqualScores3
testSimpleEqualScores2
testSimpleEqualScores1
0.0000f
"found
d4:
"doc3
"albino"
others:
TestDisjunctionMaxQuery
"testBooleanRequiredEqualScores1"
"elephant"
"testBooleanOptionalNoTiebreaker"
"testBooleanOptionalWithTiebreaker"
"d3"
"doc2
"score0
>?
"0.000000000"
skipOk
"d4
testBooleanOptionalNoTiebreaker
"d1
fourth"
"d3
"d2
testBooleanOptionalWithTiebreaker
match?
testSkipToFirsttimeMiss
"doc0
"firsttime
testBooleanRequiredEqualScores
"testSimpleEqualScores3"
"4
d2:
testSkipToFirsttimeHit
"DOES_NOT_EXIST"
"testSimpleEqualScores2"
"testBooleanOptionalWithTiebreakerAndBoost"
TestDocBoost
testFilteredDocIdSet
TestDocIdSet
testNullDocIdSet
"gotten:
"answer:
filteredSet
intIter
maxdoc
getElevatedQuery
TestElevationComparator
"NEVER"
int2str
"KEY"
"MATCH"
testNoop
reqB
optB
"floats
TestFieldCache
floats
"ints
"longs
testInfoStream
shorts
"doubles
"shorts
testFieldCacheRangeFilterDoubles
ranges"
case"
"inverse
testFieldCacheRangeFilterShorts
testFieldCacheRangeFilterRand
testSparseIndex
TestFieldCacheRangeFilter
concenatted
medIdO
.5f
maxIdO
testFieldCacheRangeFilterLongs
"DocIdSet
"infinity
fcrf
"overflow
minIdO
testFieldCacheRangeFilterFloats
testFieldCacheRangeFilterInts
TestFieldCacheTermsFilter
y"
assertScoreEquals
"sorter"
2.5f
TestFilteredQuery
testFilteredQuery
filteredquery
newStaticFilterB
newStaticFilterA
0.0000001f
TestFilteredSearch
enforceSingleSegment
SimpleDocIdSetFilter
testFilteredSearch
filterBits
searchFiltered
"36"
Willis"
testBoostOnlyRewrite
"ddddX"
"bbbbb"
"student"
"aaaaa"
Art
"Bruce
"Joanne
"anotherfield"
bruce"
"Brute
"aaccc"
"Giga
"ManagingGigabytesManagingGigabyte"
"segment"
"12345678911"
testFuzziness
"ManagingGigabytesManagingGigabytes"
Rowling"
"Willis
"J.
testGiga
K.
"Lucenne"
willis"
testTokenLengthOpt
Science"
testFuzzinessLong
"abbbb"
"stellent"
"aaabb"
Roling"
"aaaaaaa"
"aaaab"
"aaaaccc"
"aaacccc"
"sdfsdfsdfsdf"
Action"
"B.
Dummies"
"aaaac"
"giga~0.9"
"JK
"1234569"
"1234567891"
"aabbb"
TestFuzzyQuery
normsQuery
300f
maq
TestMatchAllDocsQuery
400f
"pie"
trouble
pizza\""
bluebird)
testHashCodeAndEquals
note"
"someMoreText"
raspberry
"blue
testBooleanQueryContainingSingleTermPrefixQuery
(piccadilly
"body:\"strawberry
TestMultiPhraseQuery
testPhrasePrefixWithBooleanQuery
"someField"
"someText"
"object"
pizza)\""
pie
"body:\"blueberry
"body:\"(blueberry
"pizza"
indexSearcher2
hits3
termsSet
customSimilarity
time....."
galaxy
subSearcherQuery
searcher1
"id:doc1"
away....."
contents2
searchers3
msrchr
searchers2
scoreN
"MultiSearcher
testCreateDocFrequencyMap
testDocFreq
contents1
mSearcher2
bizarre
ramDirectory2
testCustomSimilarity
testFieldSelector
ramDirectory1
"doc0"
docFrequencyMap
"Once
mSearcher3
upon
"id:doc2"
manifested
itself...."
indexSearcher1
testNormalization10
testTwoTermQuery
pi*\""
TestMultiSearcherRanking
multiThreee"
testOneTermQuery
queryStr
+nomatch"
"multiThree~"
addCollection1
"{multiA
addCollection2
"Single:
multiSearcherHits
docMulti
multiP}"
"Multi:
singleSearcherHits
singleSearcher
"+three
multiTwo"
testNoMatchQuery
"\"blueberry
checkQuery
testRangeQueryRand
"max,max,F,F,c"
ih
testRangeQueryIdCollating
testRangeQueryId
"min,min,F,F,c"
"med,med,F,F,c"
TestMultiTermConstantScore
small
"min,min,T,T,c"
"p*u?"
"pr*t?j"
"mismatch
cspq
"max,max,T,T,c"
"pres*"
testBasicsRngCollating
testBooleanOrderUnAffected
"pre*n?t"
"nul,min,F,T,c"
cswcq
wild
hit#"
numebr
testRangeQueryRandCollating
"max,nul,T,T,c"
testEqualScores
"med,med,T,T,c"
"pre*"
MultiThreadTermVectorsReader
verifyVectors
result"
runsToDo
TestMultiThreadTermVectors
mtr
threadsAlive
trTopDocs
"trie"
nrTopDocs
TestMultiValuedNumericRangeQuery
"00000000000"
testMultiValuedNRQ
testNot
TestNot
TestNumericRangeQuery32
'field4'
testEnum
Integer.MAX_VALUE
6666
"TermEnum
testFloatRange_2bit
testFloatRange
Integer.MIN_VALUE
"test14"
testFloatRange_8bit
terms."
testFloatRange_4bit
testDoubleRange
testSorting_6bit
testRange_6bit
Long.MAX_VALUE
testLeftOpenRange_6bit
ascfield6
"field6"
testDoubleRange_8bit
field6
66666L
Long.MIN_VALUE
testRandomTrieAndClassicRangeQuery_6bit
"ascfield6"
testDoubleRange_2bit
TestNumericRangeQuery64
testRangeSplit_6bit
testDoubleRange_4bit
testDoubleRange_6bit
testRightOpenRange_6bit
TestParallelMultiSearcher
TestPhrasePrefixQuery
notexist
testBarelyCloseEnough
testWrappedPhrase
testSlop1
"notexist"
"slop
sloppy
testEmptyPhraseQuery
testSlopScoring
woo"
moves"
repeatedField
stopAnalyzer
testPhraseQueryWithStopAnalyzer
"phrase
0.44
"reversed,
matter"
"palindrome"
"woo
entry"
"woo"
testNonExistingPhrase
0.31
"lastname"
foobarword
"marketing
yyy
testPalyndrome2
"reverse
testPalyndrome3
testMulipleTerms
right"
enough"
testOrderDoesntMatter
"nonexist"
repeated
"nonexisting
"exact
testPhraseQueryInConjunctionScorer
exact"
testExact
testNotCloseEnough
"firstname"
TestPhraseQuery
"field:\"?
lastname
"marketing"
firstname
"found"
0.71
"entry"
part"
test\""
is\""
hi|hello
scorer"
termAttr
StopWhitespaceAnalyzer
mq
INCREMENTS
pspans
"pos:
testSetPosition
sawZero
"\"1
pls
testPayloadsPos0
payloadAttr
TestPositionIncrement
posIncrAttr
testNegativeScores
TestPositiveScoresOnlyCollector
return:
numPositiveScores
"/Computers/Mac/One"
"/Computers/ZZZ"
TestPrefixFilter
testPrefixFilter
"/Computers/Linux"
"/Computers/Mac/Two"
"/Computers/AAA"
"nonexistantfield"
"/Computers/ObsoleteOS"
"tangfulin"
testPrefixBooleanQuery
"tang"
11377
testTermBooleanQuery
"meaninglessnames"
"notexistnames"
TestPrefixInBooleanQuery
5138
5137
TestPrefixQuery
/Computers/Mac"
below"
/Computers
goldFreqs
checkGold
testConstructor
go"
TestQueryTermVector
"valu"
"not_exist"
TestQueryWrapperFilter
qwf
TestScoreCachingWrappingScorer
testGetScores
mscores
ScoreCachingCollector
numToCollect
numBitsToSet
randBitSets
termflag
oClauses
nClauses
termsInIndex
MatchingHitCollector
doConjunctions
doNestedTermConjunctions
doSloppyPhrase
createDummySearcher
nMatches
maxOuterClauses
randBitSet
matches="
nextClearBit
maxClauses
createRandomTerms
TestScorerPerf
testConjunctions
doTermConjunctions
validate
doNestedConjunctions
16.0f
TestSetNorm
testSetNorm
testSimilarity
testBQ15
testMA1
testMA2
"\"w2
"\"w3
testP2
testP3
testP7
testP4
testP5
w2^1000.0"
-QQ"
"+w1
-w2)
+w4))"
testFQ2
testFQ3
testFQ1
testFQ6
testFQ4
"+yy
testTermQueryMultiSearcherExplain
"maxDocs=3"
(xx
testBQ10
testBQ11
+(qq
testBQ14
testBQ16
testBQ19
"1=3"
+w2"
testMPQ1
testMPQ2
testMPQ3
testMPQ4
testMPQ5
testMPQ6
testP1
"handle:\"1
testP6
"docFreq=3"
w2\"~3"
+w3"
w3\""
(+w3
testBQ8
testBQ6
(-xx
testBQ4
testBQ5
testBQ2
testBQ3
testBQ1
"xx^1000"
(qq
-(-qq
"xx^100000"
w5)"
w2\"~2"
testT2
testT1
testBQ9
testBQ17
(+qq
w3\"~1"
testDMQ8
"w1^1000"
testDMQ9
"QQQQQ"
testBQ7
"-yy
testDMQ4
w2)
"2=3"
testBQ20
-(+w3
w2\""
testCSQ1
testCSQ3
testCSQ2
testDMQ2
testDMQ3
testDMQ1
testDMQ6
testDMQ7
testDMQ5
TestSimpleExplanationsOfNonMatches
doc:
expectedNumResults
DOC_2
DOC_2_B
numResultsExpected
testDoc1_Query1_All_Slops_Should_match
testDoc2_Query1_Slop_6_or_more_Should_match
QUERY_4
QUERY_2
QUERY_1
TestSloppyPhraseQuery
score2="
+"
"slop="
Wrong
testDoc3_Query1_All_Slops_Should_match
S_1
S_2
makePhraseQuery
testDoc2_Query2_All_Slops_Should_match
testDoc4_Query4_All_Slops_Should_match
DOC_4
DOC_3
DOC_1
DOC_1_B
checkPhraseQuery
DOC_3_B
"slop:
"CEAGI"
testReverseSort
LongParser"
field1(field2)(docID):"
"AECIG"
FloatParser"
testTopDocsScores
testStringSort
"topn
getYIndex
"nosuchfield"
"DJHBF"
testOutOfOrderDocsScoringSort
"IGEAC"
parallelSearcher
"JIHGFEDCBA"
num2
testSortWithScoreNoMaxScoreTracking
"rev
testNewCustomFieldParserSort
"DJHFB"
testSortWithScoreAndMaxScoreTrackingNoResults
actualTFCClasses
"IGECA"
testSortCombos
testBuiltInSorts
testSortWithScoreAndMaxScoreTracking
lastSub
getEmptyIndex
"IEGCA"
"OutOfOrderOneComparatorScoringNoMaxScoreCollector"
testCustomFieldParserSort
testMultiSort
testTypedSort
1234567890L
"OutOfOrderOneComparatorScoringMaxScoreCollector"
testSortWithoutFillFields
"AGICE"
"DHFBJ"
"ECAGI"
testEmptyFieldSort
"GICEA"
lastDocId
"tracer2"
searchY
searchX
NUM_STRINGS
"DFHBJ"
"JFHDB"
"FBJHD"
"BFHJD"
"CIGAE"
"AIGEC"
"ZYXW"
"fail:"
"GCIEA"
testLocaleSort
"CEGIA"
ByteParser"
DoubleParser"
Locale.US
various"
"ZWXY"
IntParser"
testSortWithoutScoreTracking
getXIndex
fail:"
Locale.UK"
ShortParser"
"ACEGI"
"CAEGI"
"DHJFB"
getFullStrings
"OutOfOrderOneComparatorNonScoringCollector"
testParallelMultiSort
UK
queryG
queryE
"JHFDB"
"IGAEC"
"BJFHD"
"BDFHJ"
tfcOptions
"DHFJB"
testInternationalSort
testInternationalMultiSearcherSort
"IAGCE"
"docIdSet
TestSpanQueryFilter
getDocIdSetSize
10"
assertContainsDocId
testFilterWorks
"info.getPositions()
TestTermRangeFilter
testRangeFilterRand
testRangeFilterRandCollating
testRangeFilterIdCollating
"notcontent"
A,B,C
upperterms
"A,B,C,D
B,
testTopTermsRewrite
"A,B,<empty
term:
SingleCharTokenizer
testEqualsHashcode
"A,B,C,D,
A,B,<empty
testExclusive
string>,C
"hashcode
string>,D
string>,C,D
testExclusiveLowerNull
without"
added,
testInclusiveLowerNull
SingleCharAnalyzer
initializeIndex
"A,B,D
"A,B,D,
TestTermRangeQuery
string>
"equivalent
testInclusive
allowedTerms
lowerterms
savedClauseCount
checkBooleanTerms
itself
equal()"
A,
<empty
testInclusiveCollating
insertDoc
testExclusiveCollating
allTerm
"fetch"
1.6931472f
TestHit
"dogs
"next
TestTermScorer
"TestHit{"
"playing"
test3
testMixedVectrosVectors
chocolate
mod2
mod3
testKnownSetOfDocuments
test4Map
vTerms
testRareVectors
freqInt
posVec
correct:"
"computer
"lab"
testTermVectorsFieldOrder
freqVec
"mapper.getTermVectorEntrySet()
testTermOffsetVectors
testDoc2
testDoc3
testDoc1
"old"
colored
shouldBePosVector
sorted"
lab"
"chocolate"
shouldBeOffVector
"vectorEntrySet
"eating"
testDoc4
expectedFields
"eating
TestTermVectors
test4
knownSearcher
expectedFreq
vectorEntrySet
grows
lab
fieldMapper
flen
val="
validateField
buildDir
maxFieldLen
TestThreadSafe
loadDoc
tarr
"^
Thr
testLazyLoadThreadSafety
threadArray
exceptionDoc="
doTestTimeout
N_THREADS
exceptionDoc
MULTI_THREAD_SLACK
required:
timoutException
"greedy="
totalTLCResults
pancakes"
0!"
multiThreaded
withTimeout
"elapsed="
tlCollector
slowdown
"greedy,
SLOW_DOWN
usual):
probably
slower
testSearchMultiThreaded
allowed="
oneHour
"Informative:
testTimeoutNotGreedy
(allowed-resolution)="
lastCollected="
"docThatNeverMatchesSoWeCanRequireLastDocCollectedToBeGreaterThanZero"
totalResults
maxTimeStr
createTimedCollector
"lastDoc="
testTimeoutMultiThreaded
lastCollected
TestTimeLimitingCollector
,&&
setSlowDown
testTimeoutGreedy
maxTime
"2*resolution
testModifyResolution
expected!"
MyHitCollector
doTestMultiThreads
(no
elapsed="
base="
results!"
myHc
testSecondResultsPages
testMaxScore
MyTopsDocCollector
8.601509f
6.9916306f
1.3077754f
numResults
7.888485f
9.17561f
testZeroResults
testFirstResultsPage
doSearch
7.9793315f
0.28146625f
8.365894f
5.383931f
6.8489285f
6.953706f
8.723962f
MAX_SCORE
3.1796896f
5.060466f
testGetAllResults
testGetResultsFromStart
2.9655676f
TestTopDocsCollector
4.1858315f
0.39971232f
testResultsOrder
TestTopScoreDocCollector
testOutOfOrderCollection
"InOrderTopScoreDocCollector"
"org.apache.lucene.search.TopScoreDocCollector$"
actualTSDCClass
"OutOfOrderTopScoreDocCollector"
Id
"*n?"
"*t?1"
testPrefixTerm
"*hij**"
"M*tal*"
"*1"
"metals?"
"*ab*"
"h*"
qtxt="
"\\79
"*hi*"
abcdefg1"
"op*q*"
getIndexStore
hijklmn1"
0.4F
"nowildcardx"
testParsingAndSearching
"\\\\7*"
"\\\\\\\\*"
0.3F
testEmptyTerm
testTermWithoutWildcard
"metal?"
"M?t?ls"
"\\\\
wq
0.2F
"mXtals"
"b*a"
"hi*k*"
"matchNone:
"mXtXls"
matchOneDocWild
"metals"
"prefixx"
expectedMatches
"*a*h"
"opq**"
prefix:
testQuestionmark
"*o*"
"nowildcard"
"opq*"
testAsterisk
"m?t?ls"
TestWildcard
"m*tals"
"a*h"
0.1F
"a?h"
"metal*"
"o*"
"metal"
"\\\\*"
"*a*"
wild:
wq3
wq2
wq1
"*h*"
q="
matchAll
"?a"
query6
query7
query5
query8
"***"
"m*tal*"
"matchAll:
"*tall"
"*tal*"
"abc*"
"hij**"
"abc**"
"hi*"
"*g?"
"*opq**"
"prefix*"
"op*"
"*f?1"
"hij*"
"*m?1"
"*abc**"
"*u?"
"*?1"
"m*tal"
matchOneDocPrefix
"?*1"
"meta??"
"*tal"
"m?tal"
matchNone
"*op*"
"ab*e*"
"**1"
"*?"
opqrstu1"
"added:
2!"
"000000000"
docIDFieldVal
DOC_TEXT_LINES
doMultiSegment
scoreAndID
textLine
N_DOCS="
JustCompileDocValues
JustCompileFieldCacheSource
JustCompileSearchFunction
JustCompileValueSource
"same
identical,
testCustomScoreShort
q5CustomMulAdd
td5CustomMulAdd
CustomMulAddQuery
"rewritten
by:
q4CustomAdd
"Explain
logResult
h3CustomMul
neutral"
mul"
td4CustomAdd
fieldScore
td1
(just
q3CustomMul
td2CustomNeutral
"CustomMulAdd,
testCustomScoreInt
boosted)
rewrite"
h2customNeutral
CustomAddQuery
td3CustomMul
"score5="
add"
testCustomScoreByte
h4CustomAdd
subScore
"#hits
h2CustomNeutral
topDocsToMap
testCustomExternalQuery
exp2
h5CustomMulAdd
testCustomScoreFloat
dboost
qValSrc1
qValSrc2
"score1="
testRewrite
score5
TestCustomScoreQuery
doTestCustomScore
score4
"customAdd"
CustomExternalQuery
"new
"score2="
"customMulAdd"
"score3="
q2CustomNeutral
rewrites"
qValSrc
verifyResults
#hits"
"score4="
"fieldScore
mul
"-1.0f
"10.0f
testGetAverageValue
0.5f"
testGetMaxValue
3.5f"
testGetMinValue
1.0f"
DocValuesTestImpl
TestDocValues
testExactScoreInt
testRankByte
testExactScoreShort
expectedArrayTypes
testRankShort
testCachingFloat
testExactScoreByte
testExactScoreFloat
testCachingByte
aSd
testCachingShort
testCachingInt
testRankInt
"compare:
testRankFloat
TestFieldScoreQuery
testCachingReverseOrd
Explain
testOrdFieldExactScore
testReverseOrdFieldExactScore
TestOrdValues
"--------
"IC"
testReverseOrdFieldRank
shuold
expectedId
"IE"
testOrdFieldRank
field!"
testCachingOrd
differ):
"noPayloadField"
wordList
ninety"
ninety
nine"
"[\\s]+"
hundred"
testLongerSpan
testComplexNested
hundred
"ninety
testPayloadNear
"twenty
TestPayloadNearQuery
payload4
payload2
spanTermQuery
testIgnoreSpanScorer
numTens
testMultipleMatchesPerDoc
noPayloadField
boostingFuncTermQuery
testNoMatch
boostingFuncTermQuery2
FullSimilarity
TestPayloadTermQuery
theSearcher
testNoPayload
JustCompileSpanScorer
JustCompileSpans
JustCompileSearchSpans
JustCompilePayloadSpans
JustCompileSpanQuery
347
testTerm2
testSpansSkipTo
736
t6
t4
t5
testSpanNearUnordered
591
592
595
594
597
596
599
598
270
271
272
273
274
275
277
278
524
525
526
527
520
521
522
523
528
447
906
646
649
433
699
testSpanOr
851
559
558
746
747
557
556
550
testSpanExactNested
testSpanNearExact
147
testTerm
near2
near1
514
689
687
testNpeInSpanNearWithSpanNot
TestBasics
tt2
tt1
testSpanComplex1
378
371
373
372
375
374
377
706
707
797
796
607
586
587
584
585
583
580
581
588
589
519
518
"seventish"
511
510
513
515
517
516
626
590
629
593
976
975
974
973
972
971
970
979
656
657
659
hun
861
to1
881
569
756
"sevenon"
775
774
777
776
771
770
773
772
779
778
686
669
667
666
696
697
542
543
540
541
546
547
544
545
548
549
996
529
to2
term5
term4
833
831
786
787
testSpanWithMultipleNotSingle
579
573
572
571
570
577
576
575
574
726
testNpeInSpanNearInSpanFirstInSpanNot
733
508
506
507
505
633
636
637
639
skipToAccoringToJavaDocs
term40c
testPhrase
878
879
877
874
875
872
873
870
871
891
647
582
"eight"
801
766
767
578
testSpanNearOrdered
678
679
674
675
676
677
670
671
672
673
537
536
535
534
533
532
531
530
testSpanWithMultipleNotMany
539
538
986
847
821
"sevento"
term40
568
757
737
560
561
562
563
564
565
566
567
504
testPhrase2
727
609
testSpanNearOr
"sevenly"
509
472
473
470
471
476
477
474
475
478
479
going"
"spanA
"spanB
going
testNoop1
qB
testSpans2
8.7654321f
testEquality1
testSpans1
testSpans0
"linda"
testRewrite2
spanA
"gender"
"XXXXX"
testRewrite0
testRewrite1
"costas"
spanB
"bubba"
tough
TestFieldMaskingSpanQuery
9f
"sally"
"male"
qA1
qA2
qA
"james"
"greta"
"lisa"
"xXXX"
testNoop0
spanB"
testSimple1
testSimple2
"female"
"dixit"
testNearSpansNextThenSkipPast
"Scorer
TestNearSpansOrdered
testNearSpansNext
testNearSpansSkipPast
testNearSpansSkipToLikeNext
testSpanNearScorerSkipTo1
testNearSpansSkipTo1
testNearSpansSkipTo0
testSpanNearQuery
testSpanNearScorerExplain
positive:
doc#1
testNearSpansNextThenSkipTo
s3
payloads:"
"k:Noise:11"
"payload[0]
TestPayloadSpans
"match:"
numSpans
testNestedSpans
nestedSpanNearQuery
s:"
testFirstClauseWithoutPayload
testShrinkToAfterShortestMatch2
testShrinkToAfterShortestMatch3
pp"
"payloads
ten
"np"
"nopayload
expectedNumPayloads
testHeavilyNestedSpanQuery
"a:Noise:10"
109
getSpanNotSearcher
"isPayloadAvailable
spanNearQuery3
ten"
span:"
nopayload
"eleven"
be:
eleven
"payload
testShrinkToAfterShortestMatch
":Entity:"
eleven"
checkSpans
expectedPayloadLength
spanNearQuery2
"\nSpans
"mark"
"j
clauses2
clauses3
nine
testSpanTermQuery
expectedNumSpans
spq
np"
"nopayload"
e:"
testPayloadSpanUtil
spans:"
numPayloads
eight
expectedFirstByte
payloadSet
"doc:"
":Noise:"
testST4
testSNear4
testSNear7
testSNear6
testSNear1
testST1
testST2
testSNear2
testSNear8
testSNot10
testSNear5
testST5
testSNear3
testSO1
testSO3
testSO2
testSO4
testSNear9
testSF6
testSF4
testSF5
testSF2
testSF1
testSNot2
testSNot1
testSNot7
testSNot5
testSNot4
testSNear11
testSNear10
TestSpanExplanationsOfNonMatches
skipTo"
testSpanNearOrderedEqual01
testSpanNearOrderedEqual03
testSpanNearOrderedEqual02
chased
testSpanNearOrderedEqual04
orderedSlopTest3Equal
next"
TestSpans
orSpans
"initial
u1u2
testSpanScorerZeroSloppyFreq
"u2"
testSpanOrDoubleSkip
testSpanOrTripleSameDoc
testSpanNearUnOrdered
orderedSlopTest3SQ
createSpan
ate
u2"
"unusedTerm"
orderedSlopTest3
"t1
t3"
spanScorer
"rabbit"
mouse,
expectedDocs
u1"
"final
"chased"
testSpanOrDouble
testSpanOrUnused
testSpanNearOrdered01
"u2
"u1
quickly"
"third
testNPESpanQuery
zero,
market"
testSpanNearOrderedOverlap
testSpanOrEmpty
orderedSlopTest1Equal
mouse
makeSpanTermQuery
shouldn't:
testSpanNearOrderedEqual14
testSpanNearOrderedEqual15
testSpanNearOrderedEqual12
testSpanNearOrderedEqual13
testSpanNearOrderedEqual11
testSpanOrMovesForward
sqa
testSpanNearOrdered02
testSpanNearOrdered03
testSpanNearOrdered04
testSpanNearOrdered05
testSpanOrSingle
tstNextSpans
"Has
"u1"
0.3884282f
scoreEq
tolerance
FIELD_ID
"TEXT"
warning,
topdocs
think
0.26516503f
we,
0.35355338f
Should
should.
testSingleSpanQuery
"It
we."
0.93163157f
we?"
0.45927936f
spanQuery2
"single
0.625f
1.0191123f
it?"
spanQuery1
TestSpansAdvanced2
testMultipleDifferentSpanQueries
0.73500174f
shouldn't."
testVerifyIndex
getMaxSizeInBytes
createdFiles
unSyncedFiles
"MockRAMDirectory:
preventDoubleWrite
crash"
setNoDeleteOpenFile
to"
randomIOExceptionRate
getRandomIOExceptionRate
noDeleteOpenFile
overwrite"
getNoDeleteOpenFile
forced
singleByte
freeSpace
crashed;
"MockRAMDirectory
realUsage
TEST_FILE_LENGTH
"pos="
byten
filepos="
inputBufferSize
checkReadBytes
testReadBytes
"33"
tweakBufferSizes
testEOF
testReadByte
"testSetBufferSize"
"tmpFile"
TestBufferedIndexInput
MyBufferedIndexInput
tmpInputFile
ip
testSetBufferSize
allIndexInputs
bii
runReadBytes
runReadBytesAndClose
MockFSDirectory
"IndexInput"
".lck"
"testsubdir"
checkDirectoryFilter
"subdir"
lock2
"afile"
TestDirectory
testDontCreate
"testDirectInstantiation"
testCopySubdir
"foo."
testFSDirectoryFilter
"testnotdir"
lockname
testRAMDirectoryFilter
testDirectInstantiation
testNotDirectory
testDetectClose
fileExtensions
TestFileSwitchDirectory
0x0003F
j="
bt
"output
DenseRAMFile
0x0007F
written!
TestHugeRamFile
testHugeFile
singleBuffers
testObtain
TestLock
LockMock
(after
RAMDirectory"
testSimpleFSLockFactory
specificLockName
testRAMDirectoryNoLocking
prefix2
testCustomLockFactory
lockPrefixSet
TestLockFactory
testStressLocks
"index.TestLockFactory7"
twice"
testDefaultRAMDirectory
testNativeFSLockFactoryPrefix
freed"
out:"
"index.TestLockFactory6"
WriterThread
IndexWriters
"IndexSearcher
hitException
"TestLockFactory.8"
"RAMDirectory.setLockFactory
testNativeFSLockFactory
here.
MockLock
IndexWriter)"
prefix1
obtaining
fdir2
"RAMDirectory
Searcher:
testDefaultFSLockFactoryPrefix
LockFactory:
Lock.obtain
MockLockFactory
"Stress
"succeeded
take"
_testStressLocks
makeLockCount
fdir1
"TestLockFactory.10"
fs1
testNativeFSLockReleaseByOtherLock
outside
SingleInstanceLockFactory"
"TestLockFactory.8.Lockdir"
numIteration
Writer:
calls
testStressLocksNativeFSLockFactory
locksCreated
TestRAMDirectory
"contains
"sizeContent"
ObjectOutput
testRAMDirectorySize
"initially
"RAMDirIndex"
docsPerThread
testIllegalEOF
docsToAdd
headerSize
"abcdefghijklmnopqrstuvwzyz"
storePathname
randomField
storeDirectory
cx
TestWindowsMMap
"testLuceneMmap"
randomToken
alphabet
testMmapIndex
fx
isSimpleFSIndexInputClone
testWithDifferentLocales
changed:"
unexpectedly
systemLocales
getAvailableLocales
defaultLocale
LuceneTestCase.newRandom
reportAdditionalFailureInfo
"tests.verbose"
TestWatchman
LuceneTestCaseJ4.newRandom
"<unknown>"
'java.io.tmpdir'."
tests,
model
FrameworkMethod
@Test."
'tempDir'
annotated
"tempDir"
checkedClasses
"cost
10.0
"1923"
nextSize
copyCost
copyCostPerElement
testMaxSize
1923
elemSize
testParseInt
"-10000"
testInvalidElementSizes
"0.34"
TestArrayUtil
testGrowth
"TestTerm"
"Hash
testToStringAndMultiAttributeImplementations
TypeAttribute,
"Attributes
"AnotherTestTerm"
TestAttributeSource
left"
"Both
typeAtt"
OffsetAttributeImpl"
typeAtt2
PayloadAttributeImpl"
testCloneAttributes
"Iterator
TypeAttributeImpl"
src2
src3
termAtt"
testDefaultAttributeFactory
restoreState()
FlagsAttributeImpl"
instances"
touched"
"AnotherTestType"
PositionIncrementAttributeImpl"
AttributeSources
"OtherTerm"
"OtherType"
restore"
testCaptureState
printed
"TestType"
third
different"
TermAttributeImpl"
testDgaps
testWriteRead
doTestDgaps
3123
3126
doTestWriteRead
bv2
415
418
doTestSubset
TestBitVector
"TESTBV"
doTestCountVectorOfSize
doTestClearVectorOfSize
testCount
count1
testGetSet
doCompare
createSubsetTestVector
testConstructSize
doTestConstructOfSize
doTestGetSetVectorOfSize
subsetPattern
testSubset
"1234\ud801"
"orld"
"hellow"
"1234\ud801\udc1c789123\ud801\ud801\udc1c\ud801"
"23\ud801\ud801\udc1c\ud801"
"123\ud801"
testCodePointAtCharArrayInt
testCodePointAtCharArrayIntInt
testFillNoHighSurrogate
'\udc1c'
"\ud801"
cpAt3
java5
java4
"\udc1c7891"
StringIndexOutOfBoundsException
testCodePointAtCharSequenceInt
"\ud801\udc1c789"
testNewCharacterBuffer
"array
TestCharacterUtils
"Abc\ud801"
testFillJava14
testFillJava15
highSurrogateAt3
testInitValue
InitValueThreadLocal
TEST_VALUE
testNullValue
testDefaultValueWithoutSetting
TestCloseableThreadLocal
"initvaluetest"
ctl
readerX
readerA
TestFieldCacheSanityChecker
testInsanity1
testInsanity2
testInsanity3
wB
wA
dirB
dirA
INSANITY"
errors"
testSanity
insanity"
"theString"
testAllNullInput
0xE4
NUM_RANDOM_TESTS
encodedBuf1
encodedBuf2
TestIndexableBinaryStringTools
"decode()
binaryBuf
encodedLen2
2:
differently:"
charArrayDumpNIO
testSingleBinaryRoundTripNIO
encodedComparison
testRandomBinaryRoundTripNIO
testEncodedSortabilityNIO
MAX_RANDOM_BINARY_LENGTH
testEncodedSortability
originalBuf1
numBytes2
numBytes1
testAllNullInputNIO
"encode()
charBuf
"Round
byteNum
originalArray1
testEmptyInput
originalBuf2
results:"
"original:
original:
0xA6
binaryDump
"decoded
testRandomBinaryRoundTrip
decoded
charNum
testSingleBinaryRoundTrip
randomInt
0xB2
encodedBuf
Original
decoded:
decodedBuf
testEmptyInputNIO
binaryDumpNIO
"encodedBuf:
0xC9
encodedLen
decode/decode
decodedLen
trip
encoded2
encoded1
encodedLen1
originalComparison
charArrayDump
0xD8
original2
originalString1
originalString2
originalStringBuf1
originalStringBuf2
"decodedBuf:
encoded:
testNum
"forward
1024L
shift="
1.0E-1f
0x7fffffffffffecfL
64765767
double"
0x7fffecf
TestNumericUtils
prefixVal
0x8000000000001L
0x7fffed
testIntSpecialValues
4000L
1.0E-2f
0xfL
0x8000040
overlap"
2.3E25f
testSplitIntRange
testSplitLongRange
0x7fffec8
0x7fffef
50006789999999999L
0x3L
1.0E15f
0x7fffec78
0x80000000000003L
0x800000000000040L
concenated
1.0E15
sub-range
0x00L
bound"
765878989
0x7fffffffffffefL
"ranges
9500L
0x7fffffffffffec7fL
"decoding
fail"
assertIntRangeSplit
testFloats
63L
"actual
5003400000000L
0x7fffffffffffec8L
0x7ffff
0x80000000000024L
9500
0x80000000000000L
intVals
longVals
2.3E25
0x800000000000250L
0x8000043
0x8000251c
300L
0xffL
assertLongRangeSplit
0x800003
0x8000000000002510L
"difference
testDoubles
neededBounds
1.0E-1
conversion
1.0E-2
coded
0x7fffffffffffec78L
0x800024
0x7fffec7f
0x80000000000020L
0x8000250
0x80002510
testLongConversionAndOrdering
0x80001
"min,
0x7fffffffffffedL
100000L
0x800000000000043L
0x800000000000251cL
useBitSet
0x7ffffffffffffL
testIntConversionAndOrdering
4000
testLongSpecialValues
prefixVals
0x800020
doIterate2
doIterate1
testHashCodeEquals
testBitUtils
sum2
IntegerQueue
testPQ
testFixedSize
testInsertWithOverflow
TestPriorityQueue
i3
i5
i4
i6
strin"
TestRamUsageEstimator
"catchmaster"
str"
rue
Holder
"hollow"
b4
b3
exponent
orig_floatToByte
testFloatToByte
f4
mantissa
f5
TestSmallFloat
orig_byteToFloat
0xffffff
b5
testByteToFloat
test07c
test07a
tstInts
expectedByteSize
test06b
tstVIntList
tstViaBitSet
test07b
vIntListByteSize
MAX_INT_FOR_BITSET
TestSortedVIntList
test04b
tstIterator
test12
fibArray
vintList
test08c
test08b
Matcher"
VB3
test13Allocation
test02
test01
test06
test10
VB2
VB4
test11
test05
test05b
"Size"
svil
test04c
reverseDiffs
test04a
test08a
tstIllegalArgExc
vIntByteSize
testStringDifference
TestStringHelper
prevInterned
randStr
internedStrings
TestStringIntern
myInterned
testStringIntern
testStrings
otherInterned
interned
makeStrings
TestVersion
"LUCENE_CURRENT
onOrAfter("
always
"Method
protectedTest
"protectedTest"
TestClass4
TestClass5
TestClass2
TestClass3
TestClass1
IAE
declared
bogus()
publicTest(String)"
TestClass2,
"Violating
publicTest(String)
"LuceneTestCase
TestVirtualMethod
"publicTest"
succeeded"
publicTest
protectedTestMethod
publicTestMethod
64512
CACHE_SIZE
CacheThread
totHit
miss
TestDoubleBarrelLRUCache
testThreadCorrectness
addResults
OBJ_COUNT
totMiss
TestSimpleLRUCache
documentHasDefault
useDocSet
clusterer
engineNL
Clustering
searchHasDefault
Engines"
"Finished
useCollection
Initializing
documentClusteringEngines
searchClusteringEngines
useResults
"clustering."
"collection"
"docs.useDocSet"
CLUSTERING_PREFIX
produceSummary
initAttributes
docsHolder
docAsList
getAllDocuments
Sets
isBlank
snippetField
outputSubClusters
outputClusters
snippetFieldAry
carrotAlgorithmClassName
clusteringAlgorithmClass
carrotClusters
algorithmClass
blank."
docsIter
highlt
"solrId"
getClusters
"Carrot2
outCluster
maxLabels
CachingController
clustersToNamedList
controller
getPhrases
carrotDocument
urlField
newHashSet
configured,
extractCarrotAttributes
idFieldName
getSubclusters
theQuery
StringUtils
"algorithm"
"outputSubClusters"
"carrot."
ImmutableSet
"produceSummary"
"numDescriptions"
CARROT_PREFIX
"fragzise"
"Offers
Web
foundations
relationships
Resources,
Business
discovering
mainstream
Allows
data."
differentiated
SQL.
software,
"Statistical
"data
nature,
Pentaho
certificates
"http://www.investorhome.com/mining.htm"
courses,
explore
Contact
"GRC
standards-compliant
\"DATA
warehousing
Wikipedia,
accomplished
MicroStrategy,
social
Home
involves
useful
Book
market
June
Microsoft
sharing
"http://www.twocrows.com/"
works."
customer
organizations
patterns."
Crows
"Insightful
concepts
dedicated
predict
"http://www.thearling.com/"
"http://www.datamininglab.com/"
Moreover,
intelligent
"http://www.datamining.com/"
applications
Powers
behaviors,
Alvis
basket
"http://ocw.mit.edu/OcwWeb/Sloan-School-of-Management/15-062Data-MiningSpring2003/CourseHome/index.htm"
"About.com
Thearling's
development,
What
development.
hand,
"http://www.thearling.com/text/dmwhite/dmwhite.htm"
Mining?"
through
statistical
scoring
(more
technologies."
Consultant.
"http://www.statsoft.com/products/dataminer.htm"
offers
degrees
"What
"http://databases.about.com/cs/datamining/g/dmining.htm"
powerful
practice
"Dedicated
Social
15.062
example,
courses
Answers.com"
SourceWatch"
Platform
Discovery
crucial
research
"Article
integrated
(Article)
geared
cutting-edge
mining.
mining,
iterative
creates
mining:
mining?
massive
learning,
"Oracle
probability
"http://www.cs.wisc.edu/dmi/"
(KDD),
leverage
Commercial
"http://www.statsoft.com/textbook/stdatmin.html"
"http://www.kdnuggets.com/"
molecular
Data
Success
Enterprise
Oracle
networks,
"Kurt
information,
"http://www.the-data-mine.com/"
Snooping
"http://www.answers.com/topic/data-mining"
mining
Williams
exploratory
Institute"
"STATISTICA
recognizing
CRM
encyclopedia"
meetings."
"http://www.statserv.com/datamining.html"
Solutions"
Software,
Predictive
"With
MINING.
projects,
Software:
Research"
"Pentaho
knowledge-discovery
SAS"
Home"
businesses
"Electronic
Review:
networks
knowledge
OpenCourseWare
analytic
there's
summarizing
fuel
"http://www.anderson.ucla.edu/faculty/jason.frand/teacher/technologies/palace/datamining.htm"
High-Dimensional
(trend/buzz
While
Brazma.
"CCSU
Find
"http://www.microstrategy.com/data-mining/index.asp"
marketing,
including
peer-reviewed
automatic
predictive
"http://www.ccsu.edu/datamining/"
Investing
intelligence
publications,
correlations
techniques."
techniques
Analytics
discovery
agencies
"http://www.autonlab.org/tutorials/"
article
algorithms."
"http://www.oracle.com/technology/products/bi/odm"
Statistical
Graham
Textbook:
"St@tServ
Cross-sell
mining."
simply
(KDD)
learning
industries,
Events
Nuggets"
publishing
"http://datamining.typepad.com/"
(Whitepaper)
Portal
"Newsletter
Modeling
systems
Definition
graphical
Center
"http://en.wikipedia.org/wiki/Datamining"
methodology,
page.
Wikipedia"
behavior
Open
trees,
Miner
technology
Product
visualization.
media
probability,
smarter
Stories.
how,
use."
About
Institute
Intelligence:
events,
Mine"
here!
data-mining
neural
"Includes
Have
organization
"KD
Visualization
stores
"http://www.sas.com/technologies/analytics/datamining/index.html"
Office,
Group
"http://www.spss.com/datamine"
"Outlines
Applications
easy
News
"Investor
aspects
Tutorials"
jobs,
SQL
accessible
Management
people
"Thearling.com"
Application
trends
Sloan
analytics."
decision
computer-based
integration
site-wide
School
business
"MIT
marketing
journal
assists
"Elder
perspectives
applications:
"http://www.dmoz.org/Computers/Software/Databases/Data_Mining/"
Solutions
applications,
consulting
computational
managers
Knowledge
unnoticed
"http://www.sqlserverdatamining.com/"
"http://databases.about.com/od/datamining/a/datamining.htm"
analysis,
gleaned
"http://en.wikipedia.org/wiki/Data_mining"
"Commentary
"Introduces
Techniques"
modeling."
offering
tutorials
automated
databases,
open,
organizations,
Twitter
embed
Media"
discovery,
discovery.
classic
Introduction
(DMI)
sales
"http://www.sourcewatch.org/index.php?title=Data_mining"
modeling
Mining
Introduction"
amounts
biology,
useful,
UW-Madison
"DMI:Data
Mining,
students
Mining"
menuing
inherently
Mining:
allowing
"SQL
assets
n.
Databases:
An
uncovered
"http://www.pentaho.com/products/data_mining/"
"Predictive
Computers:
testComponent
ClusteringComponentTest
"clusters
"docClustering"
MockDocumentClusteringEngine
AttributeUtils
checkCluster
"docList
hasSubclusters
testCarrotStc
testNumDescriptions
cluster"
"mine"
testWithoutSubclusters
checkEngine
"clustering
clusteringParams
"numClusters"
clusters:
testCarrotAttributePassing
testWithSubclusters
CarrotClusteringEngineTest
testCarrotLingo
"depth"
getClusteringEngine
testProduceSummary
expectedNumDocs
expectedLabelCount
"stc"
expectedSubclusterCount
"labels
expectedNumClusters
"subclusters
checkClusters
engineName
ProcessingException
addPhrases
IntRange
Bindable
"Cluster
newArrayList
createCluster
labelBase
Input
Lists
CLUSTERS
newCluster
constraint
ProcessingComponentBase
"MockClusteringAlgorithm"
Output
documentIndex
addSubclusters
disp
"fetchSize
getSentDate
"Its
AndTerm
imap
FetchProfile
InternetAddress
"message/rfc822"
logConfig
getDisposition
getRecipients
mailbox"
Multipart
"recent"
FLAG_DRAFT
getNextMail
getMessages
FLAG_FLAGGED
sun
RecipientType
FLAG_ANSWERED
adresses
Session
"multipart/*"
getCustomSearch
topLevelFolders
fetchMailsSince
"mail.imap.fetchsize"
criteria
mode..."
"messageId"
getNextBatch
getBodyPart
DRAFT
FLAG_SEEN
addEnvelopToDocument
createFilters
TO_CC_BCC
MessagingException
Flags
"Current
"Messages
"X-Mailer"
FolderIterator
getGroup
"batchSize
"mail.store.protocol"
MessageIterator
"fetch
mail
"conection
SearchTerm
HOLDS_MESSAGES
msgIter
"fetchSize"
processAttachment
getTopLevelFolders
"allTo"
"'user|password|protocol|host|folders'
getStringContent
getStringFromContext
Flag
"NO
internet
CustomFilter
FLAG_DELETED
getBoolFromContext
"flagged"
currentBatch
ifNull
getStore
folderNames
Folder
doBatching
"attachment"
ComparisonTerm
MailsSinceLastCheckFilter
folders
MimeMessage
customFilter
connected
getFolder
"mail.imap.timeout"
MailEntityProcessor
isGroup
"exclude
getFrom
messagesInCurBatch
lastMsg
"Connected
Batch
FLAGS
getDefaultFolder
"folders
getSystemFlags
CC
lastFolder
getDocumentFromMail
"user
"recurse
mailbox
outerMost
AddressException
"customFilter"
retreival
BCC
getSubject
RECENT
"xMailer"
"mail.imap.connectiontimeout"
"connectTimeout"
totalInFolder
ANSWERED
ATTACHMENT
SENT_DATE
addPartToDocument
getBaseType
Address
"pwd
getMessageCount
fetchMailSince:
hdrs
ContentType
"include
recurse
getDefaultInstance
invalidateHeaders
getMessageID
IMAPMessage
ENVELOPE
"protocol
"Custom
"draft"
fetchSize
XMAILER
getFullName
getContent
HOLDS_FOLDERS
addAddressToList
ATTACHMENT_NAMES
mailBox
"multipart/alternative"
SUBJECT
ParseUtils
FLAG_RECENT
isMimeType
hasMessages
"sentDate"
ReceivedDateTerm
getSearchTerm
"answered"
"host
"Folder
excludeFolder
getIntFromContext
Batching
SEEN
FLAGGED
folderIter
"attachmentNames"
getUserFlags
applied.
fullName
connectToMailBox
"deleted"
DELETED
tikaConfig
getHtmlHandler
OutputKeys
TikaEntityProcessor
startPrefixMapping
endPrefixMapping
getXmlContentHandler
BodyContentHandler
"html"
tikaParser
"'format'
"meta"
"tikaConfig"
"org.apache.tika.parser.AutoDetectParser"
getTextContentHandler
ContentHandlerDecorator
AUTO_PARSER
newTransformerHandler
tikaConfigFile
setOutputProperty
text|html|xml|none"
content"
contentHandler
Tika
ParseContext
Config"
TransformerHandler
SAXTransformerFactory
"format"
"2008-12-26
".*top1.*"
testConnection
"top1,top2"
"someconfig"
testRecursion
testIncludeAndExclude
"top2"
"top1/child11"
"</document>"
"<document>"
"imaps"
"top1
"top2
"<entity
messages"
".*grandchild.*"
getConfigFromMap
processor=\"org.apache.solr.handler.dataimport.MailEntityProcessor\"
testFetchTimeSince
00:00:00"
testInclude
TestMailEntityProcessor
meta=\"true\"
name=\"docTitle\"/>"
name=\"author\"/>"
column=\"text\"/>"
url=\"../../../../../extraction/src/test/resources/solr-word.pdf\"
TestTikaEntityProcessor
column=\"Author\"
testIndexingWithTikaEntityProcessor
type=\"BinFileDataSource\"/>"
"dataimport-schema-no-unique-key.xml"
</document>"
column=\"title\"
processor=\"TikaEntityProcessor\"
<document>"
"Looking
parentDataSource
dataConfigFileName
dataimport.properties"
entityFields
"solr/conf/dataimport.properties"
extraParams
dataimport.properties
BinContentStreamDataSource
BinFileDataSource
BinURLDataSource
"clob"
readFromClob
ContentStreamDataSource
"FIND_DELTA"
"FULL_DUMP"
"global"
"DELTA_DUMP"
"solrcore"
parentContext
putVal
globalSession
entitySession
ROOT_ENTITY
"onImportStart"
PROCESSOR
getStringAttribute
keyword.
"deleteQuery"
<document>
"configuration
"session"
getTxt
"dih"
nnm
FUNCTION
"dataimporter"
nanoTime
'class'
SCRIPT
(.):
"function"
period
getAllAttributes
"rootEntity"
alreadyFound
"<function>
"1.0f"
Reserved
node."
"onImportEnd"
RESERVED_WORDS
"FATAL:
"'threads'
Not
Re-loaded
"status-messages"
Import"
setIndexStartTime
schemaPk
"defaultValue"
RUNNING_FULL_DUMP
indextStartTime
NO_CONFIG_FOUND
"reload-config"
DataImporter"
Full
copyProps
name=\"enableDebug\">true</str>
verifyWithSchema
indexStartTime
JOB_FAILED
But
'<dataConfig>'
counterpart
configStr
Initialized.
running..."
sucessfully"
IDLE
Requests
Rows
doFullImport
"show-config"
DataConfig"
doDeltaImport
SolrSchema
importLock
run"
identifyPk
<str
importer.
Could
context"
started.
elems
schemaFields
DEBUG_MODE
"verbose"
Documents
DataSource:
rootEntity
Add
"toWrite"
dataSourceProps
docRootFound
below
"synchronous"
adding:
"Status"
requestStatistics
"idle"
"enableDebug"
"mode"
"Processing
getParamsMap
getSolrWriter
"dataimport"
importer
"busy"
debugEnabled
"Rows
solrconfig.xml:
dsConfig
09:53:43Z
"Requests
DataImportHandler
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/DataImportHandler.java
processConfiguration
"importResponse"
"datasource"
DataImportHandler"
"documents"
datasource:
myName
configLoc
"verbose-output"
DataImportHandler.java
"initArgs"
dataConfigFile
899579
solrconfig.xml
"Documents
ENABLE_DEBUG
cumulative
"statusMessages"
debugDocuments
errCode
fmtCache
"dateTimeFormat"
customLocale
tName
LINE
"time-taken"
popAllTransformers
dataImportHandlerException
"-----------
tCount
"document#"
transClass
"---------------------------------------------"
debugStack
"transformer:"
"EXCEPTION"
solrWriter
peekStack
DebugInfo
#{0}-------------"
"entity:"
becoming
displayName
"Stack
getTransformerName
docroot"
EPOCH
skipped
Deleted
completed.
Changed
"rowCount"
Obtained"
collection."
".delta"
entityProcessorWrapper
entity:"
"Deltas
cleanByQuery
"running
lastIndex
deletedKeys
deletedSet
processorWrapper
TIME_ELAPSED
addStatusMessage
parentCtx
Delta"
Documents"
modifiedRow
functionsNamespace
allPks
entity1
"docCount"
DeletedRowKey
deltaRemoveSet
seenDocCount
addFieldToDoc
delQuery
completed
"Committed"
executorSvc
Started"
failedDocCount
multithreaded
imports
buildDocument
"deletedDocCount"
"postImportDeleteQuery"
import"
writable"
notifyListener
"Aborted"
Elapsed"
"Optimized"
completeCleanDone
parentKeyList
deleteted
Added/Updated:
doDelta
indexerNamespace
doFullDump
context1
lastIndexTimeProps
full-import"
ModifiedRowKey()
collectDelta
started"
deltaSet
LAST_INDEX_TIME
epw
work.
Rolled
handleSpecialCommands
deletedRows
persistedProperties
myModifiedPks
pkIter
"Completed
getModifiedParentRows
"preImportDeleteQuery"
processing:
"Identifying
"$docBoost"
invokeEventListener
fullCleanDone
runAThread
ModifiedRowKey
"queryCount"
"Rolledback"
entityInitialized
INDEX_START_TIME
"skipDocCount"
"index_start_time"
"transformRow"
resolved
cacheWithWhereClause
"cacheKey"
isFirstInit
cacheVariableName
lookupKey
"cacheLookup"
'cacheLookup'
"continue"
"onError"
"'cacheKey'
"getNext()
rowIdVsRows
cachePk
invocation
Map<String,
m)method"
transformedRow
"Transformer
loadTransformers
oMap
transformRow(Map<String.Object>
"transformer
checkStopTransform
transArr
Object>>"
Transformer:
aTransArr
List<Map<String,
stopTransform
transClasses
tmpRows
Object>
"^(\\w*?)\\((.*?)\\)$"
"formatDate"
ESCAPE_SOLR_QUERY_CHARS
evaluator:
datemathfmt
evaluators
"escapeQueryChars"
"^'(.*?)'$"
1.5"
doub
"'formatDate()'
"'escapeQueryChars'
FORMAT_METHOD
"\\\\'"
"'escapeSql'
formatDate(<var>,
DATE_FORMAT_EVALUATOR
"escapeSql"
'<date_format_string>').
"'encodeUrl'
dateFmt
variableWrapper
SQL_ESCAPE_EVALUATOR
varName
URL_ENCODE_EVALUATOR
getSolrQueryEscapingEvaluator
"encodeUrl"
variableval
clob"
CLOB"
FieldStreamDataSource
File:
File0:
file0
"basePath"
biggerThan
addDetails
"biggerThan"
"newerThan"
getFolderFiles
"fileDir"
"fileName"
EXCLUDES
smallerThanStr
smallerThan
fileObj
"recursive"
newerThan
olderThan
"fileSize"
"olderThan"
biggerThanStr
"fileAbsolutePath"
"\\$\\{(.*?)\\}"
"excludes"
"'baseDir'
DIR
recursive
PLACE_HOLDER_PATTERN
"smallerThan"
"fileLastModified"
fileNamePattern
excludesPattern
"stripHTML"
STRIP_HTML
splitHTML
strReader
stripHTML
stripping
HttpDataSource
"TRANSACTION_READ_COMMITTED"
jndiName
resolveVariables
getColumnLabel
SQL:
getResultSet
readFieldNames
logError
stmt
"sdouble"
CONVERT_TYPE
resultSet
BLOB
maxRows
CONN_TIME_OUT
"JdbcDataSource
setMaxRows
getIterator
"TRANSACTION_NONE"
hasnext
TRANSACTION_NONE
CONCUR_READ_ONLY
"sfloat"
setReadOnly
"TRANSACTION_SERIALIZABLE"
colNames
TYPE_FORWARD_ONLY
"JDBC
connLastUsed
ResultSetMetaData
transactionIsolation
"slong"
convertType
TRANSACTION_READ_COMMITTED
"TRANSACTION_READ_UNCOMMITTED"
metaData
"TRANSACTION_REPEATABLE_READ"
jndi
createStatement
getMetaData
"transactionIsolation"
jndival
database"
fieldNameVsType
ResultSetIterator
CLOSE_CURSORS_AT_COMMIT
"boolean"
setTransactionIsolation
bsz
"readOnly"
TRANSACTION_READ_UNCOMMITTED
setFetchSize
getConnection():
connection"
holdability
"Creating
rSetIterator
"autoCommit"
"jndiName"
getARow
getLong
Types
"Executing
VARCHAR
TRANSACTION_REPEATABLE_READ
BIGINT
Statement
"CLOSE_CURSORS_AT_COMMIT"
SQLException
tmpConn
FETCH_SIZE
"convertType"
TRANSACTION_SERIALIZABLE
javax.sql.DataSource"
ResultSet
HOLD_CURSORS_OVER_COMMIT
driver:
closeConnection
acceptLineRegex
"Problem
"skipLineRegex"
skipLineRegex
"acceptLineRegex"
LOG_TEMPLATE
LOG_LEVEL
LogTransformer
"logTemplate"
"warn"
"logLevel"
isErrorEnabled
"trace"
getPercentInstance
"illegal
style
localeStr
"currency"
"formatStyle"
localeRegex
parsePos
"percent"
getCurrencyInstance
"^([a-z]{2})-([A-Z]{2})$"
styleSmall
parseNumber
ended
PLAIN_TEXT
"plainText"
PlainTextEntityProcessor
gNames
PATTERN_CACHE
SPLIT_BY
readfromRegExp
srcColName
otherVars
reStr
nameOfGroup
readBySplit
initEngine
scriptLang
"javax.script.ScriptEngineManager"
<script>\n"
"getEngineByName"
invokeFunctionMethod
"<script>
"invokeFunction"
"eval"
"</script>"
scriptEngineMgr
<dataConfig>"
getEngineMethod
evalMethod
scriptText
above"
delCmd
filePath
propInput
dump
deleteing:
configDir
filePrefix
"dataimport.properties"
persistFilename
propOutput
Start
".properties"
deleteCommand
rollback."
IMPORTER_PROPERTIES
commit."
"deletedPkQuery"
primaryKeys
DOT_PATTERN
".*?\\.(.*)$"
deletedPkQuery
deltaQuery
DELTA_IMPORT_QUERY
"dataimporter.delta."
"parentDeltaQuery"
DEL_PK_QUERY
primaryKey
"'deltaImportQuery'
"deltaImportQuery"
getDeltaImportQuery
PARENT_DELTA_QUERY
"deltaQuery"
deltaImportQuery
fillTokens
pcs
aparam
"(\\$\\{.*?\\})"
WORD_PATTERN
getObjectAsString
resolvable
templateVsVars
getVars
variable:
checkLimited
limitedContext
"parentContext
"arow
"connectionTimeout"
".*?charset=(.*)$"
CHARSET_PATTERN
cType
"\\w{3,}:/"
"baseUrl"
thePart
mergeAll
templateString
DOT_SPLIT
"\\."
useSolrAddXml
xml,
streamRows
XPATH_FIELD_NAME
"useSolrAddSchema"
"BREAK"
getRowIterator
placeHolderVariables
HAS_MORE
processed:"
"/add/doc/field/@name"
increased."
transFact
'forEach'
records."
xslTransformer
closeIt
commonField
caw
SimpleCharArrayReader
addCommonFields
initXpathReader
Perhaps
"/add/doc/field"
Aborting."
Transformeation"
xpathReader
"Caught
xslTransformer:
throwExp
END_MARKER
allNames
ArrayBlockingQueue
url:"
STREAM
isEnd
"$forEach"
commonVal
COMMON_FIELD
XPATH
"commonField"
records.
row:
row.
fetchNextRow
AtomicReference
readUsefulVars
lastRow
blockingQueue
NEXT_URL
xsltSource
"$nextUrl"
xml:"
"forEach"
commonFields
"/add/doc"
"readTimeOut"
splitEscapeQuote
valuesAddedinThisFrame
hasText
rootNode
quoteCount
decends
dn
xpathName
childrenFound
getMatchingNode
addField0
SUPPORT_DTD
checkForAttributes
getOrAddNode
"(\\S*?)?(\\[@)(\\S*?)(='(.*?)')?(\\])"
cleanThis
wildAncestor
putNulls
"\\|"
searchList
getDeepCopy
wildCardNodes
IS_VALIDATING
forEachPath
handleStartElement
childNodes
recordStarted
"forEach
xpseg
putText
ATTRIB_PRESENT_WITHVAL
isRecord
buildOptimise
"xpath
paths
flattenedStarts
nn
attribAndValues
searchL
'//':
childNode
andAnswer
getInitialContext
anyObject
getCurrentArguments
spi
InitialContextFactory
IAnswer
withWhereClause
withoutWhereClauseWithTransformers
withKeyAndLookup
withoutWhereClause
withoutWhereClauseWithMultiRowTransformer
TestCachedSqlEntityProcessor
DoubleTransformer
UppercaseTransformer
doWhereTest
xNamespace
csep
"id=x.id"
"x.id"
id=${x.id}"
InvocationHandler
"dsc"
proxy
newProxyInstance
Proxy
TestClobTransformer
"</b>\n"
"dataconfig-contentstream.xml"
"data-config.xml"
"inst"
qparams
C1"
TestContentStreamDataSource
"<b>\n"
"contentstream-solrconfig.xml"
testDataConfigWithDataSource
"\t\t\tquery=\"select
"data-config-with-datasource.xml"
acode
TestDataConfig
name=\"atrimlisting\"
name=\"autos\"
'${indexer.last_index_time}'\">\n"
"\t\t\tdeltaQuery=\"select
"\t<document
atrimlisting\"\n"
acode,make,model,year,msrp,category,image,izmo_image_url,price_range_low,price_range_high,invoice_range_low,invoice_range_high
"atrimlisting"
pk=\"acode\"\n"
"dataimport-nodatasource-solrconfig.xml"
hh:mm:ss.SSS"
testTransformRow_SingleRow
now2
now1
TestDateFormatTransformer
testTransformRow_MultipleRows
"MM/dd/yyyy
"dateAdded"
singleEntityMultipleRows
name=\"X\"
name=\"desc_s\"
singleEntityOneRow
"RegexTransformer"
dc_deltaConfig
"desc_s"
testDeltaImportNoRows_MustNotCommit
singleEntityNoRows
dc_singleEntity
column=\"desc\"/>\n"
column=\"id\"/>\n"
testImportCommand
"mockDs"
MockTransformer
testContext
dataConfigWithTwoEntities
column=\"mypk\"
column=\"ID\"
testDynamicFields
EndEventListener
newerThan=\"${dih.last_index_time}\"
testDeleteDocs
template=\"xyz\"
MockDataSource2
"desc:one"
testStopTransform
type=\"MockDataSource\"
testSingleEntity_CaseInsensitive
books
TestDocBuilder2
testSkipDoc
onImportEnd=\"TestDocBuilder2$EndEventListener\">\n"
testSkipRow
category='${dataimporter.request.category}'\">\n"
x\">"
name=\"${dih.request.mypk}\"
testRequestParamsAsVariable
dataConfigWithDynamicTransformer
transformer=\"TestDocBuilder2$AddDynamicFieldTransformer\">\n"
processor=\"FileListEntityProcessor\"
column=\"text\"
"name_s"
"ApacheSolr"
category='search'"
name=\"books\"
"dynamic_s"
"name_s:xyz"
"mypk"
"Context
dataConfigForSkipTransform
name=\"${dih.request.text}\"
name=\"authors\"
AddDynamicFieldTransformer
StartEventListener
"dynamic_s:test"
"desC"
testSingleEntity
dataConfigFileList
testFileListEntityProcessor_lastIndexTime
testRequestParamsAsFieldName
"<dataConfig>
"xyz"
"single-entity-data-config.xml"
template=\"${x.file}\"
requestParamAsVariable
dataConfigWithTemplatizedFieldNames
x\""
query=\"${books.id}\">"
"desc:ApacheSolr"
column=\"Desc\"
"name_s:abcd"
"data-config-with-transformer.xml"
"\t\t\t\tbaseDir=\"${dih.request.baseDir}\"
"Start
onImportStart=\"TestDocBuilder2$StartEventListener\"
column=\"name_s\"
dataConfigWithCaseInsensitiveFields
transformer=\"TemplateTransformer\">\n"
"\t\t\t\tfileName=\".*\"
"NA"
multiTransformer
"T1"
"T2
T1
TestEntityProcessorBase
"T2"
"T1
type=\"MockDataSource\"/>"
transformer=\"TestErrorHandling$ExceptionTransformer\"
<id>3</id>\n"
dataConfigWithoutStreaming
name=\"node\"
name=\"child\"
testMalformedNonStreamingXml
forEach=\"/root/node\"
TestErrorHandling
xpath=\"/root/node/id\"
dataSource=\"str\"
testAbortOnError
url=\"test\"
StringDataSource
onError=\"abort\">\n"
stream=\"true\"
onError=\"continue\">\n"
forEach=\"/root/node\">\n"
type=\"TestErrorHandling$StringDataSource\"
name=\"str\"
xpath=\"/root/node/desc\"
testTransformerErrorContinue
dataConfigAbortOnError
ExceptionTransformer
testMalformedStreamingXml
dataConfigWithStreaming
onError=\"skip\">\n"
wellformedXml
dataConfigWithTransformer
"exception-transformer"
"foo\\\\"
"c\\:t"
"\"\"Albert
D'souza\""
'ds,o,u\'za',"
'hello!',
testGetDateFormatEvaluator
D''souza\"\""
"foo\\"
sqlEscaper
testGetSqlEscapingEvaluator
"'foo\""
"A.key,
"A.key"
sqlTests
"foo''''"
testEscapeSolrQueryFunction
"foo''"
'yyyy-MM-dd
HH:mm'"
"c:t"
dateFormatEval
"foo\"\""
urlEvaluator
"foo\""
testGetUrlEvaluator
"review:\"hybrid
"foo'"
urlTests
"ds,o,u'za"
sedan\""
"\"Albert
200]"
"${dataimporter.functions.escapeQueryChars(e.query)}"
TestEvaluatorBag
"''foo\"\""
runTests
a.b,
"'NOW-2DAYS','yyyy-MM-dd
column=\"y\"
dataSource=\"f\"
dataField=\"a.xml\">\n"
TestFieldReader
a\"
xpath=\"/x/y\"/>\n"
name=\"b\"
"</x>"
"<x>\n"
<y>Hello</y>\n"
type=\"FieldReaderDataSource\"
forEach=\"/x\"
name=\"f\"/>\n"
name=\"a\"
"'NOW'"
childdir
"${a.x}"
t.xml"
"/child"
"xml$"
changeModifiedTime
smallestFile
minLength
fileListEntityProcessor
"^.*\\.xml$"
getFiles
testBiggerSmallerFiles
"abcdefgij"
".xml$"
testRECURSION
testNTOT
"'NOW-2HOURS'"
"4r3d"
"trim_id"
"com.mysql.jdbc.Driver"
"jdbc:fakedb"
retrieveFromJndiWithCredentials
SetUp
sysProp
"Fred"
jdbcDataSource
make,model,year,msrp,trim_id
notNull
"java.naming.factory.initial"
retrieveFromJndi
"connection"
trim_id
registerDriver
msrp
retrieveFromDriverManager
"msrp"
"jdbc:mysql://localhost/autos"
"java:comp/env/jdbc/JndiDB"
make='Acura'"
TestJdbcDataSource
/Volumes/spare/ts/config/jm.xsd\n"
deleted.\n"
10:53
gn
tacked
112700
103419
11976
97764
"DELETE
990
/Volumes/spare/ts/config/xlink.xsd\n"
1498
"\\.xml"
/Volumes/spare/ts\n"
/Volumes/spare/ts/acm19/data/00001292old.xml\n"
"'\n"
looks
/Volumes/spare/ts/config/sml-delivery-norm-2.1.dtd\n"
"412624
"412622
"412623
"412621
only_xml_files
/Volumes/spare/ts/config/sml-delivery-2.1.xsd\n"
only_xml_files_no_xsd
rw
1155
/Volumes/spare/ts/acm19/data/00000510.xml\n"
"412589
442
14124
/Volumes/spare/ts/config/s-deliver.xsl\n"
1938
12847
/Volumes/spare/ts/config/dcterms.xsd\n"
10:18
Apr
8894
-rwxr-xr-x
-ls;
/Volumes/spare/ts/config/video.gif\n"
/Volumes/spare/ts/config/s-deliver.css\n"
3156
/Volumes/spare/ts/config/sml-delivery-norm-2.0.dtd\n"
rawLine='"
"\\.xsd"
LANG
"dummy.lis"
stamp
"412590
"412591
/Volumes/spare/ts/config/dc.xsd\n"
'find
125296
like,
/Volumes/spare/ts/acm19/data\n"
"412584
"412577
no_xsd_files
filecontents
36256
/Volumes/spare/ts/acm19/data/00001292.xml\n"
/Volumes/spare/ts/config\n"
11:09
/Volumes/spare/ts/config/xml.xsd\n"
"412588
"412587
"412586
"412585
"412583
"412582
setting\n"
drwxr-xr-x
depending
/Volumes/spare/ts/acm19\n"
TestLineEntityProcessor
athough
varies
"412594
11:10
8318
"412592
"412593
/Volumes/spare/ts/acm19/data/00000603.xml\n"
format\n"
getDecimalSeparator
"245"
testTransformRow_InvalidInput3_Currency
"789"
"5a67"
"678"
"outputs"
"567b"
testTransformRow_InvalidInput2_Number
DECIMAL_SEP
"567"
"inputs"
getGroupingSeparator
testTransformRow_InvalidInput3_Number
245678
GERMANY
testTransformRow_InvalidInput1_Number
123567
testTransformRow_InvalidInput1_Percent
"de-DE"
testTransformRow_InvalidInput2_Currency
"localizedNum"
GROUPING_SEP
"num"
testTransformRow_MultipleNumbers
testTransformRow_MalformedInput_Number
outputRow
testTransformRow_SingleNumber
TestNumberFormatTransformer
GERMAN_GROUPING_SEP
123789
query=\"x\">\n"
type=\"TestPlainTextEntityProcessor$DS\"
"\t<dataSource
DS
column=\"plainText\"
TestPlainTextEntityProcessor
DATA_CONFIG
processor=\"PlainTextEntityProcessor\"
"Warranty:(.*)"
"(\\w*)
Hwy,\\s*?\\d*?\\s*?mpg
"D''souza"
"range"
"Fuel
Hwy,
"D'souza"
"(.*)"
Mangar"
***
Range:
".*(${e.city_mileage})"
range:
Economy
"rowdata"
"(Range)"
"col1"
Paul"
",firstName,lastName"
commaSeparated
capacity:(.*)"
(\\w*)"
eAttrs
"a,bb,cc,d"
"participant"
"Seating
"city_mileage"
$1
Range:\\s*?(\\d*?)\\s*?mpg
"Paul"
"Noble"
testMultiValuedRegex
"26"
Hwy,\\s*?(\\d*?)\\s*?mpg
mpg
"person"
"hltCityMPG"
"seating_capacity"
mileage
Noble
"warranty"
Range:\\s*?\\d*?\\s*?mpg
"t4,t5"
"60"
(\\w*)
"fullName"
"duff"
"checkNextToken"
!=''
processor=\"XPathEntityProcessor\"\n"
/mbmessage/articles/article\"
f1(row){"
Scott"
f1(row,context){"
funcName
"Scott"
"function
row.put('$hasMore',
"<script><![CDATA[\n"
"nextToken"
'true');}\n"
oneparam
"\t\t\t\txpath=\"/mbmessage/articles/navigation/nextToken\"
name=\"mbx\"
checkNextToken(row)\t{\n"
checkScript
"}]]></script>\t<document>\n"
"\t\t\tforEach=\"/mbmessage/articles/navigation
nt
row.get('name'));"
row;\n"
pk=\"articleNumber\"
column=\"nextToken\"\n"
row.get('nextToken');"
){
"\t\t\turl=\"?boardId=${dataimporter.defaults.boardId}&amp;maxRecords=20&amp;includeBody=true&amp;startDate=${dataimporter.defaults.startDate}&amp;guid=:autosearch001&amp;reqId=1&amp;transactionId=stringfortracing&amp;listPos=${mbx.nextToken}\"\n"
(nt
TestScriptTransformer
transformer=\"script:checkNextToken\">\n"
readScriptTag
"row.put('name','Hello
tranformer
getDs
singleBatch
ea
tranformerList
"SELECT
TestSqlEntityProcessor
tranformerWithReflection
T"
name=\"checkDateFormat\"
id=${dataimporter.delta.id}\"
TestSqlEntityProcessor2
class=\"org.apache.solr.handler.dataimport.TestSqlEntityProcessor2$DateFormatValidatingEvaluator\"/>\n"
"id:10"
id=5"
"id:11"
'15'"
"id:17"
dataConfig_deltaimportquery
dataConfig_2threads
'17'"
y.A=5"
testCompositePk_FullImportNoCommit
testLastIndexTime
${dih.functions.checkDateFormat(dih.last_index_time)}\"
testCompositePk_DeltaImport_DeletedPkQuery
testCompositePk_DeltaImport_DeltaImportQuery
'5'"
testCompositePk_FullImport_MT
y.A=10"
DateFormatValidatingEvaluator
pk=\"id\"
"id:15"
dataConfig_LastIndexTime
deltaDeleteRow
threads=\"2\">\n"
testCompositePk_DeltaImport
y.A=11"
"\t<function
dataConfig_delta
"id:1
TestSqlEntityProcessorDelta
name=\"id\"/>\n"
TestSqlEntityProcessorDelta2
template=\"prefix-${x.id}\"
"solr_id:prefix-1"
dataConfig_delta2
column=\"tmpid\"
"dataimport-solr_id-schema.xml"
"solr_id:prefix-1
"solr_id:prefix-2"
name=\"solr_id\"/>\n"
1199429363730"
TestTemplateString
"indexer"
${indexer.last_index_time}"
1199429363730l
EMPTY_PROPS
${e.firstName}
"Shekhar"
"mrname"
"Mangar,
Shekhar"
testTransformRow
${e.middleName}"
"emails"
"c@d.com"
"middleName"
"mail"
TestTemplateTransformer
"${e.mail}"
mails
${e.name}"
Mangar,
"${e.lastName},
"a@b.com"
y.A=3"
threads=\"2\"
TestThreaded
y.A=4"
TestURLDataSource
"baseurl"
"http://example.com/"
variableResolver
"dataimporter.request"
"${dataimporter.request.baseurl}"
substitutionsOnBaseUrl
dateNamespaceWithValue
"hello.my.new"
testFunctionNamespace1
"hello.my.new.world1"
dateNamespaceWithExpr
"val
testDefaultNamespace1
HH:mm')}"
test3LevelNestedNamespace
"hello.my"
"hello.my.world1"
testSimpleNamespace
TestVariableResolver
"world1"
testDefaultNamespace
testNestedNamespace
World"
"${dataimporter.functions.formatDate(A.dt,'yyyy-MM-dd
"WORLD"
"${dataimporter.functions.formatDate('NOW/DAY','yyyy-MM-dd
"hello.world"
HH:mm:ss')}"
"WORLD1"
"s.gP()"
"${dataimporter.functions.test('TEST')}"
"\t\t<price>10.90</price>\n"
"/root"
heart</title>\n"
"\t\t<year>1985</year>\n"
"/catalog/cd"
"\t\t<company>CBS
streamStopsAfterInterrupt
name=\"price\"><xsl:value-of
"\t\t<title>Greatest
monitor
"1982"
withFieldsAndXpathStreamContinuesOnTimeout
name=\"company\"><xsl:value-of
method='xml'
encoding='UTF-8'
"1B2"
version=\"1.0\"\n"
"\t<cd>\n"
cdData
name=\"year\"><xsl:value-of
</xsl:for-each>\n"
"\t</cd>\n"
Hits</title>\n"
simulateSlowResultProcessor
</doc>\n"
select=\"artist\"/></field>\n"
"\t\t<country>UK</country>\n"
xsl
"cd.xml"
"<xsl:template
Dylan</artist>\n"
"\t\t<country>USA</country>\n"
Records</company>\n"
withFieldsAndXpathStream
indent='yes'/>\n"
"/catalog/cd/year"
"x.xsl"
xPathEntityProcessor
select=\"company\"/></field>
name=\"country\"><xsl:value-of
"<xsl:stylesheet
streamWritesMessageAfterBlockedAttempt
testXml
"/catalog/cd/title"
"\t\t<company>Columbia</company>\n"
testMultiValuedFlatten
Burlesque"
encoding=\"UTF-8\"?><root><a>1</a><a>2</a></root>"
simulateSlowReader
Parton</artist>\n"
match=\"/\">\n"
"\t\t<year>1988</year>\n"
"<catalog>\n"
testMultiValued
"\t\t<title>Hide
href=\"solr.xsl\"?>\n"
</add>
"\t\t<title>Empire
"\t\t<artist>Bonnie
select=\"title\"/></field>\n"
name=\"title\"><xsl:value-of
name=\"artist\"><xsl:value-of
Burlesque</title>\n"
MICROSECONDS
"\t\t<artist>Dolly
select=\"year\"/></field>
withDefaultSolrAndXsl
TestXPathEntityProcessor
"\t\t<price>9.90</price>\n"
select=\"catalog/cd\">\n"
"</xsl:template>\n"
<doc>\n"
"</catalog>\t"
withFieldsAndXpath
"testdata.xml"
"\t\t<year>1982</year>\n"
"\t\t<company>RCA</company>\n"
"</xsl:stylesheet>"
<xsl:for-each
"/catalog/cd/artist"
"<xsl:output
"Empire
select=\"price\"/></field>\n"
select=\"country\"/></field>\n"
"xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n"
Tyler"
testXmlFlatten
Tyler</artist>\n"
rowsToRead
"\t\t<artist>Bob
encoding=\"UTF-8\"?><root><a>1<b>B</b>2</a></root>"
'//'."
'/'."
a=\"x5\"
<b>\n"
"<root>\n\t<a>\n
invalid."
forEach
<p>Access
<b><x>x3</x></b>\n"
name\"/>\n"
"/root/a
this!</i>
"C.2.1"
<antetitulo><i>
"/root/a/b[@k]/y"
underlined!"
forEach\n"
"/anycd"
bold
<y>y4</y>\n"
"10097"
a=\"x0\"
"/root/a/b/@a"
xmlns:xhtml=\"http://xhtml.com/\"
"<root><b><a
"Within
"<merchantProduct
My
<c>C.1.2</c>\n"
<antetitulo></antetitulo>\n"
attribValWithSlash
<a>\n
"<anycd>\n"
"/root/node/id"
a=\"x4\"
"C.1.2"
"/root/x"
name=\"item
</a>\n"
attrInRoot
"/root/a/b/x"
<y>y1</y>\n"
"</anyd>"
mixedContentFlattened
forEach"
"y0"
"/root/a/b"
"antler"
</a>"
<boo>Within
text</texto>\n"
"/root/b/c"
<b>B.1.2</b>\n"
elems2LevelWithAttribMultiple
x=\"a/b\"
<xhtml:u>underlined</xhtml:u>!\n"
"/root/i/x/a"
"/p/b"
a=\"x2\"
"underlined"
"cont"
"/anycd/contenido//boo"
</cat>\n"
"/anyd/contenido/titulo"
<x>x3</x>\n"
"/root/contenido/texto"
its"
"cond-0"
m=\"p\">\n"
a=\"x6\"
<x>x0</x>\n"
of</boo>My
"/root/b/@b"
<a>A.1.1</a>\n"
<cat>\n"
"/anyd/contenido"
<x>x3</x>\n\t
"/root/contenido/resumen"
mid=\"189973\">\n"
b=\"y6\"
</texto>\n"
"B.1.2"
<in_stock
level</boo>
"<anyd>\n"
"\t<a>\n\t
b=\"y2\"
b=\"y4\"
"/root/b/a[@x='a/b']/@h"
</contenido>\n"
attributes2LevelHetero
<texto>
"/root/b/@a"
"/root/contenido/titulo"
"</b></root>"
idioma=\"cat\">\n"
"\t</a>\n"
"<a>\n
"/anyd/contenido/resumen"
"descdend"
TestXPathRecordReader
<b><y>y4</y></b>\n"
"/r/merchantProduct/condition/@type"
"/root/i/x/c"
h=\"hello-A\"/>
<boo>this
b=\"y0\"
"//boo"
"x0"
"<r>\n"
attributes2Level
"catName"
"</xhtml:p>"
"/root/b"
putNullTest
<i>flattened
type=\"cond-1\"
"B.1.1"
"\t
"189973"
<a>A.2.1</a>\n"
"/root/a/b[@k][@m='n']/x"
id=\"814636052\"
<boo>title</boo></i>
"top
mid=\"189974\">\n"
"/root/contenido/@id"
<name>hello</name>\n"
</i>\n"
k=\"x\"
"/root/contenido"
"/root/a/b[@k='x']/x"
<price>301.46</price>\n"
sameForEachAndXpath
>This
<i>skip
</x>"
testError
well</boo></p>\n"
"301.46"
"</r>"
clauses</i>
<b>\n\t
<x>\n"
<boo>antler</boo></i></antetitulo>\n"
<a>\n\t
"814636052"
/root/x"
"B.2.2"
id=\"10097\"
"/root/a/b[@k='x']/y"
"<x>\n
elems2LevelWithAttribVal
its</boo>
"/anyd/contenido/texto"
"/r/merchantProduct/@id"
<i>sub
"A.1.1"
"sub
mixedContent
</titulo>\n"
level"
b=\"y1\"
"resume"
k=\"y\">\n"
"/root/cat/name"
k=\"x\">\n"
type=\"stock-5\"
"/p/u"
"/root/i/x/b"
"</anycd>"
"/r/merchantProduct/@mid"
</merchantProduct>\n"
<b
elems2LevelMissing
"/anycd//boo"
"y2"
unsupported_Xpaths
</b></root>"
m=\"n\">\n"
<x>x1</x>\n"
element</boo></status>\n"
id=\"814636051\"
"x1"
<c>C.2.2</c>\n"
"C.2.2"
"\t<a>\n"
<xhtml:b>bold</xhtml:b>
attributes2LevelMissingAttrVal
"/root/a/b[@k][@m='n']/y"
"/root/a/b[@k]/x"
//b
"814636051"
<y>y0</y>\n"
k=\"y\"
"/root/i"
"inr_descd"
<i>\n"
a=\"x1\"
"<root>\n\t<a>\n\t
type=\"stock-4\"
"/root/x/b/@a"
"302.46"
"hello-A"
<contenido
"//b"
"/root/node/desc"
"/r/merchantProduct/price"
<status>as
"/anyd/contenido/@id"
"/root/a/b/y"
"/anyd/status"
m=\"n\"
<resumen>
<c>C.2.1</c>\n"
"A.2.1"
<b><c>Hello
<item
any_decendent_of_a_child1
any_decendent_of_a_child2
a=\"x3\"
"x2"
"y1"
"189974"
"<xhtml:p
"conditionType"
<boo>inner
"mid"
"/root/a/b/@b"
<boo>not
<b>B.1.1</b>\n"
"/root/node"
<boo>top
any_decendent_from_root
"/r/merchantProduct"
<titulo>
<condition
<price>302.46</price>\n"
"cond-1"
elems2LevelWithAttrib
</resumen>\n"
flattened
"<root><b>\n"
</b>\n"
<b>B.2.2</b>\n"
type=\"cond-0\"
</x>\n"
b=\"y5\"
"/p"
"//boo/i"
"/root/x/b/@b"
<y>y4</y>\n\t
streamType
autoDetectParser
parsingHandler
XML_FORMAT
endDocument
"XML"
"ExtractingDocumentLoader:
PARSER
parsers.
extractOnly
"xhtml"
setOutputFormat
serialize
extractFormat
TextSerializer
supply
MatchingContentHandler
"_metadata"
xpathExpr
OutputFormat
metadataNL
didn't
XMLSerializer
setOutputCharStream
parameter."
"Text"
BaseMarkupSerializer
XPathParser
serializer
"stream_name"
"stream_source_info"
"stream_content_type"
"stream_size"
"fmap."
"resource.name"
"literal."
"extractOnly"
"stream.type"
"boost."
"extractFormat"
Rich
"tika.config"
configDateFormats
Format:
tikaConfigLoc
"date.formats"
CONFIG_LOCATION
createFactory
theBldr
catchAllBuilder
unknownFieldPrefix
schFld
contentFieldName
fieldBuilders
qName
transformValue
lowerNames
bldrStack
findMappedName
captureFields
captureAttribs
"extractionLiteralMV:two"
"extractionLiteral:one"
"fmap.Author"
"rsp
"fmap.language"
"<?xml"
"+id:simple4
"+id:simple2
"+id:simple3
"extractedCreator"
"solr-word.pdf"
"title:Welcome"
"fmap.content"
"fmap.creator"
"extractedLanguage"
"solr-word"
"t_"
"simple2"
"defaultExtr:http\\://www.apache.org"
testPlainTextSpecifyingMimeType
"linkNews"
"fmap.Last-Modified"
+t_abcxyz:[*
"literal.extractionLiteralMV"
"extractedProducer"
testExtractOnly
"simple4"
"t_href"
"100.0"
"//doc[1]/str[.='simple3']"
"t_href:http"
testLiterals
"abcxyz"
"extractedAuthor"
ExtractingRequestHandlerTest
"defaultExtr"
"id:simple2"
"extraction
"version_control.xml"
"fmap.a"
testExtraction
"boost.t_href"
+t_content_type:[*
+t_p:\"here
"solr-word.pdf_metadata"
testPlainTextSpecifyingResourceName
"text/plain"
"arabic.pdf"
"stream_name:version_control.xml"
testDefaultField
testXPath
"fmap.Keywords"
"fmap.producer"
"extractedDate"
"simple.html"
"version_control.txt"
+t_content:Solr"
"fmap.created"
"extractedKeywords"
"/xhtml:html/xhtml:body/xhtml:a/descendant:node()"
"example.html"
"literal.extractionLiteral"
testArabicPDF
"extractedContent"
"extractionLiteralMV:one"
"nl
+t_href:[*
"title:solr-word"
"simple3"
"extractedContent:Apache"
"fmap.content_type"
"wdf_nocase:السلم"
page_count
doc_slice
Page
getResults_found
results_per_page
results_found
getPage_count
getCurrent_page_number
doc_list
getResults_per_page
current_page_number
Starting
templates
"v.template."
VelocityEngine
SortTool
Template
EscapeTool
"list"
"v.properties"
"\\\\\""
"v.contentType"
ListTool
getTemplate
layout_template
"math"
"v.json"
wrap_response
xmlResult
"\\\\n"
"solr.resource.loader.instance"
"params.resource.loader.instance"
"{\"result\":\""
"velocity"
NumberTool
MathTool
app
getJSONWrap
replace1
json_wrapper
"\\\\r"
"v.base_dir"
ComparisonDateTool
getEngine
replaced
VelocityContext
FILE_RESOURCE_LOADER_PATH
"v.layout"
parsedResponse
RESOURCE_LOADER
template_root
stringWriter
"params,file,solr"
generic
"\"}"
"$response.response.response_data"
"v.template.custom"
VelocityResponseWriterTest
testTemplateName
vrw
"response_data"
"SolrDocument["
",start="
",docs="
"{numFound="
403
401
UNAUTHORIZED
getErrorCode
UNKNOWN
_documentBoost
"SolrInputDocument["
documentBoost
")={"
nxt
"Store
abbreviation
Position
"TermVector
"Tokenized"
TermVector"
"Indexed"
"Binary"
Norms"
Last"
"Omit
First"
"Multivalued"
Tf"
display
Missing
Stored"
"Stored"
abbrev
".fieldvalue"
".fieldname"
".fieldtype"
".showmatch"
"{main("
"),extra("
"stream.body"
"stream.url"
"stream.contentType"
"timeAllowed"
"tag"
"EXPLICIT"
"omitHeader"
"NONE"
"),defaults("
"{params("
"ps"
"pf"
GEN
".other"
".sort"
".mincount"
".missing"
FACET_METHOD_fc
".prefix"
".method"
".zeros"
".enum.cache.minDf"
".end"
".gap"
".hardend"
".offset"
'other'
".date"
".limit"
".fragmentsBuilder"
".fragListBuilder"
".requireFieldMatch"
".snippets"
".pre"
".usePhraseHighlighter"
".fl"
".formatter"
".highlightMultiTerm"
".fragsize"
".maxAlternateFieldLength"
".pattern"
".increment"
".alternateField"
".fragmenter"
".post"
".mergeContiguous"
".slop"
".maxAnalyzedChars"
".useFastVectorHighlighter"
"LIST"
LIST
"maxqt"
"minwl"
"DETAILS"
"maxwl"
"mintf"
"mlt."
"match.offset"
"match.include"
"maxntp"
"mindf"
default:
"{required("
"isShard"
"shards.start"
"shards.rows"
toMultiMap
getFieldDouble
SPELLCHECK_PREFIX
"spellcheck."
"collate"
".facet"
".flag"
"limit"
"mincount"
"upper.incl"
"lower.incl"
TERMS_PREFIX
"maxcount"
"tf_idf"
"tv."
TV_PREFIX
"docIds"
"maxSegments"
"String
byte1
byte0
four."
aLen
outCursor
inCursor
"=="
intToAlpha
numBytesInPartialGroup
numGroups
numFullGroups
ch3
alphaToInt
base64ToInt
missingBytesInLastGroup
resultLen
intToBase64
byte2
base64toInt
"key:
numKept
"ConcurrentLRUCache
newestEntry
eset
isCleaning
destroyed
runNewThreadForCleanup
lowerWatermark
runCleanupThread
"upperWaterMark
newNewestEntry
PQueue
CleanupThread
getCumulativeNonLivePuts
acceptableWatermark
lastAccessed:"
evictEntry
newThreadForCleanup
oldCacheEntry
thisEntry
getCumulativeMisses
numPasses
EvictionListener
markAndSweepLock
timeCurrent
islive
acceptableWaterMark
otherEntry
lastAccessed
evictedEntry
evictionCounter
evictionListener
wakeThread
accessCounter
wantToRemove
oldestEntry
newOldestEntry
cleanupThread
isDestroyed
numRemoved
myInsertWithOverflow
myMaxSize
lastAccessedCopy
upperWaterMark"
wantToKeep
missCounter
putCounter
eSize
"lowerWaterMark
nonLivePutCounter
setLastAccessed
markAndSweep
DEFAULT_CHARSET
getCharsetFromContentType
setSourceInfo
"charset="
DEFAULT_TWO_DIGIT_YEAR_START
"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
applyPattern
DEFAULT_HTTP_CLIENT_PATTERNS
"EEEE,
hh:mm:ss"
dateValue
PATTERN_ASCTIME
dd-MMM-yy
hh:mm:ss
formatIter
"yyyy-MM-dd"
JANUARY
PATTERN_RFC1123
dateParser
set2DigitYearStart
"dateValue
PATTERN_RFC1036
zzz"
HH:mm:ss
DOCUMENT_TYPE_NODE
colon_index
hasChildNodes
DOCUMENT_NODE
ENTITY_NODE
nfe
getNamedItem
NOTATION_NODE
childNodesToList
nodesToNamedList
endName
ATTRIBUTE_NODE
substituteSystemProperties
parsePropertyString
nodesToList
propertyRefs
setNodeValue
substituteProperty
missing_err
DOCUMENT_FRAGMENT_NODE
clst
readUnsignedByte
DataInput
readBoolean
skipBytes
readUnsignedShort
writeBoolean
DataOutput
writeChar
BUFSIZE
initval
"fallthrough"
mixed
0xdeadbeef
recursiveHasNext
Iterators
IteratorChain,
iterators
hasNext()"
itit
0xff00000000000000L
SOLRDOC
readExternString
writeIterator
readSmallInt
readByteArray
ITERATOR
readOrderedMap
readSolrDocument
retVal
EXTERN_STRING
BYTEARR
BOOL_TRUE
readMap
writeExternString
readSolrDocumentList
readSmallLong
'javabin'
NAMED_LST
writeByteArray
BOOL_FALSE
SLONG
writeKnownType
charArr
solrDocs
readStr
tagByte
SINT
writePrimitive
TAG_AND_LEN
stringsMap
stringsCount
stringsList
ORDERED_MAP
nameValueMapToList
_value
_key
nv
"regex:"
STOPPED
PAUSED
rt
STARTED
resume
"sub1"
"sub1.1"
culmTime
"\\\\(?=,)"
"(?<!\\\\),"
"%3D"
"%2B"
"%25"
toLower
"#29;"
"#30;"
"#14;"
"#16;"
"#21;"
"#28;"
"#31;"
"#23;"
"#26;"
"#12;"
"#25;"
"#27;"
"#15;"
"#11;"
"#20;"
"#17;"
"#24;"
"#22;"
"#2;"
"#3;"
"#18;"
"#6;"
"#5;"
"#7;"
chardata_escapes
"#1;"
attribute_escapes
"#0;"
"#4;"
escapes
"#8;"
"#19;"
Factory
inQueue
outQueue
writeToken
fillCount
ignoreStr
lastWordStart
"forceFirstLetter"
okPrefix
wordCount
OK_PREFIX
"keep"
"minWordLength"
"onlyFirstWord"
maxWordCount
"keepIgnoreCase"
DEFAULT_MAX_WORD_COUNT
CapitalizationFilter
"maxTokenLength"
"maxWordCount"
setStrength
variant,
"tertiary"
TERTIARY
setDecomposition
tailor
decomposition:
"variant"
required."
createFromLocale
built-in
PRIMARY
language,
IDENTICAL
decomposition
CANONICAL_DECOMPOSITION
SECONDARY
ruleset
NO_DECOMPOSITION
javadocs
"Then
custom.
FULL_DECOMPOSITION
strength
RuleBasedCollator.
createFromRules
customized
strength:
commonSet
gramToken
saveTermBuffer
lastStartOffset
lastWasCommon
isCommon
isGramType
previousType
commonGramsQuery
"Delimiter
encoderClass
"identity"
"delimiter"
"minWordSize"
"minSubwordSize"
"maxSubwordSize"
dictionary"
"onlyLongestMatch"
doubleMetaphone
firstAlternativeIncrement
setMaxCodeLen
primaryPhoneticValue
alternatePhoneticValue
TOKEN_TYPE
remainingTokens
MAX_CODE_LENGTH
DEFAULT_MAX_CODE_LENGTH
ElisionFilterFactory"
articlesFile
"uuml"
lastNumRead
"sup3"
"mu"
"notin"
"ccedil"
"reg"
"hArr"
"zwj"
"uarr"
"upsih"
"radic"
"spades"
"equiv"
"nbsp"
"Egrave"
"scaron"
"Ograve"
"atilde"
"psi"
"Dagger"
"lsquo"
"iacute"
"rlm"
readNumericEntity
"oacute"
"kappa"
numEaten
MISMATCH
checkEscaped
"Prime"
"ang"
"nabla"
"Pi"
"bull"
"szlig"
"middot"
"Omicron"
"frac34"
"Xi"
"Uuml"
"lambda"
"Delta"
"piv"
escapedTags
"Agrave"
"Tau"
"ldquo"
"Kappa"
"prop"
"harr"
"omicron"
pushed
"real"
"rArr"
"Alpha"
"phi"
safeReadAheadLimit
"minus"
"frasl"
"Icirc"
"eth"
"oplus"
"fnof"
"Gamma"
"Iuml"
"Zeta"
"epsilon"
"supe"
readBang
"thinsp"
"acirc"
"micro"
"zwnj"
"ETH"
"Aring"
"para"
"Mu"
"Oslash"
"Otilde"
"ograve"
"weierp"
quoteChar
"Iota"
"sim"
"brvbar"
readAttr2
"larr"
"diams"
"style"
"amp"
"frac12"
"isin"
"ucirc"
isUnicodeIdentifierStart
"ordf"
"zeta"
"loz"
"permil"
"Theta"
"cedil"
"iquest"
"ntilde"
"ocirc"
"rang"
isHex
"cong"
"crarr"
"Ocirc"
"thetasym"
"igrave"
inScript
"infin"
"Sigma"
"lowast"
numReturned
"tau"
"Iacute"
findEndTag
"egrave"
"Eacute"
"icirc"
"sube"
MATCH
"Omega"
"frac14"
"rarr"
"aelig"
"Rho"
readEntity
"Eta"
"Yacute"
"oelig"
"hearts"
"Uacute"
"nsub"
"rceil"
"eacute"
"ouml"
"rsaquo"
"lsaquo"
"THORN"
"trade"
"ensp"
"iexcl"
"perp"
"Lambda"
numWhitespace
entityTable
eaten
"lArr"
"Oacute"
"Auml"
"yacute"
"upsilon"
"acute"
"lfloor"
isSpace
"alefsym"
"theta"
"euro"
"Ucirc"
getReadAheadLimit
"Epsilon"
"bdquo"
"sdot"
"sup1"
"lt"
"oslash"
"agrave"
"lang"
"hellip"
"Nu"
"Ccedil"
"OElig"
"rho"
"prod"
"dArr"
"Ugrave"
"sect"
"quot"
"rsquo"
"Atilde"
"lceil"
"ordm"
"cup"
"image"
"eta"
"euml"
"Aacute"
"Ntilde"
isIdChar
"Upsilon"
"laquo"
"gamma"
"otilde"
"AElig"
"Acirc"
"sup2"
"Chi"
"oline"
"dagger"
"uArr"
"beta"
"yuml"
"Beta"
lastMark
"mdash"
"iuml"
"rdquo"
entityChar
readName
"ndash"
"Ouml"
"Ecirc"
"clubs"
readScriptString
readProcessingInstruction
"alpha"
readComment
"omega"
"raquo"
"sigma"
isFirstIdChar
"emsp"
"macr"
"aacute"
"uml"
"plusmn"
"times"
"cent"
"curren"
"there4"
readTag
entityVal
"uacute"
"aring"
"Scaron"
"sigmaf"
eatSSI
"tilde"
"Psi"
"Yuml"
"prime"
"iota"
"otimes"
"xi"
"pound"
nextSkipWS
"lrm"
"rfloor"
"forall"
"Euml"
"chi"
"copy"
"Igrave"
"Phi"
"auml"
"darr"
"sbquo"
"circ"
"shy"
"asymp"
"ugrave"
"thorn"
"sup"
"ecirc"
HTMLStripCharFilterFactory
unhyphenate
LowerCaseFilterFactory
readPos
mappingFile
"mapping"
writePos
Mapping
"\"(.*)\"\\s*=>\\s*\"(.*)\"\\s*$"
"typeMatch"
NumericPayloadTokenFilterFactory
foundDelimiter
prepareReplaceBlock
lastDiff
replaceBlockBuffer
"maxBlockChars
blockBufferLength
replaceBlock
blockBuffer
lastMatchOffset
sourceBlock
getReplaceBlock
appendReplacement
appendTail
blockChars
replaceBlockBufferOffset
"blockDelimiters"
"maxBlockChars"
'replace'
"replace"
'all'
PatternReplaceFilterFactory
'first'
ofs
"group"
lastNonEmptySize
matchList
phonetic
"setMaxCodeLen"
initializing:
Soundex
RefinedSoundex
encoder:
PositionFilterFactory
"positionIncrement"
posIncrement
saved
oldLen
maxPosQuestion
"maxFractionAsterisk"
maxFractionAsterisk
maxPosAsterisk
minTrailing
posA
posQ
"minTrailing"
"maxPosAsterisk"
"maxPosQuestion"
cfgLanguage
"from
posIncGap
stopWordFiles
isEnablePositionIncrements
"enablePositionIncrements"
stopFilter
pushTok
newPosIncAtt
newOffsetAtt
repPos
repTok
nextTok
origPos
origTok
origPosInc
firstTok
newTermAtt
newTypeAtt
targetIt
lastTok
lastOffsetAtt
firstPosIncAtt
sourceIt
Rule:"
strList
toToks
getSynonymMap
"tokenizerFactory"
Synonym
fromToks
expansion
tokList
splitByTokenizer
loadTokenizerFactory
getSynList
synonymFile
"expand"
mappingSep
loadTokenizer
synList
"synonyms"
synSep
mergeTokens
",ORIG"
mergeExisting
tok2
tok1
lst2
lst1
"SynonymFilter:
INCLUDE_ORIG
IGNORE_CASE
"],"
"TokenizerChain("
TokenOffsetPayloadTokenFilterFactory
TrieTokenizer
getNumericTokenStream
TrieIndexTokenizer"
newEnd
endOff
value.
false."
TypeAsPayloadTokenFilterFactory
flushConcatenation
shouldConcatenate
savedType
shouldGenerateParts
lastConcatCount
termAtttribute
hasOutputToken
concatenate
accumPosInc
termbuffer
hasOutputFollowingOriginal
hasSavedState
savedEndOffset
WordDelimiterConcatenation
concatenation
startOffSet
subwordCount
concatAll
generatePart
concat
savedStartOffset
endOffSet
savedBuffer
hasIllegalOffsets
writeAndClear
"catenateAll"
"stemEnglishPossessive"
"catenateNumbers"
"splitOnNumerics"
"generateNumberParts"
WordDelimiterFilterFactory
"generateWordParts"
"preserveOriginal"
"catenateWords"
"splitOnCaseChange"
lastType
TITLECASE_LETTER
OTHER_NUMBER
isLowerCase
endsWithPossessive
SURROGATE
skipPossessive
charType
hasFinalPossessive
endBounds
LETTER_NUMBER
tab
MODIFIER_LETTER
startBounds
isBreak
ENCLOSING_MARK
errIfMissing
explicitly
setting,
version."
"LUCENE_$1$2"
"^(\\d)\\.(\\d)$"
xstr
happen.
parsedMatchVersion
versionWarningAlreadyLogged
Lucene,
xpath"
'V.V'"
compatibility
upgrades
xpath:"
important
"then
QName
"sizable
returnPrevNotClosed
solr.xml:
setAbortOnConfigurationError
solrLoader
</cores>\n"
fileCopy
MB32
"solr/cores/@managementPath"
"collection1"
"abortOnConfigurationError"
DEFAULT_DEFAULT_CORE_NAME
solrConfigFilenameOverride
getDefaultCoreName
"re-using
idir
"sharedLib"
defaultAbortOnConfigError
writeAttribute
adminHandlerClass
instanceDir=\".\"
"creating
"swaped:
"shareSchema"
"solr/cores/@defaultCoreName"
libLoader
fcout
"solr/cores/@adminPath"
setAdminPath
coreAdminHandler
unnamed
dcoreName
"<solr
?>\n"
adminHandler
setManagementPath
DEF_SOLR_XML
"replacing
corePropsFile
"solr/cores/@adminHandler"
"</cores>\n"
"registering
<core
"Persisting
"looking
encoding='UTF-8'?>"
solrHome
newCore
"solr/@sharedLib"
adminPath=\"/admin/cores\"
xforward
isShutDown
"adminPath"
">\n"
".xml"
"</core>"
returnPrev
"/>\n"
checkDefault
tmpFile
defaultCoreName
cfgis
"solr/@persistent"
'DEFAULT_CORE'
"adminHandlerClass
defaultCoreFound
<core"
containerProperties
"</solr>"
<cores
"property"
"<cores"
numCoresAbortOnConfigError
fcin
readProperties
"solr/cores/@shareSchema"
"<property"
createMultiCoreHandler
"adminHandler"
"loading
xnf
managementPath
shareSchema
persistent=\"false\">\n"
cores."
indexSchemaCache
writeProperties
"solr/cores/core"
"<solr"
"</solr>\n"
getCoreProps
defaultCoreName=\""
library:
persistent
n0
\'instanceDir\'"
initImplicitProperties
"solr.core.instanceDir"
"solr.core.configName"
propertiesName
implicitProperties
getDefaultDataDir
"solr.core.dataDir"
"solr.core.schemaName"
"solr.core.name"
updateCommitPoints
cleanReserves
wrapperList
previousTime
latestCommit
reserveTime
reserveCount
indexCommitVersion
reserves
savedCommits
solrVersionVsCommits
IndexCommitWrapper
currentTime
timeToSet
"setTermIndexInterval"
JMXConnectorServer
newMBeanServer
actionName
registerMBean
InvalidAttributeValueException
setAttributes
ReflectionException
mbeans
jmxRootName
Supported"
found,
AttributeList
JMXConnectorServerFactory
agentId:
unregister
attibute
Adding
MBeanException
bean:
Server:
unregisterMBean
isRegistered
DynamicMBean
exposing
managedResource
Servers
staticStats
"Incorrect
AttributeNotFoundException
JMX."
newJMXConnectorServer
attrInfoArr
dynamicStats
attrInfoList
MBeanAttributeInfo
solrconfig.xml,
"class
NL_TAGS
loadSubPlugins
"args
sending
requests
"queries"
"QuerySenderListener
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/core/RequestHandlers.java
RequestHandlers.java
_handler
requestHandler
_className
getWrappedHandler
startup
LazyRequestHandlerWrapper
rev
handlers
getHandlerClass
_args
"Lazy["
DEFAULT_HANDLER_NAME
"About
"env"
"wait"
envp
"exe"
INVALID_PROCESS_RETURN_CODE
waitFor
cmdlist
RunExecutableListener
"Executable
callback
hashSetInverseLoadFactor
urlSnippet
hashDocSetMaxSize
OPENTIME
"[solrconfig.xml]
"query/boolTofilterOptimizer/@cacheSize"
updateHandlerInfo
"directoryFactory"
MatchVersion:
"query/useColdSearcher"
instead"
cacheControl
pluginStore
"<pingQuery>
max-age
"query/cache"
"fieldValueCache"
initLibs
qtokens
"queryResponseWriter"
attributes:
<searchComponent/>"
"query/queryResultCache"
"\\bmax-age=(\\d+)"
"query/maxWarmingSearchers"
"openTime"
loadUpdatehandlerInfo
"@lastModFrom"
"@never304"
"/config/"
defaultIndexConfig
(consider
"admin/pingQuery"
commitIntervalLowerBound
"valueSourceParser"
"//HashDocSet/@loadFactor"
"updateRequestProcessorChain"
"mainIndex/reopenReaders"
CACHE_PRE
lib
"jmx/@serviceUrl"
"@etagSeed"
attempting
"searchComponent"
"query/documentCache"
ttlMatcher
instead)"
'dir'
"queryParser"
"requestDispatcher/httpCaching/"
"//HashDocSet/@maxSize"
"indexReaderFactory"
registering
"updateHandler/autoCommit/maxTime"
<pingQuery>
UpdateHandlerInfo
'/admin/ping'
"PingRequestHandler
"jmx/@agentId"
BOGUS
"query/boolTofilterOptimizer/@threshold"
"query/useFilterForSortedQuery"
loadPluginInfo
.05f
"query/filterCache"
ttlStr
MAX_AGE
SolrConfig:
"mainIndex"
'path'"
"requestHandler"
pingQueryParams
"updateHandler/commitIntervalLowerBound"
ClassLoader"
readPingQueryParams
<highlighting/>
never304
"lib:
"lib"
lastModFrom:
"query/boolTofilterOptimizer/@enabled"
"query/maxBooleanClauses"
"updateHandler/autoCommit/maxDocs"
"query/queryResultMaxDocsCached"
"//listener"
"query/enableLazyFieldLoading"
"query/fieldValueCache"
"mainIndex/deletionPolicy"
deprecated,
"cacheControl"
"query/queryResultWindowSize"
"updateHandler/@class"
config:
httpCachingConfig
"mainIndex/unlockOnStartup"
cacheControlHeader
newSearcherHolder
updateProcessorChains
decrementOnDeckCount
parameter,
Overlapping
newIndexDir
SolrCore.java
name='facet.sort'>."
"phps"
locked.
valueSourceParsers
hide
<admin/gettableFiles>,
getSchemaResource
initIndexReaderFactory
_searchers
boolean_query_max_clause_count
maxWarmingSearchers="
loadSearchComponents
initWriters
newIndexDirFile
updateHandlerClass
[count:{}]
CLOSING
dataDir="
Instantiating
removeLocks
Policy
addIfNotPresent
solrDelPolicy
valueSourceParser
loadUpdateProcessorChains
searcherExecutor
searcher:"
initValueSourceParsers
propsFile
newestSearcher
coreDescriptor
Unlocking..."
currSearcher
{}.
defClassName
"fieldCache"
911216
"Registered
"BooleanQuery.maxClauseCount=
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/core/SolrCore.java
"REFCOUNT
<bool
23:09:12Z
reqHandlers
hook
"solrconfig.xml
registerSearcher
onDeckSearchers="
"SolrCore"
forceNew
searchComp
responseWriters
initQParsers
defaultResponseWriter
getCanonicalFile
qtime
_searcher
decrement="
name='facet.sort'>.
"//bool[@name='facet.sort']"
SolrEventListener:
createUpdateHandler
UpdateRequestProcessorChain:
"index/"
solr-user@lucene.apache.org"
logid
"php"
"refCount"
createInstance
initListeners
"currentSearcher"
getConfigResource
searchComponents
getConstructors
qParserPlugins
facetSort
"handler"
<string
Object"
currSearcherHolderF
con
"Null
initDeprecatedSupport
Handler"
createEventListener
"PERFORMANCE
Component:
parserName
newSearchHolder
"admin/gettableFiles"
"coreName"
returnSearcher
ShowFileRequestHandler."
"aliases"
newHolder
alreadyRegistered
"SolrCore.initIndex"
initDirectoryFactory
2010-02-17
echoParams
initDeletionPolicy
pluginInfos
searcher.
"ERROR!!!
"ruby"
"updateHandler"
"Deletion
onDeckSearchers
gettable
"python"
core"
closeHooks
indexDirFile
request."
currSearcherHolder
optimizedKept
"SolrDeletionPolicy.onInit:
maxCommitAgeTimeStamp
deletion"
",segFN="
maxOptimizedCommitsToKeepString
maxCommitAgeString
"SolrDeletionPolicy.onCommit:
"maxCommitAge"
totalKept
keepOptimizedOnlyString
updateCommits
fdir
setMaxCommitsToKeep
",generation="
"maxOptimizedCommitsToKeep"
"commit{"
",filenames="
maxCommitAge
maxCommitsToKeep
maxOptimizedCommitsToKeep
getMaxCommitsToKeep
point's
commits:"
keepOptimizedOnly
"newest
"num="
"dir="
",version="
"maxCommitsToKeep"
maxCommitsToKeepString
"keepOptimizedOnly"
SolrInfoRegistry
getRegistry
"search."
Aware
oldLoader
"analysis."
object:
"update.processor."
"handler.component."
solr.home:
JNDI"
"JNDI
"/home
cwd="
".solr.home"
pathname
awareCompatibility
defaulted
"request."
'Aware'
getURLs
(could
shortname
interface:
subPackages
packages
"org.apache"
classNameCache
class-name
"handler.dataimport."
cName
subpackage
(NoInitialContextEx)"
JNDI)"
classpath
waitingForCore
"schema."
of:
"Odd
read)
classloader"
classloader:
replaceClassLoader
"update."
NoInitialContextException
"java:comp/env/"
infoMBeans
"core."
"/home"
constructor
oldElements
"handler."
classLoader
JNDI:
"response."
waitingForResources
"util."
"spelling."
uniq
"posInc"
"Tokenizing
occured
getTermsToMatch
ListBasedTokenStream
tokenIterator
writeCharStream
listBasedTokenStream
tokensNamedLists
tokenFilterFactory
convertTokensToNamedLists
iterating
tokenstream"
tokenizerChain
tokenNamedList
analyzeTokenStream
delcmd
BinaryUpdateRequestHandler
isId
parseAndLoadDocs
javabin
BinaryUpdateRequestHandler.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java
getAddCommand
documentLoader
getErrHeader
setErrHeader
setEncapsulator
ESCAPE
FIELDNAMES
fstrat
encapsulator:'"
lineno
prepareFields
CSVStrategy
CSVLoader
skipStr
"\n\tvalues={"
'from:to'
CSVRequestHandler.java
skipFields
"trim"
CSVRequestHandler
hasHeader
fieldnames=<fields>*
FieldTrimmer
line="
adder
FieldSplitter
fesc
formatted
colonSplit
escape:'"
fmap
ESCAPE_DISABLED
escStr
commaSplit
COMMENTS_DISABLED
skipLines
"CSVLoader:
CSV
FieldAdderEmpty
SingleThreadedCSVLoader
fenc
setEscape
header=true"
SPLIT
mapArgs
adders
SKIPLINES
rows"
strategy
sepStr
setUnicodeEscapeInterpretation
TRIM
fieldnames
setDelimiter
adderKeepEmpty
input_err
CSVParser
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/CSVRequestHandler.java
fsep
"CSVLoader:"
separator:'"
mapRule
ENCAPSULATOR
encStr
csv
HEADER
keepEmpty
,got
FieldMapperSingle
FieldAdder
input="
encapsulator
Handler:
630746
2008-02-25
07:02:09Z
boosts"
DisMaxRequestHandler.java
"DisjunctionMax
relevancy
"across
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/DisMaxRequestHandler.java
"http://wiki.apache.org/solr/DisMaxRequestHandler"
variety
expects
indexTokens
hasId
13:40:27Z
fieldValues
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
analyze"
extractSingleContentStream
fieldTokens
"DocumentAnlysisRequestHandler
824333
DocumentAnalysisRequestHandler.java
analysisContext
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/DumpRequestHandler.java
(debug)"
DumpRequestHandler.java
"streams"
"sourceInfo"
"context"
DumpRequestHandler
"Dump
FieldAnalysisRequestHandler.java
analysisResults
analyzeValues
analyzedTokens
805844
useDefaultSearchField
fieldTypeAnalysisResults
analyzeResults
15:38:11Z
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
fieldNameAnalysisResults
field/query
matchOffset
BOOST_ORDER
mltquery
"MoreLikeThis
getMoreLikeThis
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
includeMatch
fillInterestingTermsFromMLTQuery
InterestingTerm
MoreLikeThis"
debug"
(?q=)
boostFields
mltQuery
"exception_during_debug"
termStyle
setBoosts
ContentStreams"
"http://wiki.apache.org/solr/MoreLikeThis"
MoreLikeThisHandler.java
mltDocs
"Service
"admin/healthcheck/text()"
health
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/PingRequestHandler.java
load-balancer"
pingrsp
RequestHandler:
"Reports
PingRequestHandler.java
application
healthcheck
"Ping
"currentFileSize"
isReplicating
"filechecksum"
getFileList
replicateOnCommit
"SnapPull
writeNothing
"commitReserveDuration"
checkSumVal
totalPercent
getIndexSize
getReplicateAfterStrings
useChecksum
setTimesReplicatedSinceStartup
sindexVersion
"isSlave"
backupOnOptimize
downloadSpeed
"compression"
'details'
enableMaster
"filesToDownload"
"replicationEnabled"
"disablereplication"
PACKET_SZ
"ERROR"
CMD_FETCH_INDEX
getIndexVersion
'masterUrl'
getAsMap
"fetchindex"
"timeElapsed"
cfileName
REPLICATE_AFTER
"checksum"
"filecontent"
sChecksum
currFileSizeDownloaded
DeflaterOutputStream
"offset"
906086
indexCommitPoint
replicationEnabled
getFileChecksum
estimatedTimeRemaining
getCheckSum
"replicateAfter"
"replicatableGeneration"
CMD_DISABLE_POLL
"masterDetails"
checksumMap
"replicatableIndexVersion"
list:
"disablepoll"
snapPullLock
addVal
bytesToDownload
packetsWritten
readableSize
"confFiles"
CMD_ABORT_FETCH
solrPolicy
"currentFileSizeDownloaded"
"Replicate
doSnapShoot
enableSlave
RESERVE
paramsCopy
getFileStream
includeConfFiles
percentDownloaded
isSlave
"isMaster"
getReplicationDetails
OK_STATUS
CMD_DISABLE_REPL
replicateOnOptimize
computeIndexSize
"abortfetch"
inFile
"currentFile"
registerCloseHook
"totalPercent"
"generation"
snapshooting"
nameAndAlias
currFileSize
"filesDownloaded"
"numFilesDownloaded"
ReplicationHandler.java
"isReplicating"
2010-02-03
backupOnCommit
Slaves"
showSlaveDetails
"confchecksum"
"filelist"
snapPuller
CMD_ENABLE_REPL
"message"
getFileInfo
getTimesReplicatedSinceStartup
"00:00:10"
getCheckSums
"replicationStartTime"
ERR_STATUS
isMaster
CMD_ENABLE_POLL
"backup"
"Polling
lasmodified
versionGen
oldCommitPoint
"filestream"
confFileNameAlias
clzz
"indexPath"
"bytesToDownload"
details:
replicateAfter
versionAndGeneration
"invalid_master"
fileMeta
"enablepoll"
CMD_FILE_CHECKSUM
tempSnapPuller
"downloadSpeed"
reserveCommitDuration
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/ReplicationHandler.java
startup"
"timeRemaining"
indexversion
"enablereplication"
"confFilesToReplicate"
registerFileStreamResponseWriter
sOffset
NEXT_EXECUTION_AT
Master
snapShooter
"lastmodified"
provides
CMD_SHOW_COMMITS
currFile
indexversion"
snapshotStats
"masterUrl"
"cf"
getCommit
"alias"
bytesRead
"Commits
Startup
"nextExecutionAt"
LEN
"numFilesToDownload"
"isPollingDisabled"
"indexSize"
CONF_CHECKSUM
replicateOnStart
numTimesReplicated
15:21:56Z
Specified"
"indexversion"
"len"
getEventListener
"backupAfter"
"currentFileSizePercent"
"totalTime"
"handlerStart"
getInitArgs
timedOut
"httpCaching"
"timeouts"
"appends"
numTimeouts
"avgRequestsPerSecond"
"avgTimePerRequest"
handlerStart
change
experimental.
future."
Replication"
INTERVAL_PATTERN
'HH:mm:ss'"
replicationTimeTaken
Scheduled
setCredentials
"replication.properties"
downloadConfFiles
"previousCycleTimeInSeconds"
snappuller
getLatestVersion
"Fetch
executorService
"User
executorStartTime
"indexReplicatedAtList"
searcherRefCounted
errorCount
tmpIdxDirName
checkSumServer
isIndexStale
tmpIndexDir
"conf."
httpBasicAuthUser
deleteTmpIdxDir
readToStringBuffer
"httpReadTimeout"
'pollInterval'.
segmentsFile
connTimeout
useExternal
createHttpClient
"Force
"pollInterval"
"timesFailed"
fsyncExceptionCopy
"segments_"
"httpBasicAuthPassword"
Updating
isShutdown
recieved
fetching
map1
copyAFile
nextTime
fetchPackets
HTTP_BASIC_AUTH_PASSWORD
logReplicationTimeAndConfFiles
master:
restart
"Poll
numFailures
auth
fileFetcher
startExecutorService
"index."
checkCompressed
localIndexFile
Exception:
"Slave's
longbytes
fileOutputStream
FileFetcher
"(\\d*?):(\\d*?):(\\d*)"
INTERVAL_ERR_MSG
pollInterval
createTempindexDir
checkSumClient
"Checksum
Downloaded
modifyIndexProps
"'masterUrl'
packetSize
replicationStartTime
"Slave
folder:
copiedfiles
replication"
fsyncService
slave"
isFullCopyNeeded
"timesIndexReplicated"
copyIndexFiles
reloaded"
confFilesToDownload
un-used
fsyncException
poll,
tmpFileFetcher
"Master
oldFile
HTTP_READ_TIMEOUT
"confFilesReplicatedAt"
confFilesCount
MAX_RETRIES
cleanup"
generation:
useInternal
getNamedListResponse
localFilesInfo
downloadCompleteIndex
master."
getDateAsStr
confFilesDownloaded
"lastCycleBytesDownloaded"
latestGeneration
intbytes
HTTP_BASIC_AUTH_USER
HTTP_CONN_TIMEOUT
NO_CONTENT
"replicationFailedAtList"
fetchFileList
indexFileInIndex
pollIntervalStr
files."
AuthScope
downloadIndexFiles
replicationTime
UsernamePasswordCredentials
isSuccess
"inside
"confFilesReplicated"
copy2Dir
rename:
indexFileInTmpDir
installed.
ReplicationHandlerException
myHttpClient
isConf
copyTmpConfFiles2Conf
completely.
"httpBasicAuthUser"
indexversion:
nameVsFile
Timer
pollDisabled
includeChecksum
ERR
indexCount
isTerminated
secs"
hr
modified,
initialDelay
fileChannel
started."
backupFile
"httpConnTimeout"
tmpIdxDir
latestVersion
"replicationFailedAt"
successfulInstall
fetchFile
modifiedConfFiles
getModifiedConfFiles
details"
saveAs
properties..."
"Master's
"indexReplicatedAt"
tmpconfDir
copy"
"timesConfigReplicated"
packets
httpBasicAuthPassword
terminateAndWaitFsyncService
canWrite
"snapshotCompletedAt"
directoryName
fileCopier
preserveFileDate
createSnapshot
FileCopier
rcnt
lock:
"fileCount"
"snapDir"
snapShotDir
"snapShootException"
SNAP_DIR
"snapshot."
writing."
"Spelling
boolean"
integer"
RAM
SUGGESTIONS
EXTENDED
"suggestionCount"
float"
DEFAULT_MORE_POPULAR
results'
spellcheckerIndexDir
termSourceField
"frequency"
dirDescription
"reopen"
popular'
POPULAR
"'Only
SOURCE_FIELD
DEFAULT_ACCURACY
rebuild
QUERY_PREFIX
"dictionary."
Command:
THRESHOLD
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/SpellCheckerRequestHandler.java
DEFAULT_DICTIONARY_THRESHOLD
"threshold"
suggestionField
"cmdExecuted"
SpellCheckerRequestHandler
DICTIONARY_PREFIX
"query."
"'Extended
"spellcheckerIndexDir"
"(ramdir)"
"sp."
"Threshold
SpellCheckerRequestHandler.java
getDictionary
DEFAULT_EXTENDED_RESULTS
"Accuracy
"termSourceField"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/StandardRequestHandler.java
"http://wiki.apache.org/solr/StandardRequestHandler"
StandardRequestHandler.java
requestedCats
requestedKeys
"luceneImplVersion"
2010-03-02
mBeanInfo
917812
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/SystemInfoRequestHandler.java
"cwd"
"unknown"
solrP
SystemInfoRequestHandler
"luceneSpecVersion"
arrayToSet
00:01:30Z
catName
"objects"
cats
"solrImplVersion"
SystemInfoRequestHandler.java
"srcId"
hostname
luceneP
catInfo
"solrSpecVersion"
"SolrCore.update(add)"
sawWaitSearcher
delete/@"
"XMLLoader:
FactoryConfigurationError
/delete/"
commit/@"
sawWaitFlush
"fromPending"
doc/@"
doc/field/@"
"fromCommitted"
deleteCmd
XmlUpdateRequestHandler.java
chain:
\"legacy\"
processorFactory
"overwriteCommitted"
"overwritePending"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java
AdminHandlers.java
"Register
StandardHandler
Handlers"
Standard
'/admin'"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/AdminHandlers.java
Typically
"luke"
AdminHandlers
AdminHandler
Admin
path.
CoreAdminHandler.java
handlePersistAction
RequestDispatcher"
CREATE"
Multiple
handleSwapAction
"Persistence
operation:
handling
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
"saved"
handleAliasAction
action"
Cores"
dirNames
doPersist
solrconf.xml\n"
wrappedReq
'status'
handleMergeAction
"CoreAdminHandler
handleReloadAction
separatorChar
handleUnloadAction
"'ALIAS'
handleRenameAction
handleCreateAction
normalizePath
handleStatusAction
'reload'
opts
handleCustomAction
junkWords
getAnalyzerInfo
"docId"
ttinfo
"histogram"
Inspired
"docFreq"
"queryAnalyzer"
Browser.
cfilters
"copySources"
getIndexedFieldsInfo
LukeRequestHandler.java
getSchemaInfo
"http://wiki.apache.org/solr/LukeRequestHandler"
getFieldFlagsKey
docinfo
"charFilters"
Frequency
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
field)"
2010-03-15
hist
"filters"
(df)
"(unstored
"dynamicBase"
"copyDests"
DEFAULT_COUNT
deletion.
countTerms
TopTermQueue
typeusemap
aslist
"dynamicFields"
populateFieldInfo
"hasDeletions"
aninfo
"defaultSearchField"
highestOneBit
minFreq
distinctTerms
maxBucket
DOC_ID
getDocumentFieldsInfo
http://www.getopt.org/luke/"
modeled
"indexAnalyzer"
LOG2
marked
Luke:
923411
tchain
filtfac
19:56:25Z
"uniqueKeyField"
"optimized"
histogram
NUMTERMS
Declared"
PluginInfoHandler.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
getSolrInfoBeans
"Registry"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
Properties"
PropertiesRequestHandler.java
"system.properties"
ShowFileRequestHandler.java
"Admin
view
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
adminFile
Get
getFileContents
"hidden"
show:
isHidden
USE_CONTENT_TYPE
hiddenFiles
access:
path:
"upTimeMS"
getJvmInfo
"bootclasspath"
getCoreInfo
"#.#"
jvm
executing:
luceneImplVersion
getRuntimeMXBean
"uname
(%"
getBootClassPath
Info"
"lucene-impl-version"
"lucene-spec-version"
"uname"
"totalPhysicalMemorySize"
"ulimit
"committedVirtualMemorySize"
getClassPath
"java.vm.version"
"solr-impl-version"
-a"
"commandLineArgs"
"systemLoadAverage"
"instance"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
luceneSpecVersion
RuntimeMXBean
"java.vm.name"
"ulimit"
"processors"
getLuceneInfo
"(error
getInputArguments
-n"
mx
"total"
SystemInfoHandler.java
"windows"
"maxFileDescriptorCount"
"processCpuTime"
"used"
"memory"
getSystemInfo
"classpath"
"jvm"
percentUsed
"free"
"solr-spec-version"
"openFileDescriptorCount"
"totalSwapSpaceSize"
solrImplVersion
schema!"
solrSpecVersion
"threadDump"
ThreadDumpHandler.java
getLockName
ste
tinfos
getAllThreadIds
"suspended"
getLockOwnerName
getPeakThreadCount
ThreadInfo
"threadCount"
getThreadName
"cpuTime"
"peak"
"%.4fms"
"userTime"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
getLockOwnerId
getDaemonThreadCount
"lock"
"thread"
Dump"
isSuspended
getThreadInfo
isInNative
owner
formatNanos
"deadlocks"
getThreadCpuTime
ThreadMXBean
getThreadMXBean
"daemon"
tids
getThreadCount
getThreadId
findMonitorDeadlockedThreads
"stackTrace"
tid
getThreadUserTime
tmbean
isThreadCpuTimeSupported
DebugComponent.java
746031
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/DebugComponent.java
22:20:32Z
sdebug
dkey
stdinfo
didx
Information"
2009-02-19
skey
sexplain
Faceting"
countFacets
needRefinements
refineFacets
",count="
needRefinement
17:28:56Z
paramStart
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/FacetComponent.java
numRequested
counted
maxPossible
",termNum="
FacetComponent.java
facetOn
"{term="
"=$"
DistribFieldFacet
facetStr
returnedKey
shardCounts
facet_counts
"Handle
FieldFacet
"__terms"
FacetBase
fieldCounts
refList
refinements
missingMaxPossible
dff
shardNum
missingMax
ShardFacetCount
maxCount
sfc
2009-06-04
queryFacets
facetCommand
countSorted
termsVal
termsKey
initialLimit
refine
fillParams
commandPrefix
QueryFacet
newRequest
781801
facet_queries
numReceived
facet_fields
ntop
_toRefine
queryFacet
statsTermNum
facetStats
statsTermCounts
stringIntegerEntry
facetStatsTerms
numStatsTerms
statsTermCount
sumData
09:43:50Z
defHighlighter
"Highlighting"
defaultHighlightFields
899572
HighlightComponent.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/HighlightComponent.java
"highlight"
"moreLikeThis"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
MoreLikeThisComponent.java
This"
mltcount
Like
sortVals
returnScores
doFieldSortValues
idArr
createDistributedIdf
createMainQuery
doPrefetch
scoreObj
QueryComponent.java
createRetrieveDocs
fqp
prevShard
defType
uniqueDoc
"'start'
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/QueryComponent.java
resultSize
luceneIds
mergeIds
responseDocs
fsv
elevation"
elevate.xml"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
child"
"enableElevation"
FORCE_ELEVATION
QueryElevation
"either:
Boosting
'<doc
particular
"')"
"config-file"
fC
fD
.../>'
"QueryElevationComponent
ENABLE
elevate
"Boosting
QueryElevationComponent.java
"queryFieldType"
QueryElevationComponent."
"forceElevation"
"elevate/query"
searchHolder
elev
StrField"
loadElevationMap
'id'"
"queryBoosting"
"http://wiki.apache.org/solr/QueryElevationComponent"
QueryElevationComponent"
FieldType:
qparser
globalCollectionStat
GlobalCollectionStat
isNeedDocList
needDocList
setNeedDocList
STAGE_START
"fsv"
CompletionService
"prepare"
dbgCmp
cancelAll
"last-components"
ExecutorCompletionService
commExecutor
INIT_SO_TIMEOUT
component:"
Exception"
takeCompletedOrError
SimpleSolrResponse
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/SearchHandler.java
HttpCommComponent
ssr
"first-components"
comm
shard-connection-timeout
"http://"
shard-socket-timeout
makeDebugLast
declaredComponents
'components'"
completionService
components:
soTimeout
INIT_CONNECTION_TIMEOUT
shardHandler
"timing"
"Impossible
"First/Last
nextStage
"shard-connection-timeout"
SearchHandler.java
"shard-socket-timeout"
"process"
"components"
sortVal
objA
objB
comparatorStringLocale
getCachedComparator
,shard="
ShardComparator
,score="
,positionInResponse="
fieldNum
sd1
sd2
comparatorScore
,orderInShard="
,sortFieldValues="
comparatorMissingStringLast
comparatorNatural
purpose="
nResponses
"ShardRequest:{params="
PURPOSE_GET_TERM_DFS
PURPOSE_REFINE_TOP_IDS
rspCode
",shardAddress="
"\n\texception="
"\n\trequest="
"\n\tresponse="
"\n}"
"ShardResponse:{shard="
spellchecker:
queryConverters
"buildOnCommit"
"Specified
defined,
suggEntry
"buildOnOptimize"
checker:
name."
bestIter
suggestionList
converter"
SpellCheckComponent.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
"Collation:"
buildSpellIndex
getSpellCheckers
collate
origVsSuggested
buildOnOptimize
hasDefault
"Registering
optimized
origVsSuggestion
Checker
alternative
collVal
buildOnCommit
suggested
suggestedVsWord
SpellCheckerListener
origVsFreq
hasFreqInfo
origFreq
spellCheckResp
happen)"
"deprecation"
DEFAULT_ONLY_MORE_POPULAR
reloading
sugEntry
spellCheckers
checkers"
spellingResult
statsFs
SimpleStats
"Calculate
getFieldCacheStats
"$URL$"
stats_fields
"$Id$"
shardStv
getStatsCounts
Statistics"
isShard
statsFields
getStatsFields
1.0D
getAverage
nl2
vvals
getStandardDeviation
0.0D
addTo
FACETS
890199
07:06:22Z
theTerm
fieldmap
freqmin
termmap
fieldTerms
freqmax
upperCmp
upperStr
TermsComponent.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/handler/component/TermsComponent.java
upperIncl
termsResponse
flagParam
Enumerators"
createShardQuery
shards.qt
lowerIncl
terms.fl
indexedText
fieldterms
lowerStr
lowerTestTerm
oldtc
termlist
flagParams
listAndSet
uniqFieldName
"uniqueKeyFieldName"
"doc-"
Vectors"
positionsNL
docIds
useOffsets
theOffsets
TVMapper
usePositions
getDocFreq
docNL
fieldNL
uniqId
"tv"
tfIdfVal
altText
bufferedEndOffset
alternateField
snippets
fragsBuilder
"fragmentsBuilder"
bufferedTokenStream
isMergeContiguousFragments
bufferedToken
alternateFieldLen
altTexts
flb
altList
fragmentsBuilder:
"fragListBuilder"
numFragments
getQueryScorer
tvStream
maxCharsToAnalyze
doHighlightingByHighlighter
bufferedOffsetAtt
reqFieldMatch
windowSize
docSummaries
fragListBuilder:
MultiValuedStream
fset
bufferedStartOffset
frags
OrderedToken
getPhraseHighlighter
solrFb
getSpanQueryScorer
"fragmenter"
hasPrevious
summaries
fvh
docTexts
useFastVectorHighlighter
solrFlb
doHighlightingByFastVectorHighlighter
formatter:
createAnalyzerTStream
TokenOrderingFilter
bestTextFragments
ot
highlightMultiTerm
"formatter"
printId
fragmenter:
INCREMENT_THRESHOLD
GapFragmenter.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/GapFragmenter.java
fragOffset
LuceneGapFragmenter
"GapFragmenter"
"HtmlFormatter"
klaas
HtmlFormatter.java
05:39:15Z
2007-07-20
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/HtmlFormatter.java
557874
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/MultiColoredScoreOrderFragmentsBuilder.java
"MultiColoredScoreOrderFragmentsBuilder"
MultiColoredScoreOrderFragmentsBuilder.java
MultiColoredSimpleFragmentsBuilder
"MultiColoredSimpleFragmentsBuilder"
MultiColoredSimpleFragmentsBuilder.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/MultiColoredSimpleFragmentsBuilder.java
DEFAULT_INCREMENT_GAP
minOffset
DEFAULT_SLOP
maxOffset
rawpat
RegexFragmenter.java
DEFAULT_PATTERN_RAW
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/RegexFragmenter.java
,\\n\"']{20,200}"
textRE
defaultPatternRaw
DEFAULT_PATTERN
defaultPattern
DEFAULT_MAX_ANALYZED_CHARS
2009-08-07
LuceneRegexFragmenter
801872
hotspots
hotIndex
"RegexFragmenter
maxAnalyzedChars
minFragLen
addHotSpots
currentOffset
targetFragChars
goal
targetPattern
temphs
03:21:06Z
maxchars
incrementGapThreshold
ScoreOrderFragmentsBuilder.java
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java
"ScoreOrderFragmentsBuilder"
SimpleFragListBuilder.java
"SimpleFragListBuilder"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java
SimpleFragmentsBuilder.java
"SimpleFragmentsBuilder"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java
storedFieldsToHighlight
fieldRegex
emptyArray
storedFieldName
makeParams
DateField:
'gap'
excludeTag
'start'
othersP
enumMethod
excludeTagList
'end'
getFacetQueryCounts
rangeCount
qobj
getTermCounts
iHigh
counts"
getListedTermCounts
getFieldCacheCounts
minDfFilterCache
iLow
resOuter
endS
comes
getFacetDateCounts
qlist
resInner
getFacetFieldCounts
getFacetTermEnumCounts
loop
vc
infinite
excludeStr
facetValue
nullStrComparator
negative?)"
'start':
hasVal
QUERYTYPE_NAME
defval
XSL_NAME
searcherHolder
getIntParam
origParams
ROWS_NAME
QUERY_NAME
START_NAME
getStrParam
phase1_time
termInstances
TopTerm
"{field="
0x00ff8000
bigTermDocSet
multi-valued
",phase1="
tnums
numTermsInField
baseDocs
vIntSize
tindex
",time="
baseSize
",termInstances="
0xff800000
termsInverted
memsz
total_time
termValue
ipos
TNUM_OFFSET
getTermDocs
whichArray
docbase
",nTerms="
",uses="
intervalBits
midPoint
newMaxTermCounts
faceting
endPos
sizeOfStrings
facet_ft
",bigTerms="
0x0000ff80
"Approaching
doNegative
",memSize="
newtarget
uninvert
endTerm
",tindexSize="
ilen
fterm
0xff000000
termLabel
.9
bigTerms
intervalMask
0xffffffff
0xfffffffc
tempArr
maxTermCounts
topTerm
newend
"UnInverted
lastTerm
endDocumentList
SCORE_FIELD
idxInfo
isStreamingDocs
allDocs
writeResponseHeader
IdxInfo
DocListInfo
startDocumentList
writeOther
writeAllDocs
solrQueryRequest
Resolver
useFieldObjects
GenericBinaryResponseWriter
GenericTextResponseWriter
CONTENT_TYPE_JSON_UTF8
JSON_NL_STYLE
firstArrElem
JSON_WRAPPER_FUNCTION
"flat"
wrapperFunction
JSON_NL_ARROFMAP
writeNamedListAsArrMap
"arrmap"
JSON_NL_ARROFARR
writeNamedListAsArrArr
firstVal
writeNamedListAsFlat
hexdigits
JSON_NL_FLAT
repeatCount
"json.wrf"
namedListStyle
JSON_NL_MAP
indentArrElems
writeNamedListAsMapWithDups
newKey
"text/x-json;
PHPWriter
"text/x-php;charset=UTF-8"
"array("
cesu8Setting
'\u07ff'
"b:1;"
"\";"
"d:"
"s:"
"jetty.home"
"Array
"text/x-php-serialized;charset=UTF-8"
"i:"
nBytes
'\u007f'
CESU8
"N;"
"a:"
PHPSerializedWriter
"solr.phps.cesu8"
"b:0;"
"float('NaN')"
needUnicode
"text/x-python;charset=US-ASCII"
PythonWriter
"float('Inf')"
CONTENT_TYPE_PYTHON_ASCII
"False"
"None"
"True"
"text/plain;
charset=US-ASCII"
_baseWriter
"base"
getBaseWriter
"nil"
"text/x-ruby;charset=UTF-8"
"(0.0/0.0)"
"(1.0/0.0)"
CONTENT_TYPE_RUBY_UTF8
RubyWriter
defaultReturnFields
setEndTime
endtime
setAllValues
addToLog
"\n\t\t
writeAttr
CURRENT_VERSION
ver
"<responseHeader>"
fidx1
fidx2
"noSchema"
"<result"
stylesheet
fieldnameComparator
"<response
defaultIndent
defaultFieldList
writePrim
contentLen
indentThreshold
xsi:noNamespaceSchemaLocation=\"http://pi.cnet.com/cnet-search/response.xsd\">\n"
writeDocuments
"stylesheet"
DocumentListInfo
"</lst>"
escapeAttr
xw
cdata
xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n"
closeTag
XML_START1
escapeCdata
"</responseHeader>"
XML_STYLESHEET
XML_START2_SCHEMA
writeCdataTag
tlst
"</result>"
2100
".xsl\"?>\n"
"<response>\n"
"\n</response>\n"
startTag
"</"
XML_START2_NOSCHEMA
href=\"/admin/"
noSchema
"</arr>"
XML_STYLESHEET_END
writeDocs
getOutputProperty
"xsltCacheLifetimeSeconds"
transformation
XSLT_CACHE_DEFAULT
DEFAULT_CONTENT_TYPE
getContentType"
TRANSFORM_PARAM
XSLT_CACHE_PARAM
mediaTypeFromXslt
"text/xml"
"getTransformer
"xsltwriter.transformer"
"media-type"
XSLTResponseWriter"
CONTEXT_TRANSFORMER_KEY
"XSLT
"xsltCacheLifetimeSeconds="
SUB_FIELD_SUFFIX
subType
subSuffix
SUB_FIELD_TYPE
"subFieldType"
subFT
"subFieldSuffix"
dynFieldProps
registerPolyFieldDynamicPrototype
attribute."
getSubType
toBase64String
Binary
FALSE_TOKEN
TRUE_TOKEN
boolAnalyzer
"dimension"
Destination
"Attribute
NULL."
MATH_TZ
millisFormat
parseDateLenient
CANONICAL_LOCALE
zz
MILLISECOND_FIELD
millis
String:'"
DateFieldSource
posBegin
0l
setEndIndex
MATH_LOCALE
millisParser
FieldPosition
setBeginIndex
ISO8601CanonicalDateFormat
toAppendTo
CANONICAL_TZ
getErrorIndex
1000d
1000l
".###"
milliIndex
"defVal"
ftypeS
ExternalFileField
(FloatField)
defValS
"valType"
type.
bitpos
0x00000010
0x00000020
"termPositions"
"termOffsets"
propertyNameToInt
"sortMissingFirst"
0x00000200
getPropertyName
0x00000040
0x00000400
0x00000100
propertyMap
bitfield
"sortMissingLast"
0x00000800
0x00000002
0x00000001
numberOfTrailingZeros
0x00000004
0x00001000
0x00000008
0x00000080
",args="
storage
"compressThreshold"
isMultiValued
trueProperties
internalVal
ftv
positionIncrementGap
",analyzer="
"___"
vec
getFieldIndex
"FieldType:
DefaultAnalyzer
".setAnalyzer("
omitTFPos
".setQueryAnalyzer("
"{class="
falseProperties
FieldType="
schemaVersion
getFieldTermVec
GeoHashField
"dynamicField"
similarityFactory
matchVersionStr
"/schema/defaultSearchField/text()"
defined:
dynFieldType
charFilterLoader
"./charFilter"
Ordering:"
fieldType"
dynamic"
registerDynamicCopyField
schemaConf
"/schema/types/fieldtype
isWildCard
DynamicDestCopy
isDuplicateDynField
fixedCopyFields
"/schema/uniqueKey/text()"
dynamicCopyFields
readAnalyzer
definition"
"/schema/fields/field
destIsPattern
analyzer/charFilter"
sourceField
"uniqueKey
"maxChars"
"./analyzer[not(@type)]
uniqueKeyFieldType
"fieldtype
copyFieldTargetCounts
sourceIsPattern
explicitRequiredProp
Copy
/schema/fields/dynamicField"
DynamicField
(OR)"
list"
SolrQueryAnalyzer
definition
dynamicCopy
copyFieldList
analyzer/filter"
getSimilarityFactory
analyzer/tokenizer"
queryParserDefaultOperator
"/schema/@name"
makeSchemaField
"/schema/solrQueryParser/@defaultOperator"
"copyField
getTargetField
"/schema/similarity"
dcopy
analyzer:
./analyzer[@type='index']"
matchCopyFields
destField
Field:"
dest='"
dstr
dyn
"./filter"
STARTS_WITH
:'"
tokenizers
"[schema.xml]
"dest"
copyFieldsMap
dup
targetField
"reading
dynamicField."
similarity"
readSchema
defaultSearchFieldName
source='"
SolrIndexAnalyzer
maxCharsInt
analyzerName
"//copyField"
tokenizerLoader
anode
"Schema
"/schema/"
DynamicCopy
"./analyzer[@type='query']"
addDynamicField
fieldsWithDefaultValue
hasExplicitField
work"
dtype
copied."
addDynamicFieldNoDupCheck
multivalued"
schemaAware
maxChars='"
"Dynamic
analyzerCache
dFields
integer.
fieldLoader
"undefined
ENDS_WITH
getDynamicFieldType
DynamicReplacement
getQueryParserDefaultOperator
Duplicate
filterLoader
"/schema/@version"
getIndexedField
/schema/types/fieldType"
"./tokenizer"
suported
PointTypeValueSource
"PointType
subSF
getSeed
RandomValueSource
randomComparatorSource
2057
RandomSortField
",properties="
"{type="
options:"
required=true"
"SchemaField:
trueProps
conflicting
falseProps
",default="
ibits
"sdouble("
SortableDoubleFieldSource
"sfloat("
SortableIntFieldSource
"sint("
"slong("
SortableLongFieldSource
Projector
"projector"
plotters
Instance
ValueSource"
DEFAULT_START_LEVEL
"SpatialTileField
plotter
Make
projectorName
START_LEVEL
tileDiff
SpatialTileField
END_LEVEL
DEFAULT_END_LEVEL
PROJECTOR_CLASS
"str("
parseFieldQuery
INT_PREFIX
schema.xml
LONG_PREFIX
0x0ffffffffL
toDouble
"ERROR:SCHEMA-INDEX-MISMATCH,stringValue="
toInt
NEW
fromString
String:
DASH
invalidateSize
"boost_parsed"
"boost_str"
BOOSTFUNC
configPath
cache"
nameAttr
"regenerator"
configs
regenImpl
cacheImpl
addMainQuery
addBoostFunctions
getAlternateUserQuery
addBoostQuery
getUserQuery
getPhraseQuery
Operation"
requestedEnd
realEndDoc
realLen
qslop
mainUserQuery
getQueryStopFilter
newtf
ExtendedDismaxQParser
escapedUserQuery
phraseFields3
phraseFields2
lowercaseOperators
QType
goat
specialSyntax
getQueryTokenizerChain
ExtendedAnalyzer
"pf2"
minClauseSize
sawLowerAnd
RANGE
removeStopFilter
allowWildcard
addShingledPhraseQueries
WILDCARD
setRemoveStopFilter
numOR
ignoreQuote
facs
"pf3"
doQuote
numAND
multBoosts
DMP
splitIntoClauses
newa
sawLowerOr
"lowercaseOperators"
qa
numMinuses
colon
PHRASE
isPhrase
makeDismax
stopIdx
boostStr
tcq
numPluses
tci
hasWhitespace
normalClauses
topQuery
getAliasedQuery
ExtendedSolrQueryParser
prod
hasSpecialSyntax
FUZZY
highFields
numNOT
shingleSize
syntaxError
numOptional
isBareWord
doMinMatched
itemsArr
"cleanupThread"
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/search/FastLRUCache.java
FastLRUCache.java
statistiscs
919871
acceptableSize="
22:49:24Z
LRU
"minSize"
minLimit
cevictions
0.9
cleanupThread="
cinserts
"Concurrent
acceptableLimit
statsList
"item_"
"acceptableSize"
2010-03-06
showItems
consumeArgumentDelimiter
{!v=value}
doConsumeDelimiter
nestedQuery
parameter.
nestedLocalParams
argParser
$param
FunctionQuery("
forms.
csq
funcQ
funcStr
"incu"
"incl"
"frange"
DEFAULT_INVERSE_LOAD_FACTOR
tsize
resultCount
inverseLoadFactor
CumulativeStats
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/search/LRUCache.java
"LRU
09:42:00Z
890250
LRUCache.java
oldSort
lparser
LuceneQParser
'sort'
query,
useGlobal
queryOut
filterOut
"\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffffNULL_VAL"
bigString
NULL_ORD
missingValueProxy
nullVal
MissingLastOrdComparator
prefixText
prefixField
0xcecf7fe2
PrefixGenerator
setReq
"Infinite
"QParser"
Recursion
qplug
localType
tagStr
useGlobalParams
setLocalParams
defaultType
checkRecurse
rowsS
recurseCount
nestedParser
addTag
val_start
parentheses:
spec,
spec:
"_docid_"
DOCID
LOCALPARAM_START
wrapQuery
regionMatches
lt
mismatched
identifier
"EXCEPTION(val="
"bad
"(UNKNOWN
\\uxxxx
"After
processSort
writeFieldName
isJavaIdentifierStart
id_start
LOCALPARAM_END
functionDepth
"q.op"
writeFieldVal
ut
peekChar
QueryParser:"
writeBoost
"bottom"
function:
needOrder
sf2
sf1
defaultSort
newBq
negClause
AUTOWARMING
STATICWARMING
managed
Solr."
**NOT**
"entries_count"
826788
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/search/SolrFieldCacheMBean.java
19:44:41Z
2009-10-19
"insanity#"
"insanity_count"
SolrFieldCacheMBean.java
introspection
"entry#"
FieldCache,
subLeaves
numLeaves
leafOffsets
setInfo
",r="
",parent="
",segments="
leaves
"SolrIndexReader{this="
shortName
",refCnt="
zeroIntArray
buildInfoMap
SolrReaderInfo
leafReaders
getWrappedReader
getLeaves
subLeaf
registerTime
optionalAnswer
userCacheConfig
sortDocSet
readDocs
fieldValueCache
posQuery
replaceFlags
"autowarming
noGenericCaches
convertFilter
cacheName
luceneFilter
cachingEnabled
"searcherName"
mustCache
"readerDir"
lsort
queryResultCache
setDocSet
cacheDocSet
maxDocRequested
getSupersetMaxDoc
setDocList
getDocListNC
docListAndSet
topscore
openTime
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/search/SolrIndexSearcher.java
enableCache
toLoad
bigFilt
listSet
setPartialResults
positiveB
positiveA
supersetMaxDoc
cacheMap
sliceLen
optimizer
searcher"
needScores
"caching"
QueryCommand,
getFilterList
noCaches
"Searcher@"
getPositiveDocSet
"registeredAt"
setFilter
matchAllDocsQuery
absAnswer
getCache
setCollector
getLen
getDocListAndSetNC
clearFlags
cacheLookup
"reader"
cacheList
logme
setDocListAndSet
cacheInsert
absQ
oldList
absA
absB
setSupersetMaxDoc
fsDirectory
SetNonLazyFieldSelector
SolrIndexSearcher.java
qDocSet
documentCache
smallestIndex
filterList
getDocListC
oldnDocs
"openedAt"
useFilterCache
wildcardQuery
checkAllowLeadingWildcards
checkNullField
"_query_"
prefixQuery
"_val_"
leadingWildcards
schema.xml"
SolrSimilarity
lengthNormConfig
probe
biggerSortedList
maxsz
doca
lastEndIdx
andNotBinarySearch
intersectionBinarySearch
endIdx
leftover
zeroInts
findIndex
smallerSortedList
eidx
firstNonSorted
lenb
lena
otherDocs
nullStringLastComparatorSource
nullLast
nullFirst
"&rows="
"start="
"&sort="
dim
"linear"
"scale"
DateValueSourceParser
NamedParser
splitSources
LongConstValueSource
ms()
"exp"
gh1
gh2
Double2Parser
addParser
lv
"rint"
"cos"
"ceil"
s1MV
"ln"
mv1
mv2
fv
mvr
"acos"
"rad"
There
"tanh"
"sqrt"
"edit"
"recip"
sources.
s2MV
sources2
sources1
DoubleConstValueSource
"cosh"
legacy
"rord"
pv1
pv2
"floor"
"mul"
"ord"
non-numeric
distClass
MultiValueSources,
dest1
dest2
"abs"
"ngram"
"atan2"
"jw"
alias
ValueSources"
"sinh"
getMultiValueSources
"asin"
MVResult
"atan"
"hypot"
"cbrt"
"WildcardFilter("
WildcardGenerator
WildcardFilter
wildcardTerm
boostVal
"boost("
newQ
BoostedWeight
getScorer
"double("
countNext
ffs
"external_"
idName
internalKey
protoTerm
",dataDir="
"\tSkipping
file."
floatCache
numTimesNext
notFoundCount
idType
prevKey
notFound
",defVal="
"FileFloatSource(field="
delimIndex
"\uFFFF\uFFFF\uFFFF\uFFFF\uFFFF\uFFFF\uFFFF\uFFFF"
",keyField="
getCachedFloats
otherErrors
FunctionWeight
")^"
AllScorer
"FunctionQuery("
"literal("
"long("
"max("
PowFloatFunction
"query("
QueryDocVals("
",def="
QueryDocValues
",max="
",target="
",min="
"map("
"/("
minSource
"scale("
",toMin="
",fromMin="
maxSource
",toMax="
",fromMax="
"DocValues
(function)
StringIndexException
"top("
ValueSourceComparator
setCheckDeletes
checkDeletes
docVals
ValueSourceComparatorSource
"frange("
0x9e634b57
0x572353db
"):"
0xe16fe9e7
0xdaa47978
getSources
EARTH_RADIUS_MI
"incompatible
vec2
vec1
parsePointDouble
MILES_TO_KM
latLonStr
hsinY
hsinX
0.621371192
KM_TO_MILES
Only
latitude:
-90
diffX
diffY
latitudes
").
-180
longitudes
90:
lon:
longitude:
180:
lonDV
latDV
geoHash1
geoHash2
gh2DV
h2Pair
gh1DV
h1Pair
p1D
p1DV
p2D
convertToRads
p2DV
convertToRadians
measure
str2DV
str1DV
dist="
"sourceLocation"
"fieldType"
getFieldTypeName
suggList
WhitespaceAnalzyer
dictionary:
countLimit
strDistanceName
"Unparseable
getSourceLocation
"distanceMeasure"
getCharacterEncoding
WORD_FIELD_NAME
loadExternalFileDictionary
characterEncoding
"characterEncoding"
spellings"
THRESHOLD_TOKEN_FREQUENCY
luceneIndexDir
"thresholdTokenFrequency"
initSourceReader
":|\\d+)))[\\p{L}_\\-0-9]+"
"\\u037f-\\u1fff"
"\\u2c00-\\u2fef"
"\\xf8-\\u02ff"
NMTOKEN
"(["
SURROGATE_PAIR
"\\u200c-\\u200d"
"\\ufdf0-\\ufffd"
"\\u2001-\\ud7ff"
"\\u0370-\\u037d"
"\\u203f-\\u2040"
QUERY_REGEX
"\\u2070-\\u218f"
"\\-.0-9\\xb7"
"\\xc0-\\xd6"
"\\p{Cs}{2}"
"\\u0300-\\u036f"
"]|"
NAMESTARTCHAR_PARTS
"\\xd8-\\xf6"
"A-Z_a-z"
ADDITIONAL_NAMECHAR_PARTS
"\\uf900-\\ufdcf"
"(?:(?!("
tokenFrequency
startRow
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/tst/OldRequestHandler.java
OldRequestHandler
endRow
numRows
OldRequestHandler.java
"currDate"
"myNullVal"
queryString"
name..."
"testval1"
checks
"epoch"
a&b<c&"
results2
TestRequestHandler.java
allResults
999999999999L
"myDouble"
"myResult"
"myInt"
TestRequestHandler
"testarr1"
both3
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/tst/TestRequestHandler.java
"myFloat"
"myLong"
"myBool"
"&wow!
escaping:
rrr2
"testname1"
1e100d
"myStr"
1.414213562f
"myIntArray"
both2
results3
"myDoc"
"myNamedList"
storedId
",allowDups="
",overwriteCommitted="
",overwritePending="
",waitFlush="
",expungeDeletes="
"commit(optimize="
",waitSearcher="
"query=`"
",fromPending="
",fromCommitted="
'`'
addNoOverwriteNoDups
addConditionally
deleted:"
DirectUpdateHandler.java
DirectUpdateHandler
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/update/DirectUpdateHandler.java
12:21:22Z
openSearcher
mergeIndexes.
deleteInIndex
overwriteBoth
totDeleted
805774
numPending
"DirectUpdateHandler"
pset
existsInIndex
rollback.
combo:"
"DirectUpdateHandler
numErrorsCumulative
"optimizes"
uncommited
"adds"
"end_mergeIndexes"
DOC_COMMIT_DELAY_MS
madeIt
rwl
didCommit
iwAccess
824380
ctime
docsSinceCommit
lastAddedTime
deleteByQueryCommandsCumulative
numDocsPending
newScheduledThreadPool
commitMaxTime
INDEX"
ReadWriteLock
"cumulative_deletesById"
"deletesById"
"REMOVING
"cumulative_adds"
schedule
"autocommits"
maxDocs"
error..."
ReentrantReadWriteLock
"end_commit_flush"
efficiently
rollbackWriter
mergeIndexesCommands
"rollbacks"
"closing
"end_rollback"
"autocommit
expungeDeleteCommands
scheduleCommitWithin
getDelay
"ms;
addedDocument
"AutoCommit:
"docsPending"
readLock
maxTime"
optimizeCommands
"closed
"cumulative_errors"
didRollback
http://svn.apache.org/repos/asf/lucene/solr/branches/newtrunk/solr/src/java/org/apache/solr/update/DirectUpdateHandler2.java
15:18:08Z
"DirectUpdateHandler2"
DirectUpdateHandler2.java
"deletesByQuery"
autoCommitCount
iwCommit
_scheduleCommitWithin
"cumulative_deletesByQuery"
deleteByQueryCommands
ScheduledFuture
delAll
startDoc
addSingleField
missingFields
second='"
fields:
"ERROR:unknown
binaryField
endDoc
destinationField
first='"
isBinaryField
farr
hasField
enabled:
commitLockTimeout
"mergePolicy"
"/maxBufferedDocs"
<mergeScheduler>[classname]</mergeScheduler>"
<mergePolicy>[classname]</mergePolicy>"
"mergeScheduler"
defaultsName
"deprecated
"/infoStream/@file"
"/mergeScheduler"
"/useCompoundFile"
"/lockType"
"/infoStream"
"/mergePolicy/text()"
"/mergeFactor"
atrs
"/luceneAutoCommit"
defaultDefaults
infoStreamEnabled
"/ramBufferSizeMB"
"/mergePolicy"
"/commitLockTimeout"
"/maxMergeDocs"
"indexDefaults"
"/writeLockTimeout"
"/mergeScheduler/text()"
"/termIndexInterval"
"/maxFieldLength"
TimeLoggingPrintStream
used."
"CONFIGURATION
'simple'"
lockType:
LEGACY_DIR_FACTORY
mergefactor
assuming
"SolrIndexWriter
rawLockType
LogMergePolicy.
"Use
underlyingOutputStream
DirectoryFactory."
autoFlush
getDateTimeInstance
policy's
doc#
optimizeCallbacks
postCommit:
"postOptimize"
commitCallbacks
removeAllExisting
postOptimize:
readableId
getIndexedIdOptional
parseEventListeners
deletes)"
numDeletes
"deleteByQuery"
"maxNumToLog"
LogUpdateProcessor
adds)"
Lookup3Signature
DIGESTER_FACTORY
digester
"UTF-8
RunUpdateProcessor
getSignatureClass
"signatureClass"
getSignatureField
overwriteDupes
signatureTerm
getSigFields
"org.apache.solr.update.processor.Lookup3Signature"
getOverwriteDupes
oo
signatureClass
sigFields
"signatureField"
signatureField
"overwriteDupes"
SignatureUpdateProcessor
sig
sigString
TokenComparator
TextProfileSignature
"quantRate"
minTokenLen
quantRate
"minTokenLen"
curToken
quant
profile
"updateRequestProcessorChain
processor"
"####TEARDOWN_START
"####SETUP_START
"####SETUP_END
offsetLeft
offsetRight
NEG_CHAR
base100toBase10
9999
ZERO_EXPONENT
tens
hundreds
0xcccd
wpos
base10toBase100
div10
outend
firstDigit
mul10
BitSetIterator
getMaxSize
setMaxSize
oldValues
newSlot
oldEntries
key2
getCharArr
explainOther
maxSnippets
highlightFormatterClass
highlightFields
"Multi
tz
zone
DAY_OF_WEEK_IN_MONTH
"MONTHS"
WEEK_OF_YEAR
ops
rounding
"HOURS"
DAY_OF_WEEK
"Unit
"SECONDS"
"MILLIS"
makeUnitsMap
"MINUTES"
Unit
"\\b|(?<=\\d)(?=\\D)"
recognized:
"MILLISECONDS"
"YEARS"
"Rounding
"DATE"
AM_PM
"MILLI"
WEEK_OF_MONTH
float:
int:
pf
isFrequent
HighFrequencyIterator
minNumDocs
thresh
getFragmenterX
SolrHighlighterX
DEFAULTS
getFormatterX
getHighlighterX
getMaxSnippetsX
HighlightingUtils
HIGHLIGHTER
ival
0x0fff
0x7fff
getRefcount
-jar
-Ddata=files
"SimplePostTool:
DATA_MODES
-Dcommit="
'<delete><id>42</id></delete>'\n"
"Examples:\n"
POST_ENCODING
warnIfNotExpectedResponse
disconnect
PostException
strings;
-Ddata=args
post.jar
setRequestProperty
solrUrl
-Ddata="
setRequestMethod
"args;
(POST
"1.2"
setDoInput
"COMMITting
"Shouldn't
Solr\n"
commandline\n"
charset="
executed.
commandline
"POSTing
DATA_MODE_STDIN
Property
postFiles
DATA_MODE_ARGS
happen:
"System
filesPosted
?):
-Durl="
changes.."
startIndexInArgs
"URL
setDoOutput
via
Properties...\n"
SimplePostTool
"http://localhost:8983/solr/update"
urlc
DATA_MODE_FILES
FATAL:
setUseCaches
postFile
name=\"status\">0</int>"
-Ddata=stdin
controlled
POSTing
DEFAULT_COMMIT
hd.xml\n"
tool:
posted
postData
whether
"port.
STDIN.\n"
to,
HttpURLConnection
POST??"
VERSION_OF_THIS_TOOL
encodings
getResponseMessage
setAllowUserInteraction
SOLR_OK_RESPONSE_EXCERPT
DEFAULT_POST_URL
Solr:
*.xml\n"
'url'
'data'
DEFAULT_DATA_MODE
"Content-type"
stdin
fatal
options
pipe
"stdin"
URL="
ProtocolException
These\n"
"are
funcs
highligher
dit
"parsedquery"
"querystring"
getNumberParam
getExplainList
"\\s+[+-](?:\\s*[+-]+)+"
IdentityRegenerator
msm
calc
addOrReplaceResults
corrresponding
"otherQuery"
pClazz
ignoring"
"Replacing
"\\s+[-+\\s]+$"
otherQueryS
parseFuncs
sortE
CONSECUTIVE_OP_PATTERN
doSimpleQuery
"parsedquery_toString"
explainList
DANGLING_OP_PATTERN
fieldFilter
fieldLists
"\\^"
getBooleanParam
optionalClauseCount
strid
optionalClauses
parseQueryStrings
otherResults
specified,
"rawquerystring"
jarFiles
findMethodReturns
jar
jars
finder
malformed?"
methArgs
TOKENFILTER
TOKENIZERFACTORY
NoClassDefFoundError
findExtends
jarFile
TOKENFILTERFACTORY
"org.apache.lucene.analysis.TokenFilter"
"Finding
extenders
"org.apache.solr.analysis.TokenizerFactory"
FindClasses
JarFile
"org.apache.lucene.analysis.Tokenizer"
load:
isAnonymousClass
TOKENIZER
TOKENSTREAM
"create"
JarFile.toURL()
methName
parameterTypes
SuggestMissingFactories
isAbstract
JarEntry
"org.apache.lucene.analysis.TokenStream"
"WTF,
clazzes
"org.apache.solr.analysis.TokenFilterFactory"
bug?"
arguments)
"?!?
"Totally
deltype
"//result[@status=0]"
weird
validateAddDoc
xp
confFile
checkUpdateStatus
simpleTag
appendSimpleDoc
even"
UTF-8
"//result[@status="
"SolrCore.getOpenCount()=="
oldFiles
deleteList
getDefaultPackages
loadSingle
PluginInitInfo
defaultStr
pinfo
defaultPlugin
plugins:
preRegister
MapPluginLoader
NamedListPluginLoader
tce
lastTemplates
simplistic
TransformerProvider's
"newTemplates"
xsltStream
templates:"
appropriate
cacheExpires
lastFilename
used"
"newTransformer
XSLT
cacheLifetimeSeconds
Templates:"
"compiling
"getTransformer"
mechanism
scenarios,
"xslt/"
"(^|[,
"*,
setSortField
scorePattern
setShowDebugInfo
showDebugInfo
removeSort
removeVal
])score"
toSortString
addValueToParam
setResponseParser
setPath
addBean
3371703521752000294L
method.
mapValue
superClazz
getGenericType
infocache
ex2
'get'
DocField
storeType
"\\.*"
"Object,
AccessibleObject
"Allowed
ParameterizedType
dynamicField
isAnnotationPresent
getDocFields
allValuesMap
isContainedInMap
getGenericComponentType
List"
getActualTypeArguments
dynamicFieldNamePatternMatcher
docField
allValuesList
isList
GenericArrayType
getRawType
Object[]
storeName
gname
collectInfo
solrDocList
rawType
parameterizedType
ElementType
Target
"#default"
RUNTIME
RetentionPolicy
Retention
"/update/javabin"
BAOS
binarystream"
getbuf
_allowCompression
isMultipart
InputStreamRequestEntity
1.
_followRedirects
send
recommended
setContentCharset
solrj.
setParser
getConnectionManager
1.0"
PartBase
wparams
_httpClient
_maxRetries
requestWriter
maxRetries
setAllowCompression
HttpException
useMultiPartPost
connections
streams"
NoHttpResponseException
beanIterator
setBaseURL
streams!"
sendData
HttpConnectionManager
"Solr["
Retries
followRedirects
_invariantParams
"GET
MultipartRequestEntity
outputStream
Maximum
getResponseCharSet
1."
"CommonsHttpSolrServer:
lengthOfData
getInvariantParams
transferEncoding
charSet
tries
setMaxRetries
allowCompression
baseURL
StringPart
aliveCheckExecutor
attempts
getAliveCheckRunner
moveAliveToDead
"Alive
zombieServer
startAliveCheckExecutor
ServerWrapper
lastUsed
addIfAbsent
aliveServers
CHECK_INTERVAL
SolrServers
serverWrapper
lastChecked
"positive,
removeSolrServer
zombieServers
checkAZombieServer
addSolrServer
failedPings
solrServerUrls
checkLock
runner:
"interrupted"
{}"
"<stream>"
updateUrl
queueSize
runnerLock
remainingCapacity
Queue
"blocking
"<commit
"</stream>"
waitSearcher=\"%s\"
tmpLock
LinkedBlockingQueue
commit/optimize"
"sending:
blockUntilFinished
"finished:
waitFlush=\"%s\"
Runner
"<optimize
doc!
readDocuments
KnownType
attribute:
LST
'lst',
getLocation
RESULT
handled!"
"shoudl
"already
response!"
BOOL
type!
element,
"requires
'arr',
result.
getEventType
"branch
"not:"
setWaitSearcher
isWaitFlush
isWaitSearcher
getAction
setWaitFlush
addContentStream
setOtherCoreName
Persist
setIndexDirs
otherCoreName
Create
setCoreName
setInstanceDir
getIndexDirs
aliasCore
specified!"
"/admin/cores"
unloadCore
setFileName
MergeIndexes
createCore
"/analysis/document"
listToCommaDelimitedString
fieldNameValue
"/analysis/field"
fieldTypeValue
solrParamsToNamedList
listToSolrInputDocument
"delById"
namedListToSolrParams
delById
solrInputDocs
delByQ
docIter
doclist
"delByQ"
solrInputDocumentToList
isShowSchema
showSchema
setNumTerms
getDelegate
"/admin/ping"
commitWithin=\""
"<query>"
deleteI
deleteQ
phaseEntry
addTokenInfo
documentEntry
fieldEntry
fieldAnalysisByFieldName
addFieldAnalysis
documentKey
getIndexPhasesByFieldValue
valueEntry
valueNL
indexPhasesByFieldValue
documentAnalysisByKey
setCount
_name
_ff
_gap
getAsFilterQuery
getGap
_end
_count
_values
indexNL
fieldNameNL
fieldTypesNL
analysisNL
getFieldTypeAnalysisCount
fieldTypeNL
analysisByFieldName
queryNL
analysisByFieldTypeName
getAllFieldTypeAnalysis
fieldNamesNL
getAllFieldNameAnalysis
sum:"
stddev
max:"
stddev:"
ev
missing:"
mean:"
count:"
min:"
key:
getMean
vnl
fieldTypeInfo
fldTypes
"cacheableFaceting"
FieldTypeInfo
getIndexDirectory
isCacheableFaceting
theFields
parseFlags
flagStr
getMaxDoc
getDistinct
cacheableFaceting
getNumDocs
extractFacetInfo
_debugMap
fieldMap
minsize
_facetQuery
extractDebugInfo
_highlighting
extractHighlightingInfo
_facetDates
getSortValues
getDebugMap
_header
getExplainMap
getHighlighting
_termsInfo
removeFacets
getFacetDate
fnl
_spellInfo
extractStatsInfo
_highlightingInfo
_sortvalues
getLimitingFacets
_termsResponse
extractSpellCheckInfo
_results
_spellResponse
extractTermsInfo
_debugInfo
_limitingFacets
_fieldStatsInfo
_explainMap
_facetFields
getRequestUrl
setRequestUrl
requestUrl
getQTime
alternatives
correctlySpelled
alternativeFrequencies
getSuggestionMap
sugg
getCollatedResult
getSuggestionFrequencies
originalFrequency
suggestionMap
itemList
amp
boost=\""
toXML
DateParseException
controlRsp
randVals
controlJetty
rfloat
getRandValues
vala
keya
verifyStress
uniqueValues
RandVal
dataDirName
flagsa
shardCount
"Mismatched
destroyServers
"[id="
createServers
aSkipped
".maxScore"
"rnd_b"
".size()=="
testDistribSearch
valb
".numFound"
flagsb
posb
rdate
subDir
ORDERED
posa
".maxScore
numShards
"]==null"
".size()"
nameb
namea
jettys
controlClient
responses:\n"
stress
missing)"
nServers
fixShardCount
stress..."
"control"
testDir
serverNumber
".length:"
"skipped="
"localhost:"
compareResponses
(unordered
compare1
bSkipped
"shard"
uval
RandDate
getRandFields
rdouble
NOW-2YEARS]"
name=\"id\">2</field>"
"keywordtok:\"How
semicolon"
testDateMath
testNotLazyField
"test_postv"
"timestamp:[NOW-10MINUTES
testRemoveDuplicatesTokenFilter
val_s:aa"
picked
"backslash
C.o.w.
NOW+2HOURS]"
"bday:[NOW-1MONTH
"1147483647"
escaping
"//str[.='How
testPatternReplaceFilter
mkstr
broWn-ish
"\"1\""
"Mergefactor
NOW]"
"test_notv"
testMultipleUpdatesPerAdd
absolute
stemmed
BasicFunctionalityTest
name=\"text\">hello</field></doc>"
nOw
"Goodbye,Now"
testDefaultFieldValues
"array"
matches?"
What's
"multiDefault:a"
date#1"
"testWriter"
intDefault"
testLocalSolrQueryRequestParams
"patternreplacefilt:Up"
"test_offtv"
testSolrParams
"NOW+2YEARS"
date#3"
friend!"
"Hello,There"
"//int[@name='id'][.='42']"
work?"
"intDefault:[3
"555"
"make
arrayParams
"NOW-30MINUTES"
"text:hello"
"foo_ignored"
"test_basictv"
"\"2\""
"bt"
"NOW+30MINUTES"
stringParams
"patternreplacefilt:My__fine_feathered_friend_"
"patterntok:Goodbye"
"multiDefault"
"intDefault:42"
"NOW/HOUR"
"aa;bb"
"//date[@name='timestamp']"
birds"
"dedup"
"test_hlt"
fine-feathered
luf
"quote
name=\"id\">2</field></doc></add>"
"//arr[@name='multiDefault']"
"bday:["
"\"value\""
"id,title"
"bday:[*
"patternreplacefilt"
"<doc><field
id:42"
"defaults
"4057"
"TV"
99]"
boost=\"2.0\"
"patterntok"
"YYY"
"f.field1.i"
date#4"
july4
111]"
doc#42"
"/SECOND
"id,title,test_hlt"
Doc?"
date#2"
"title:keyword"
"2
"BOO!"
"SSS"
testLazyField
Up"
testConfigDefaults
ischema
"\"quoted\""
"/SECOND]"
"bday:[NOW+1MONTH
testIgnoredFields
testRequestHandlerBaseException
Up
name=\"id\">1</field></doc><doc><field
boolean?"
timestamp"
"bday:[NOW
"patterntok:Hello"
testXMLWriter
"/SECOND+1SECOND]"
"DDD"
"patternreplacefilt:__What_s_Up_Doc_"
multiDefault"
?\""
"everthing
"/MINUTE+1MINUTE
boost=\"2.0\"><field
testTermVectorFields
testFieldBoost
"bar_ignored:yo
testKeywordTokenizerFactory
"intDefault"
"multiDefault:muLti-Default"
subword:wi+=fi"
name=\"val_s\">apple</field></doc></add>"
"num_l:[-9223372036854775808
"num_l:9223372036854775807"
"<delete><id>45</id></delete>"
[2005-08-29
name=\"id\">101</field><field
com.caucho.util.ThreadPool.run(ThreadPool.java:423)
"num_l:[-1
"num_i:[-2147483648
"//doc[5]/int[.='1000']"
<id>44</id>
"<delete><query>id:105</query></delete>"
com.caucho.server.webapp.WebAppFilterChain.doFilter(WebAppFilterChain.java:163)
"<delete><id>44</id></delete>"
subword:1"
"-val_s:[a
syn:bar"
name=\"xaaa\">12321</field></doc></add>"
ee\"~100"
com.caucho.server.port.TcpConnection.run(TcpConnection.java:363)
syn:a"
+textgap:\"aa
I'm
"//str[.='Yonik']
name=\"num_l\">10</field></doc></add>"
name=\"a_i\">1</field><field
"//doc[1]/long[.='9223372036854775807']
overwritePending=\"true\"
subword:\"78\""
"//doc[1]/long[.='-9223372036854775808']
syn:c1"
name=\"nullfirst\">A</field></doc></add>"
name=\"when_dt\">2005-03-18T01:14:34Z</field><field
name=\"subword\">'I.B.M'
overwriteCommitted=\"true\"><doc><field
name=\"num_sd\">-1e100</field></doc></add>"
subword:\"'I.B.M.'\""
name=\"subword\">BAZ</field></doc></add>"
+teststop:world"
subword:\"e
1010];
top;"
syn:b2"
"<delete><query>val_s:[*
org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)
name=\"val_s\">pearing</field></doc></add>"
"//long[.='9223372036854775807']"
name=\"num_sd\">1e-100</field></doc></add>"
"num_i:[-10
3\""
subword:\"power-shot\""
KDF-E50A10</field></doc></add>"
name=\"id\">106</field><field
"//doc[2]/int[.='1001']
"//str[.='abc123']"
"val_s:[apple
name=\"id\">107</field><field
name=\"b_i\">50</field></doc></add>"
name=\"id\">101</field></doc></add>"
673808
"id_i:1000;
subword:IBM"
y\""
wolf"
"*[count(//@name[.='id'])=1]"
"<delete><id>102</id></delete>"
+textgap:\"bb
"*,score
please.</field></doc></add>"
It's
java.lang.Thread.run(Thread.java:595)
name=\"val_s\">port</field></doc></add>"
name=\"id\">11</field><field
resin.stdout
SD500
name=\"num_l\">2</field></doc></add>"
solr.DirectUpdateHandler2.overwriteBoth(DirectUpdateHandler2.java:317)
673806
"//doc[3]/int[.='1000']"
subword:\"SD-500\""
name=\"num_sf\">NaN</field></doc></add>"
name=\"textgap\">aa
"*[count(//doc/*)>=3]
"//*[@name='shouldbestored']"
"<delete><query>id:[100
"//*[@start='2']"
"*[count(//doc)=10]"
successful
view</field></doc></add>"
red"
riding
name=\"t_name\">cats</field></doc></add>"
xY
"//doc[4]/int[.='100']
"//int[.='2147483647']"
M\""
*}"
subword:\"10
name=\"a_i\">10</field></doc></add>"
title_lettertok:Now"
name=\"id\">104</field><field
"<delete><query>title:yonik4</query></delete>"
subword:\"SonyKDFE50A10\"~10"
name=\"subword\">foo
"id:10
673830
nullfirst"
"not(//*[@name='shouldbeunstored'])"
name=\"text\">foo
name=\"id_i\">1001</field><field
b]"
r\""
9223372036854775807]"
b])"
"//@numFound[.='1']"
name=\"num_sf\">0</field></doc></add>"
"*[count(//doc/*)>=12]"
performing.
"title:yonik3"
subword:\"foo
z];val_s
"//doc[1]/int[.='-2147483648']
subword:I--B--M"
schema.
name=\"id\">105</field><field
A's,B's,C's</field></doc></add>"
asc;"
name=\"num_sf\">-1e20</field></doc></add>"
schema:
"//doc[last()]/int[.='15']"
subword:\"SD500\""
name=\"num_l\">15</field></doc></add>"
"//doc[last()]/float[.='NaN']"
name=\"num_sf\">-Infinity</field></doc></add>"
name=\"val_s\">pearson</field></doc></add>"
"//*[@numFound='2']
perform
"id_i:[1000
name=\"num_l\">-1</field></doc></add>"
name=\"id_i\">1003</field><field
"//result[@maxScore>0]"
"teststop:stopworda"
name=\"arr_f\">1.414213562</field><field
"val_s:{a
"//doc[3]/int[.='50']
name=\"num_sf\">1.4142135</field></doc></add>"
"id:44;id
solr.SolrCore.update(SolrCore.java:795)
10.4
"num_sf:[-1
name=\"xaa\">mystr</field><field
subword:IBM's"
name=\"subword\">FooBar10</field></doc></add>"
name=\"num_sd\">Infinity</field></doc></add>"
org.apache.lucene.analysis.LowerCaseFilter.next(LowerCaseFilter.java:32)
name=\"id_i\">1002</field><field
org.apache.lucene.index.DocumentWriter.addDocument(DocumentWriter.java:81)
"//doc[1]/int[.='50']
syn:foo"
"val_s:p?a*"
syn:baz"
"val_s:{*
"num_sf:\"NaN\""
"//doc[4]/int[.='100']"
"//str[.='pear']"
name=\"num_l\">-9223372036854775808</field></doc></add>"
10A-B</field></doc></add>"
turns
subword:\"PowerShotSD500-7MP\""
"(val_s:[apple
"//doc[last()]/int[.='2147483647']"
"//doc[last()]/float[.='-Infinity']"
"*//doc[1]/str[.='banana']"
cc\""
val_s:[b
val_s:[a
syn:b"
cc</field><field
text:\"foo\""
name=\"copy_t\">Copy
a]"
name=\"fname_s\">Yonik</field><field
"num_i:2147483647"
"val_s:[a
8\""
"id:44;a_i
5-T
"num_l:\"-9223372036854775808\""
val_s:[*
+shouldbestored:hi"
"val_s:[bear
blue"
title:Now"
"//float[.='Infinity']"
673809
name=\"pi_f\">3.1415962</field><field
subword:www.yahoo.com"
"stopfilt:\"AnD\""
673807
673804
673805
Infinity]"
"//float[.='1.4142135']
"//str[.='and
name=\"val_s\">banana</field></doc></add>"
"//doc[3]/int[.='100']
.='cats']"
subword:\"powershot\""
z])"
"//@numFound[.='1']
name=\"syn\">foo</field></doc></add>"
name=\"val_s\">apple</field><field
name=\"b_i\">100</field></doc></add>"
"val_s:[*
name=\"num_i\">1</field></doc></add>"
name=\"id\">103</field></doc></add>"
name=\"subword\">10FooBar</field></doc></add>"
coming
name=\"id\">44</field><field
subword:\"ok\""
name=\"id_i\">1000</field><field
name=\"a_i\">-1</field></doc></add>"
name=\"sindsto\">abc123</field></doc></add>"
name=\"id\">12</field><field
name=\"num_sf\">Infinity</field></doc></add>"
"//*[@start='0']"
engineer</field><field
"//str[@name='t_name'][.='cats']"
"<delete><query>t_name:cat</query></delete>"
name=\"sind\">abc123</field></doc></add>"
"//double[.='Infinity']"
name=\"gack_i\">51778</field><field
PowerShot
solr.analysis.WordDelimiterFilter.addCombos(WordDelimiterFilter.java:349)
"//doc[4]/int[.='50']
fromPending=\"true\"
-val_s:[b
subword:\"foobar\""
apple]
"//int[.='-2147483648']"
http://c12-ssa-dev40-so-mas1.cnet.com:5078/select/?stylesheet=q=docTypeversion=2.0start=0rows=10indent=on
"//doc[last()]/long[.='-9223372036854775808']"
stopworda
name=\"num_l\">-987654321</field></doc></add>"
"//float[.='NaN']"
name=\"syn\">b</field></doc></add>"
subword:I.B.M"
com.caucho.server.dispatch.ServletFilterChain.doFilter(ServletFilterChain.java:99)
apple]"
"//doc[1]/int[.='1002']"
*]</query></delete>"
"+id:45
A+B\""
"num_sf:Infinity"
name=\"syn\">a</field></doc></add>"
"sind:abc123"
"*//doc[1]/str[.='pear']
673825
ff</field></doc></add>"
javax.servlet.http.HttpServlet.service(HttpServlet.java:154)
"*[count(//@name[.='title'])=1]"
673820
"<delete><query>id:[10
name=\"a_i\">0</field></doc></add>"
"//doc[2]/int[.='50']"
./str='yonik4']
text:\"bar\""
<doc>
foo\"~2"
name=\"ssto\">and
abcde12345
673829
com.caucho.server.http.HttpRequest.handleRequest(HttpRequest.java:259)
"*[count(//@name[.='sind'])=0]
"num_sd:[-1
"id:44;num_sf
"id:44;num_sd
name=\"a_i\">15</field></doc></add>"
name=\"num_sd\">2</field></doc></add>"
val_s:\"aa\""
ride"
1010]</query></delete>"
"title:yonik4"
name=\"val_s\">pear</field></doc></add>"
bar</field></doc></add>"
"//doc[1]/double[.='NaN']
<delete>
"//long[.='-9223372036854775808']"
subword:\"bar
shot
name=\"num_sf\">2</field></doc></add>"
name=\"id\">102</field><field
subword:\"A's
"num_sd:Infinity"
name=\"subword\">FooBarBaz</field></doc></add>"
"*//doc[2]/str[.='banana']"
fromCommitted=\"true\"><id>44</id></delete>"
subword:\"wi
?</field></doc></add>"
name=\"shouldbestored\">hi</field></doc></add>"
name=\"num_i\">-987654321</field></doc></add>"
"<delete
subword:wifi"
name=\"title\"
"//doc[last()]/double[.='-Infinity']"
pretty
name=\"num_sd\">-1</field></doc></add>"
"*[count(//doc)=1]
org.apache.lucene.analysis.StopFilter.next(StopFilter.java:98)
name=\"num_i\">2147483647</field></doc></add>"
"id:11
"id:12
"id:13
</doc>
name=\"id\">102</field></doc></add>"
name=\"description_t\">software
"id:44;"
E-3
name=\"b_si\">50</field></doc></add>"
"num_i:[0
110]</query></delete>"
"//arr[@name='title'][./str='yonik3'
1010]"
overwritePending=\"false\"><doc><field
name=\"val_s\">CCC</field></doc></add>"
"num_sf:\"-1e20\""
allowDups=\"true\"
overwriteCommitted=\"false\"
W2
+stopfilt:world"
name=\"num_i\">-2147483648</field></doc></add>"
subword:\"power
subword:\"foo/bar\""
name=\"num_sf\">-987654321</field></doc></add>"
"//doc[1]/int[.='100']
"//int[@name='gack_i'][.='51778']
+textgap:\"dd
name=\"subword\">http://www.yahoo.com</field></doc></add>"
subword:10"
Cows</field></doc></add>"
name=\"id\">103</field><field
"id:44;num_i
name=\"id_i\">1005</field><field
name=\"subword\">Mark,
name=\"val_s\">BBB</field></doc></add>"
"num_sd:[-Infinity
name=\"shouldbeunindexed\">hi</field></doc></add>"
"//doc[4]/int[.='50']"
"num_sd:\"-1e100\""
t\""
text:\"foo
E-50-A-10\""
name=\"num_sf\">-999999.99</field></doc></add>"
solr.EnglishPorterFilter.next(TokenizerFactory.java:163)
syn:c"
text:pretty"
com.caucho.util.ThreadPool.runTasks(ThreadPool.java:490)
name=\"num_sd\">-Infinity</field></doc></add>"
name=\"subword\">Canon
"//float[.='1.4142135']"
name=\"num_sd\">NaN</field></doc></add>"
"*[count(//doc)=6]
"//float[.='-Infinity']"
name=\"id\">100</field><field
name=\"shouldbestored\">hi</field>
*6-Y-
solr.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:191)
"num_l:[-10
name=\"id\">105</field></doc></add>"
673819
673818
tons
"//str[.='apple']
subword:Sony"
673811
673810
673813
673812
673815
673814
673817
673816
subword:\"5
subword:\"4
"//doc[1]/float[.='NaN']
"num_sf:\"-Infinity\""
fi\""
"stopfilt:\"and\""
KDF
shouldbeunindexed
name=\"num_sd\">1.4142135</field></doc></add>"
"//*[@numFound='3']
name=\"num_i\">1234567890</field></doc></add>"
AnD
b_i
subword:\"bar\""
what's
7-8--
name=\"val_s\">DDD</field></doc></add>"
100]</query></delete>"
k\""
text:\"bar
name=\"id\">44</field></doc></add>"
subword:\"I
name=\"subword\">foo-bar</field></doc></add>"
"//doc[last()]/long[.='9223372036854775807']"
testABunchOfConvertedStuff
subword:\"w
subword:\"x
"//doc[1]/double[.='-Infinity']
subword:\"o
subword:\"SD500-7MP\""
name=\"subword\">Sony
"num_i:\"-2147483648\""
"//doc[2]/int[.='1000']
name=\"ego_d\">1e100</field><field
name=\"num_l\">1</field></doc></add>"
title_stemmed:cow"
top,
name=\"num_i\">10</field></doc></add>"
name=\"shouldbeunstored\">hi</field></doc></add>"
name=\"id_i\">1004</field><field
hood</field></doc></add>"
</add>"
cat]"
dd\""
name=\"val_s\">AAA</field></doc></add>"
org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:307)
name=\"id\">44</field>
+shouldbeunstored:hi"
"//*[@numFound='1']
<commit
name=\"num_i\">15</field></doc></add>"
subword:Q"
"*
apple])"
"<commit></commit>"
1--
name=\"text\">red
"fname_s,arr_f
-1]"
B's-C's\""
1002];
"*[count(//doc)=2]
nullfirst
name=\"id\">13</field><field
"//float[@name='score']
"num_sd:\"NaN\""
"//double[.='-Infinity']"
name=\"num_i\">2</field></doc></add>"
"//doc[last()]/int[.='-1']
name=\"arr_f\">.999</field></doc></add>"
"//doc[last()]/int[.='-2147483648']"
"val_s:p*"
name=\"num_sd\">0</field></doc></add>"
"<delete><query>id_i:[1000
desc;"
asc,b_si
javax.servlet.http.HttpServlet.service(HttpServlet.java:92)
name=\"id\">10</field><field
subword:WiFi"
"id:44;
"//int[@name='xaaa'][.='12321']"
"//@numFound[.='0']"
"num_sf:[-Infinity
"title:yonik5"
"//doc[2]/int[.='100']"
name=\"teststop\">world
Ok
solr.analysis.WordDelimiterFilter.next(WordDelimiterFilter.java:325)
name=\"text\">dd
name=\"num_sd\">-999999.99</field></doc></add>"
"//@numFound[.='2']"
"//@maxScore
syn:b1"
name=\"syn\">c</field></doc></add>"
name=\"iq_l\">10000000000</field><field
7MP</field></doc></add>"
b_si
"val_s:a*"
pear]"
subword:\"foo\""
name=\"a_i\">10</field><field
name=\"subword\">10</field></doc></add>"
"num_sd:\"1e-100\""
"//double[.='NaN']"
boar]"
"val_s:a*p*"
name=\"id_i\">1006</field><field
+textgap:\"cc
pear}"
name=\"val_s\">appalling</field></doc></add>"
name=\"num_sf\">-1</field></doc></add>"
"//*[@name='shouldbeunindexed']"
Now4
syn:aa"
name=\"num_l\">9223372036854775807</field></doc></add>"
boost=\"2\">yonik4</field></doc></add>"
wolf</field></doc></add>"
subword:foobar"
"//str[@name='t_name'
"*[count(//doc)=7]
"//doc[1]/float[.='-Infinity']
2147483647]"
val_s:[apple
subword:\"IBM's-ABC's\"~2"
name=\"text\">big
name=\"stopfilt\">world
solr.DirectUpdateHandler2.doAdd(DirectUpdateHandler2.java:170)
"//str[@name='xaa'][.='mystr']
"//doc[3]/int[.='1001']"
subword:\"7
"*[count(//doc/*)=13]"
;"
subword:\"6
subword:\"Sony
org.apache.lucene.index.DocumentWriter.invertDocument(DocumentWriter.java:143)
ConvertedLegacyTest
version</field></doc></add>"
ee\"~90"
"//doc[1]/int[.='2147483647']
"//doc[1]/int[.='15']
dd\"~100"
b]
"stopfilt:\"AND\""
name=\"text\">aa
copy_t:pretty"
syn:c2"
allowDups=\"true\"><doc><field
KDF-E50A10\""
name=\"subword\">--Q
name=\"textgap\">dd
title:cow"
"sindsto:abc123"
subword:IBM'sx"
solrserver.SolrServlet.doPost(SolrServlet.java:71)
name=\"b_si\">100</field></doc></add>"
name=\"val_s\">aa;bb</field></doc></add>"
+val_s:[b
build:
com.caucho.server.cache.CacheFilterChain.doFilter(CacheFilterChain.java:188)
asc,b_i
java.util.ArrayList.RangeCheck(ArrayList.java:547)
name=\"num_i\">-1</field></doc></add>"
allowDups=\"false\"><doc><field
"+id:44
673824
"val_s:{apple
673826
673827
java.lang.IndexOutOfBoundsException:
673821
673822
673823
java.util.ArrayList.get(ArrayList.java:322)
673828
500\""
subword:yahoo"
"t_name:cat"
ff\""
name=\"title\">How
"num_sd:\"-Infinity\""
"*//doc[1]/str[.='apple']"
"//doc[1]/int[.='-1']
"//doc[last()]/double[.='NaN']"
//doc/float[@name='score']"
"//str[.='banana']"
name=\"here_b\">true</field><field
"id:44;num_l
"*[count(//doc)=7]"
subword:\"xy\""
</delete>"
name=\"num_i\">0</field></doc></add>"
subword:http\\://www.yahoo.com"
name=\"title\">yonik3</field><field
problem!
"*[count(//doc)=3]
name=\"nullfirst\">Z</field></doc></add>"
(val_s:[b
(val_s:[a
name=\"num_l\">1234567890</field></doc></add>"
name=\"num_l\">0</field></doc></add>"
(val_s:[p
a_i
name=\"id\">45</field><field
+text:\"cc
4R
text:cow"
15:11:38.375]
com.caucho.server.dispatch.ServletInvocation.service(ServletInvocation.java:208)
"+val_s:[a
"id:44
"//@numFound[.>'0']"
name=\"subword\">Wi-Fi</field></doc></add>"
"subject:hell
"minimum
"//result/doc[1]/int[@name='id'][.
"//result/doc[3]/int[@name='id'][.='8675309']"
"//result/doc[3]/int[@name='id'][.='42']"
DisMaxRequestHandlerTest
Chick"
='42']"
"//result/doc[1]/int[@name='id'][.='666']"
Galaxy"
"movie"
galaxy"
chorus
w/o
stuff
testOldStyleDefaults
"*[count(//lst[@name='t_s']/int)=3]"
"song"
traveling
p_bool
"\\(subject:hell\\s*subject:cool\\)"
qf"
Cool
"//result/doc[2]/int[@name='id'][.='8675309']"
"Jenny"
"subject:hell\\s*subject:cool"
"subject:hell^400"
matching,
Omen"
scary
"iind"
"//lst[@name='t_s']/int[@name='movie'][.='1']"
"explicit
"//result/doc[1]/int[@name='id'][.='42']"
"8675309"
traveling"
"cool
"666"
doTestSomeStuff
"features_t"
memorable
subject:cool"
"dismaxNoDefaults"
"Wikedly
"//lst[@name='facet_fields']/lst[@name='t_s']"
"relying
"//lst[@name='t_s']/int[@name='book'][.='2']"
"87.9"
"traveling
"Most
Guide
param"
"cool"
"id:9999"
"97.3"
Ever"
Hiker's
"99.45"
Hot
"//result/doc[2]/int[@name='id'][.='666']"
"guide"
"//result/doc[2]/int[@name='id'][.
chick\""
"subject:cool^4"
"77"
"//result/doc[2]/int[@name='id'][.='42']"
cross
hell"
"Hitch
ALTQ"
testExtraBlankBQ
populate
Boring
"id:666"
testSimplestParams
jenny"
='666']"
"\"cool
"/str[@name='handler'][.='org.apache.solr.handler.StandardRequestHandler']"
testExplicitEchoParams
EchoParamsTest
testDefaultEchoParams
testAllEchoParams
"/lst[@name='params']/str[@name='fl'][.='implicit']"
"explicit"
"/lst[@name='params']/str[@name='wt'][.='xml']"
"/response/lst[@name='responseHeader']"
HEADER_XPATH
"not(//lst[@name='params'])"
"/lst[@name='params']"
testDefaultEchoParamsDefaultVersion
"/int[@name='status']"
"//lst[@name='responseHeader']"
testAllConfiguredHandlers
MinimalSchemaTest
"Yonik"
"UniqueKey
"//int[@name='numTerms'][.='5']"
"Hoss"
"//null[@name='defaultSearchField']"
"//int[@name='numDocs'][.='2']"
w/handler:
"subject:Yonik"
"luke
testSimpleQueries
"project"
"//null[@name='uniqueKeyField']"
"//str[@name='id'][.='4056']"
"solr/conf/schema-minimal.xml"
"//str[@name='id'][.='4055']"
"solr/conf/solrconfig.xml"
handlerNames
testTrivialXsltWriter
testUselessWriter
"/response/lst[@name='responseHeader']/int[@name='QTime']"
testSOLR59responseHeaderVersions
"dummy.xsl"
"xslt"
"/response/responseHeader/QTime"
UselessOutputWriter
"/response/lst[@name='responseHeader']/int[@name='status'][.='0']"
"/response/responseHeader/status[.='0']"
OutputWriterTest
"useless
USELESS_OUTPUT
"useless"
"Who
"Mack
"or
Me
Again?"
yourself"
case,
testAdvanced
Hostetter</field>"
Me?"
addition
"</doc></add>"
Mack
options"
SampleTest
allowDups=\"true\">"
name=\"id\">4055</field>"
Daddy?
name=\"subject\">Hoss
Daddy"
"4059"
"//int[@name='id'][.='4055']"
classpath,
getClassesForPackage
cld
getResources
SolrInfoMBeanTest
pckgname
testCallMBeanInfo
deleteCore
schemaString
"###Starting
"###deleteCore"
"####initCore"
configString
"###Ending
"####initCore
"{!func}"
wall,
"matchesnothing"
"duplicate"
"missing_but_valid_field_t"
"mathematical
nlong
oddField
ex=b}"
"n_i"
1)"
"{!key=again
1.414f
1.414d
987
ex=a}all"
"invalid_field_not_in_schema"
ex=a}"
duplicate!"
"oh
"{!key=mykey
"foo_b"
horses"
models"
ex=a,b}"
"a_si"
"{!key=myall
analysis"
"oddField_s"
"{!tag=b}id:[3
"a_t"
lesson
ndouble
"{!key=myquick}quick"
"{!tag=t1}SubjectTerms_mfacet:(test
"SolrServerException
"odd
no,
missingField
invalidField
learned"
nint
put"
"{!ex=t1}SubjectTerms_mfacet"
7]"
ndate
"{!key=other
nfloat
eggs
"SubjectTerms_mfacet"
"{!tag=a}id:[1
TestPluginEnable
"solrconfig-enableplugin.xml"
"foo.foo2"
TestSolrCoreProperties
"solrconfig-solcoreproperties.xml"
setUpMe
solrJetty
"foo.foo1"
"//*[@numFound='20']"
2.33d
"tdate:1995-12-31T23\\:59\\:59.999Z"
"tdouble4:["
"tdate:[1995-12-31T23:00:59.999Z
"//float[@name='tfloat'][.='2519.9102']"
"tdouble4"
"Function
testTrieDateRangeSearch
NOW/DAY+5DAYS]"
2518.0]"
descending
"tlong:["
testTrieFacet_PrecisionStep
"_val_:\"sum(tdouble,1.0)\""
tint:[-9
"tint"
"//*[@numFound='10']"
tdouble:[*
"//float[@name='tfloat'][.='0.0']"
"tdouble:["
10l
"tdouble:[*
largestDate
"tfloat:124.44"
"//lst[@name='facet_fields']/lst[@name='"
5l
"//date[@name='tdate'][.='1995-12-31T23:59:59.999Z']"
"//long[@name='tlong'][.='2147483647']"
"//*[@numFound='9']"
"/DAY+"
"tfloat
"/DAY+5DAYS"
ascending
testTrieLongRangeSearch
-2]"
"tint:2"
"_val_:\"sum(tfloat,1.0)\""
testFacetField
testTrieFloatRangeSearch
tlong:[*
"//int[@name='tint'][.='9']"
"tdouble
"//int[@name='tint'][.='-10']"
"//long[@name='tlong'][.='2147483656']"
1995-12-31T23:09:59.999Z]
"1995-12-31T23:"
"tdate:[NOW/DAY
testTrieTermQuery
"tdouble:4.66"
31.11f
"tlong
":59.999Z"
9]
10]"
"tlong:[*
testFacetDate
"tdate"
1995-12-31T23:09:59.999Z]"
"tfloat:[0
"_val_:\"sum(tint,1)\""
"//double[@name='tdouble'][.='5.0036369184800005E9']"
testTrieIntRangeSearch
"']/int[@name='"
"tfloat"
"Range
"NOW/DAY+6DAYS"
"tdouble"
"schema-trie.xml"
"//double[@name='tdouble'][.='5.00363689751E9']"
tint:[*
TestTrie
"tdate:[*
"_val_:\"sum(tlong,1.0)\""
"tlong"
testTrieDoubleRangeSearch
"//*[@numFound='11']"
"tint:[-6
"_val_:\"sum(tdate,1.0)\""
testTrieDoubleRangeSearch_CustomPrecisionStep
checkPrecisionSteps
"//date[@name='tdate'][.='"
1995-12-31T23:04:59.999Z]"
"tint:[-10
"tint:[*
tint:[1995-12-31T23:00:59.999Z
"tdate
"tint
"tlong:2147483648"
"tint:[2
"factory"
CommonGramsFilterFactoryTest
"dog
"the_s"
"a_brown"
"d_like"
"brown_s"
nsf
testCaseSensitive
"s_s"
"quick_the"
"thing?"
TestFirstAndLastStopWord
"brown
CommonGramsFilterTest
"the_dog"
"dog_the"
"n_s"
testCommonGramsFilter
"the_quick"
testQueryReset
testFirstWordisStopWord
testOneWordQuery
testLastWordisStopWord
cgf
"of_the"
testOneWordQueryStopWord
"the_brown"
"s_n"
"n
"brown_the"
"s_cow"
"the_fox"
"fox_of"
"s
"The_s"
"cow_d"
"the_of"
"How_the"
"s_a"
testCommonGramsQueryFilter
"monster"
CommonGramsQueryFilterFactoryTest
testSettingSizeAndInject
DoubleMetaphoneFilterFactoryTest
filteredStream
DoubleMetaphoneFilterTest
testNonConvertableStringsWithInject
testAlternateInjectFalse
"KXFS"
"#$%@#^%&"
#$%@#^%&"
hello"
"12345
testSize4TrueInject
#$%@#^%&
testSize8FalseInject
testNonConvertableStringsWithoutInject
testSize4FalseInject
"HL"
"KSSK"
"Kuczewski"
EnglishPorterFilterFactoryTest
\u0393
proc.
"-->foo"
correctedOff
<other/>"
position:
hr<ef=aa<a>>
Buffer
ccc=\"ddddd\">
&Gamma;
":::
"<!--"
&#61;
"<!---
trimming"
href=\"#bar\">link</a>
class=\"foo\">this
&gt;
entity:
appendChars
<p>
<foo>
ggg=\"hhhh\"/>
"<EOS>"
Solr'
theChar
'Welcome
testGamma
</reserved>
tag"
&l
testMalformedHTML
strOff
-->"
’"
ffff
"reserved"
&Uuml;bermensch
testEntities
"Foundation."
"&nbsp;
"\u0393"
"<EOB>"
</close</a>"
"Forrest
&.
goldArray
&#
hr<ef=aa
testBufferOverflow
text</div>
comment"
href=\"http://lucene.apache.org/\">link</a>.
&#x393;"
<junk/>
testBuilder
</close
'Foundation.'
&lt;
"<div
<p>X
&nbsp;
removed"
--->
&zz
&#8217;"
you"
"?>"
instr."
>X
&#64;
dashes,
testReserved
"forrest"
"Welcome
testHTML
preserved:
<reserved
stripped
&g
&lt;foo&gt;
"Forrest"
<.
far:
testComment
testMoreEntities
"htmlStripReaderTest.html"
\u0393"
assertMsg
"<?"
HTMLStripCharFilterTest
"&Gamma;"
<!--
eeee
"ah<?>
\u00DCbermensch
&#33;
processBuffer
pseudo
&lt;junk/&gt;
doTestOffsets
&lt;.
&#40;
X<p>
LengthFilterTest
super-duper-trooper"
"ridding
stemming"
SnowballPorterFilterFactoryTest
"ridding"
"protwords.txt"
testProtectedOld
مَلكت
testNormalizer
"مَلكت"
normFactory
"الذين"
"أيمانكم"
"ذين"
TestArabicFilters
TestBrazilianStemFilterFactory
AB_AAB_Stream
testABAAB
AB_Q_Stream
TestBufferedTokenStream
testABQ
TestBulgarianStemFilterFactory
"Four"
testCapitalization
"Kitten"
thEre
"AnD"
"KiTTEN"
"Ryan"
"Third"
testForceFirstLetter
testMaxWordCount2
"One"
"kiTTEN"
"2nd"
"McK"
"helo
"And"
"McKinley"
"Mckinley"
testMinWordLength
"There"
"BIG"
TestCapitalizationFilter
"1st
testing"
BIG"
"Two"
testMaxWordCount
ryan"
"1st"
testKeepIgnoreCase
testMaxTokenLength
"Three"
"helo"
Ryan"
"silly"
TestChineseFilterFactory
"filter"
testFiltering
Is
silly
"是"
TestChineseTokenizerFactory
TestCJKTokenizerFactory
"国人"
"我是"
"中国"
"是中"
"Töne"
"DE"
u\u0308
"zh"
tsFull
turkish
A\u0308"
stream1
"Toene"
OE
tsUpper
StringMockSolrResourceLoader
"rules.txt"
germanOE
stream2
tailoredRules
assertCollatesToSame
"TESTING"
turkishLowerCase
upperCase
O\u0308"
u\u0308"
testSecondaryStrength
germanUmlaut
tsLower
baseCollator
turkishUpperCase
"Ｔｅｓｔｉｎｇ"
CASING"
fullWidth
"ı
testFullDecomposition
tsUmlaut
testCustomRules
WİLL
DIN5007_2_tailorings
W\u0049\u0307LL
oe
testBasicUsage
USE
ae
o\u0308
tailoredCollator
TURKİSH
getRules
tsOE
TestCollationKeyFilterFactory
casıng"
a\u0308
UE
tsHalf
halfWidth
TestCzechStemFilterFactory
payData
testEncoder
TestDelimitedPayloadTokenFilterFactory
testDelim
"payAttr
quick*0.1
red*0.1"
payAttr
"payData
quick|0.1
"the|0.1
payFloat
"the*0.1
red|0.1"
"soft"
"play"
testDecompounding
"softball"
"compoundDictionary.txt"
softball"
TestDictionaryCompoundWordTokenFilterFactory
TestDutchStemFilterFactory
"l'avion"
TestElisionFilterFactory
"frenchArticles.txt"
"avion"
TestFrenchStemFilterFactory
TestGermanStemFilterFactory
"Μάϊος
"μαιοσ"
TestGreekLowerCaseFilterFactory
ΜΆΪΟΣ"
"ত্‍
indicFilterFactory
TestHindiFilters
अाैर"
testHindiNormalizer
hindiFilterFactory
testIndicNormalizer
ecology-"
devel-\r\n\r\nop
"ecology-"
"ecological"
testHyphenatedWords
compre-\u0009hensive-hands-on
"develop"
"comprehensive-hands-on"
ecologi-\ncal"
TestHyphenatedWordsFilter
"ecologi-\r\ncal
testHyphenAtEnd
"keep-1.txt,
"keep-1.txt"
keep-2.txt"
TestKeepFilterFactory
testStopAndGo
EEE"
BBB
TestKeepWordFilter
DEFAULT_VERSION
"text20"
"matchVersion"
getDeclaredField
matchVersionField
ana1
TestLuceneMatchVersion
"textStandardAnalyzer20"
"textStandardAnalyzerDefault"
testStandardTokenizerVersions
"schema-luceneMatchVersion.xml"
"textDefault"
"escape
"\\u000"
alone."
"\\\"\n\t\r\b\f"
check."
"\\u0041\\u0042"
testParseString
characters"
"\\u0041"
TestMappingCharFilterFactory
"\\u123x"
"\\\\\\\"\\n\\t\\r\\b\\f"
c,d"
testMultiWordSynonyms
TestMultiWordSynonyms
testNGramFilter2
testEdgeNGramTokenizer3
testEdgeNGramTokenizer
testEdgeNGramFilter
testNGramTokenizer
"ready"
testEdgeNGramTokenizer2
testEdgeNGramFilter2
testEdgeNGramFilter3
testNGramTokenizer2
TestNGramFilters
testNGramFilter
"$1#$2"
testChain
"$1##$2"
"aa##bb###cc"
test1block1matchLonger
"(aa)\\s+(bb)\\s+(cc)"
TestPatternReplaceCharFilter
"$1#$2#$3"
BLOCK
"test."
test1block1matchSameLength
test2blocksMultiMatches
test1blockMultiMatches
test1block2matchLonger
$3"
aa.
"aa##bb"
"(aa)\\s+(bb)"
test1block1matchShorter
$2
bb"
---
"aa#bb"
"aa#bb#cc"
"aa."
"$1##$2###$3"
testReplaceByEmpty
"$1$2$3"
"$1
"c-"
testReplaceAll
"aabfooaabfooabfoob
testStripFirst
"-fooaabfooabfoob"
testStripAll
TestPatternReplaceFilter
"caaaaaaaaa$"
"a$"
testReplaceAllWithBackRef
testReplaceFirst
"(a*)b"
"fooaabfooabfoob"
"$1\\$"
"aa$fooaa$fooa$foo$"
"a*b"
"-foo-foo-foo-"
caaaaaaaaab"
"boo
"'bbb'
'ccc'"
"boo:and:foo"
testSplitting
:and:f"
"\"&uuml;\"
"[,;/\\s]+"
tsToString
"aaa:bbb:ccc"
\t\tccc
"\\p{Space}"
INPUT
G&uuml;nther
"--"
testOffsetCorrection
cfFactory
qpattern
input:
TestPatternTokenizerFactory
\"ü\""
"aaa--bbb--ccc"
"Günther"
"G&uuml;nther
"\\'([^\\']+)\\'"
mappingRules
'bbb'
TestPersianNormalizationFilterFactory
"B000"
"KKK"
"E034034"
"A000"
"ESKS"
TestPhoneticFilter
"B1"
easgasg"
"KK"
"E220"
testFactory
"PP"
"C3"
"A0"
"C000"
testAlgorithms
algName
assertAlgorithm
"ASKS"
"easgasg"
TestPorterStemFilterFactory
testDups
H
E"
testComplexDups
testSimpleDups
K"
testNoDups
J
TestRemoveDuplicatesTokenFilter
"\u0001owt"
th?ee
"two:\u0001eerh*
"\u0001x\uD834\uDD1Eis"
fiv*
"\u0001eno"
"one:one
two:\u0001eer?t
thr*e
"three:one
testQueryParsing
two:\u0001ru*f
"schema-reversed.xml"
?hree
t?ree
th*ee
*hree
two:\u0001x\uD834\uDD1Eis*"
"two:th?ee
two:\u0001ee*ht
two:\u0001eer*t
"\u0001eerht"
two:fiv*
testIndexingAnalysis
th?*ee
three:*hree
parserThree
one:\u0001x\uD834\uDD1Eis*"
+three:two
"two:\u0001eerh?
"two:thr*e
TestReversedWildcardFilterFactory
t*ree
+two:two
parserTwo
one:\u0001eerh*
+one:two
f*ur
one:fiv*
"si\uD834\uDD1Ex"
+two
parserOne
"*hree
"two:th?*ee
three:f*ur
ver*longtoken"
*si\uD834\uDD1Ex"
expectedOne
testReversedTokens
one:\u0001ru*f
"two:short*token
"\u0001txet"
"\u0001elpmis"
two:\u0001eerh*
"short*token
expectedThree
"two:one
"two:\u0001nekotgnol*rev"
three:*si\uD834\uDD1Ex"
expectedTwo
condText
si\uD834\uDD1Ex"
three:fiv*
testReversing
"elpmis"
TestReverseStringFilterFactory
"tset"
"силе"
caseFactory
"электромагнитной"
testLowerCase
"вместе"
TestRussianFilters
TestShingleFilterFactory
testNoUnigrams
testMaxShingleSize
testKeywordTokenizer
do?"
"Česká"
testStandardTokenizer
testLetterTokenizer
TestStandardFactories
"Česka"
testASCIIFolding
"Ceska"
"What"
"thing"
testStandardFilter
thing
"What's
testISOLatin1Folding
"What's"
"do?"
TestStopFilterFactory
"a5,5"
testOffsetBug
"a5,5
"TEST"
zoo"
testMapMerge
"c,5"
x,1,4,5"
testPositionIncrementsWithOrig
TestSynonymFilter
c2,2"
"a111"
"a3,3"
"a9"
testIncludeOrig
"zxcv"
"cc,100
a8,3
a7,4
"a3,3
"qq/ww/ee"
a111,100"
"a8"
"c,0"
"bb,100"
"b5,5"
testOverlap
a,1,2,3
"xc"
"b,0"
a9,2
"a,5"
"A5,5"
a10,2"
a11,2
"a2,2"
"c2"
a6,2"
"a5"
"a4"
"b5"
"A3"
assertTokenizesTo
"a10"
"A3,3"
"a7"
"b,5"
"zoo
"z
"b3,3"
"A5"
"a4,4
zoo
"a,0"
"$
v"
"a6"
posIncs
testMatching
"a,1,0,1
b=>a2"
testRead2waySynonymRules
c=>a3"
c=>b2"
"b=>b1"
"fg"
"a,b"
"RuntimeException
"a,b=>c"
testBigramTokenizer
a2,b"
"a=>b,c"
testReadMappingRules
"a=>a1"
"gh"
TestSynonymMap
"a=>b"
assertTokIncludes
getSubSynonymMap
"abcd=>efgh"
testRead1waySynonymRules
"a,b,c"
"a,b1
testInvalidMappingRules
"a=>b=>c"
testWordBreak
TestThaiWordFilterFactory
testTrim
whitespace
TestTrimFilter
"cCc"
testCasing
TestTurkishLowerCaseFilterFactory
"c#
"Роберт"
"Ерт"
"𠀀𠀀"
WilliAM"
"wdf_preserve:123"
"zoo-foo-123"
"\"~90"
"protectedsubword:(java)"
"protectedsubword"
"protectedsubword:(.net)"
posTst
"numberpartfail"
doSplit
"sol"
"۞test"
doSplitPossessive
"wdf_nocase:(hell
"foo-bar"
stemPossessive
"١٢٣٤"
"test's'"
testNoGenerationEdgeCase
"übelkeit"
JonEs"
"bar-baz"
Java/J2SE"
"largegap"
"456-bar-baz"
"\"~110"
"wdf_preserve:404-123*"
"NUTCH"
"camel"
.net
c++
"Роб"
"(foo,bar)"
"(übelkeit"
"zoo-foo"
"protectedsubword:(c#)"
"c++
"ra's"
am)"
"Case"
"HellO
testProtectedWords
"subword:(good
change"
"j2se
"protectedsubword:net"
"-bar-"
testOffsetChange
testRetainPositionIncrement
LargePosIncTokenFilter
"Java/J2SE"
solR"
"basic-split"
"РобЕрт"
wdf
testOffsetChange2
testOffsetChange3
testSplits
"wdf_preserve:404"
"/123/abc"
"übelkeit)"
se"
largegap
"/456/"
"222"
"numericsubword:(J2
"position
"qwe/456/"
"protectedsubword:c"
"+id:42
"wdf_preserve"
"case
+subword:\""
"LUCENE"
"numericsubword:(J2SE)"
jon)"
"70"
SE)"
"preserving
found?"
"144"
testOffsetChange4
"\u0e1a\u0e49\u0e32\u0e19"
"/123/"
NUTCH
"net
testIgnoreCaseChange
testPreserveOrignalTrue
"GoodBye
"protectedsubword:(c++)"
"camelCase"
"solR"
"ســـــــــــــــــلام"
"numericsubword"
testPossessives
"SOLR"
"LUCENE
"404-123"
"123.123.123.123"
".net
"-foo-"
testAlphaNumericWords
"aǅungla"
TestWordDelimiterFilter
"68"
Committing
"DocThread-"
DocThread
tserver
gserver
"Started
"done"
testMergeIndexes
"corec"
testMultiCore
"coreb"
"yup"
"cored"
"corefoo"
"core1
"corea"
gone"
"../../../example/solr/"
"conf/schema.xml"
"conf/solrconfig.xml"
(2),
"inStock:true"
(1),
books.csv"
"price:[5
"books.csv"
assertNumFound
"price:[2
inStockT
222"
"doc:
upres
"26.4"
"they
"timestamp_dt"
"sent
20.0
testContentStreamRequest
94.0
inStockF
(3),
"val_pi"
(5)]"
"id:id3"
"mailing_lists.pdf"
23.0
"expected:
(5),
testPingHandler
"h\u1234llo"
testLukeHandler
testAddRetrieve
"name:h\u1234llo"
had:
testFaceting
"[two
(1)]"
"features:two"
faceted
testCommitWithin
mailing_lists.pdf"
testStatistics
nums
testAddDelete
distribution"
"id:\""
(2)]"
testExampleConfig
"1112211111"
"inStock:false"
testSolrException
gotExpectedError
"test123"
SolrExceptionTest
sse
"http://localhost:11235/solr/"
"instock:false"
testFacetSort
"price2"
"a:b"
"testfield"
"qty"
"x,y"
"q=dog"
"instock:true"
"hl1"
testOrder
"hl2"
"hl3"
SolrQueryTest
testSolrQueryMethods
"x,y,score"
"a:c"
testSettersGetters
addHandler
">>>
setWar
JETTY
setContextPath
EMBEDDED
PRESS
STOP"
setServer
StartSolrJetty
SERVER,
8080
STARTING
"src/webapp/web"
TestBatchUpdate
testWithBinaryBean
testWithBinary
doIt
commonsHttpSolrServer
testWithXml
newPort
startJetty
getUrl
grab
TestLBHttpSolrServer
aSolr
"TESTING
port."
"solr0"
testTwoServers
requested
solrInstance
FAILURE:
"solr1"
"sup_simple_*"
(FDB)
name=\"response\"
SpinPoint
Electronics
name=\"name\">Maxtor
name=\"cat\"><str>electronics</str><str>hard
"supB1"
SATA-300</str><int
Ultra
name=\"inStock\">false</bool>"
Store</str><str
"</str><str
getAaa
catfield
name=\"price\">19.95</float><str
Store"
"<str>16MB
DiamondMax
"[Mobile
"Mobile
isInStock
name=\"weight\">2.0</float></doc></result>\n"
Mobile
name=\"price\">350.0</float>"
name=\"features\"><str>SATA
"supA3"
"<date
GB
mwyMileage
"supB2"
name=\"manu\">Samsung
technology,
name=\"q\">*:*\n"
"CCTV
P120
name=\"id\">6H500F0</str><bool
drive</str></arr><arr
name=\"features\">"
name=\"supplier_1\">iPod
Dock</str>"
solDocList
"supA_val"
3.0Gb/s,
name=\"supplier_2\">CCTV
Store]"
hard
SilentSeek
numFound=\"26\"
ccc]"
"sup_simple_supA"
"supA2"
name=\"manu\">Maxtor
name=\"sku\">IW-02</str>"
name=\"popularity\">1</int><float
name=\"name\">Belkin
name=\"supplier_1\">Mobile
Cable</str>"
name=\"price\">92.0</float><str
name=\"id\">SP2514N</str>"
name=\"start\">0</str><str
supplier
name=\"name\">Samsung
cache,
"<response>"
NCQ</str><str>8.5ms
"<lst
adapter,
name=\"manu\">Belkin</str><str
name=\"weight\">4.0</float></doc><doc>"
TestDocumentObjectBinder
Ltd.</str><str
"supB3"
iPod
white</str></arr><str
NotGettableItem
Corp.</str>"
name=\"features\"><str>7200RPM,
name=\"inStock\">false</bool><str
name=\"rows\">4</str></lst></lst><result
name=\"inStock\">true</bool><str
testToAndFromSolrDocument
features
ATA-133</str>"
"[CCTV
Fluid
name=\"cat\"><str>electronics</str>"
name=\"params\"><str
name=\"id\">F8V7067-APL-KIT</str>"
IDE
"<str>car
Cord
"supplier_supB"
name=\"cat\">"
seek</str>"
"<str>NoiseGuard,
name=\"sku\">F8V7067-APL-KIT</str>"
"</response>"
allSuppliers
name=\"timestamp\">2008-04-16T10:35:57.140Z</date><float
"supplier_1"
iPod,
name=\"features\"><str>car
supA
supB
setInStock
motor</str></arr><str
Bearing
setAllSuppliers
Store,
"<doc><arr
getAllSuppliers
"supplier_supA"
assertArrayEquals
name=\"timestamp\">2008-04-16T10:35:57.078Z</date></doc>"
adapter
"supplier_*"
USB
setAaa
start=\"0\"><doc><arr
name=\"QTime\">0</int><lst
Co.
cache</str></arr><str
name=\"sku\">6H500F0</str><date
SP2514N
Power
8MB
"<str>connector</str></arr><arr
singleOut
"<arr
name=\"responseHeader\"><int
name=\"popularity\">6</int><float
name=\"id\">IW-02</str><bool
"supplier_2"
supplier_simple
"sup_simple_supB"
"supA1"
name=\"cat\"><str>electronics</str><str>connector</str></arr><arr
name=\"price\">11.5</float><str
name=\"status\">0</int><int
Store</str>"
"<str>electronics</str><str>hard
Mini
name=\"name\">iPod
name=\"timestamp\">2008-04-16T10:35:57.109Z</date></doc><doc><arr
Dynamic
drive
testDynamicFieldBinding
"supB_val"
"iPod
name=\"version\">2.2</str><str
testSingleVal4Array
name=\"sku\">SP2514N</str><date
JettyWebappTest
"/test"
testJSP
"../../webapp/web"
"schema.jsp"
"analysis.jsp"
"admin/"
"threaddump.jsp"
"../../../example/solr"
LargeVolumeBinaryJettyTest
LargeVolumeEmbeddedTest
LargeVolumeJettyTest
MergeIndexesEmbeddedTest
MultiCoreEmbeddedTest
createServer
MultiCoreExampleJettyTest
SolrExampleEmbeddedTest
"CommonsHttpSolrServer
"http://localhost/?core=xxx"
SolrExampleJettyTest
testBadSetup
SolrExampleStreamingTest
"solr/shared"
TestSolrProperties
"solr-persist.xml"
getSolrXml
"yup
stopfrb
stopfra
"pwd:
stopenb"
stopena
persistedFile
"1*"
inDoc
fldB
updateUnmarshalled
compareDocs
outDoc
TestUpdateRequestCodec
fldA
"Filter1"
"Filter2"
testBuildTokenInfo
"Filter3"
numberOfTokens
expectedTokenCount
buildFakeTokenInfoList
testBuildPhases
expectedClassName
AnlysisResponseBaseTest
"JUMPING1"
expectedToken
assertPhase
DocumentAnalysisResponseTest
nameValue2
nameValue1
idValue
"analysis.fieldname"
"analysis.fieldtype"
"analysis.fieldvalue"
FieldAnalysisResponseTest
"sampleDateFacetResponse.xml"
QueryResponseTest
"terms_s"
TermsResponseTest
testTermsResponse
testSpellCheckResponse
Port:
"/spell"
testSpellCheckResponse_Extended
"samsang"
"111"
TestSpellCheckResponse
"Samsung"
"h:ello!"
"h\\~\\!"
"h\\:ello\\!"
ClientUtilsTest
testEscapeQuery
"nochange"
"h~!"
"with\\
s]"
12.01f
unsupported!"
11.01f
SolrDocumentTest
testAddCollections
testDuplicate
indoc
"[b,
f,
fval1
"asdgsadgas"
fval0
testUnsupportedStuff
fval2
testMapInterface
"length:
World
testOldZeroLength
modifiable
helloWorld
ModifiableSolrParamsTest
helloWorldUniverse
World:
Universe
testAddNormal
compareArrays
"Universe"
testAddPseudoNull
universe
testAddNull
"true-1"
pbool
"false-1"
"dint"
"true-0"
"f.fl.bool"
"false-2"
"true-"
pfloat
"f.fl.float"
"f.bad.int"
testGetParams
"f.bad.nnnn"
"notfloat"
dmap
"true-2"
"asagdsaga"
pmap
"f.bad.float"
10.6f
sx
"false-"
SolrParamTest
"f.fl.int"
"false-0"
getReturnCode
"dstr"
pint
"fakefield"
"10.6"
"f.bad.bool"
"notint"
"notbool"
"f.fl.str"
hjsakg
ContentStreamTest
testURLStream
&jag
ghaskdgasgldj
"aads
ajdsg
contentEquals
sadg
hsakdg
"README"
asl
testFileStream
"http://svn.apache.org/repos/asf/lucene/solr/trunk/"
testStringStream
hjkas
"/long"
"<double
name=\"Boolean\">false</bool>"
"Float"
"<float
name=\"Boolean\">on</bool>"
name=\"Boolean\">true</bool>"
name=\"Long\">200</long>"
DOMUtilTest
name=\"Float\">300</float>"
"/bool"
name=\"Double\">400</double>"
"STRING"
"Long"
"Boolean"
"Double"
name=\"Boolean\">off</bool>"
"/double"
name=\"Integer\">100</int>"
name=\"String\">STRING</str>"
name=\"Boolean\">no</bool>"
"<long
name=\"Boolean\">yes</bool>"
"/float"
testAddToNamedListPrimitiveTypes
"Integer"
"/int"
"String"
"/str"
assertTypeAndValue
cwd
"/conf"
"/conf/data"
testResolve
FileUtilsTest
"conf/data"
"a1a2a3c1"
UnsupportedOperationException"
testTwoIterators
testCallNextTooEarly
"a1a2a3b1b2"
IteratorChain.hastNext()
testCallAddTooLate
testNoIterator
"a1a2a3"
"Calling
testEmptyIteratorsInTheMiddle
asExpected
next()
"Empty
testOneIterator
IteratorChainTest
makeIterator
"key1"
"key2"
NamedListTest
bis
TestFastInputStream
"Helloooooooooooooooooooo"
GZIPOutputStream
testgzip
gzos
gzis
MAX_CODE_POINT
0x8a2db728
testHash
0xfe5b9199
TestHash
0xec321498
hash4
hash3
testEqualsLOOKUP3
tstEquiv
0x3ab04cc3
0xc4c20dd5
0x0e770ef3
0x03c313bb
0xebe874a3
0xcbc4e7c2
0x95965125
0x73845e86
0xF800
hashes
CCE
makeRandom
"keys
"keys"
orderOfMagnitude
rStr
theEnd
"theEnd
rSz
"funk"
rNamedList
rList
"values
"Received
MIN_HIGH_SURROGATE
TestNamedListCodec
testIterable
"ham"
"finally"
"burger"
<chez/>
chez
"Bonnie"
&lt;chez/&gt;
testNoEscape
doSimpleTest
Cl<em>y</em>de"
testAmpAscii
expectedOutput
]]>
Clyde"
Cl&lt;em&gt;y&lt;/em&gt;de"
testAmpDotWithAccents
testAmpAndTagAscii
\u00e9v\u00e9nements
testAmpWithAccents
Clyde."
"Les
testAmpAndTagWithAccents
testGt
Bonnie
TestXMLEscaping
]]&gt;
TestFSDirectoryFactory
openCalled
testAltDirectoryUsed
AlternateDirectoryTest
newReaderCalled
var1
var2
"var1"
"var2"
12"
readerFactory
IndexReaderFactoryTest
"Factory
"termInfoIndexDivisor
"readerFactory
"every
open()
"RAMDirectoryFactory
testOpenSucceedForEmptyDir
"/fake/path"
testOpenReturnsTheSameForSamePath
RAMDirectoryFactoryTest
lead
doen't
"/update/csv/asdgadsgas"
testPathNormalization
testLazyLoading
"/update/csv/"
RequestHandlersTest
instDir
FOR
testAwareCompatibility
ResourceLoaderTest
12.3f
testInstanceDir
vsp
SOLR749Test
"vsp
parserPlugin
"solrconfig-SOLR-749.xml"
testConstruction
"parserPlugin
"core
won't
SolrCoreTest
testRefCount
testRefCountMT
LOOP
/that
"infoRegistry
callees
handler2
invokeAll
xint
EmptyRequestHandler
"Handler
MT
17"
"bean
ClosingRequestHandler
long..."
"loop="
"Refcount
"/this/is
registered!"
handler1
testInfoRegistry
testLoadNewIndexDir
TestArbitraryIndexDir
"index_temp"
newDir
"name2"
"bad_solrconfig.xml"
"unset.sys.property"
TestBadConfig
testNothing
"indexDefaults/luceneAutoCommit"
"scheam.xml"
"propTest[@attr2='default-from-config']"
"empty-file-c2.txt"
expectedFiles
testTermIndexInterval
"indexDefaults/mergePolicy/@class"
refCounted
"propTest"
irf
"empty-file-b2.txt"
"indexDefaults/mergeScheduler/@class"
"indexDefaults/ramBufferSizeMB"
getTextContent
sirf
"empty-file-main-lib.txt"
"empty-file-a1.txt"
testJavaProperty
"propone-${literal}"
testLib
"prefix-proptwo-suffix"
solrReader
unexpectedFiles
"default-from-config"
testAutomaticDeprecationSupport
testTermIndexDivisor
testLucene23Upgrades
"empty-file-b1.txt"
"empty-file-c1.txt"
"propTest/@attr1"
mergeSched
TestConfig
"PROTWORDS.TXT"
"empty-file-a2.txt"
"propTest/@attr2"
"empty-file-d2.txt"
testJmxRegistration:
JMX"
"MBeanServers
testJmxRegistration
oldNumDocs
TestJmxIntegration
getPlatformMBeanServer
server:
"Servers
testJmxUpdate
server"
mbeanInfo
testJmxUpdate:
queryNames
SolrIndexSearcher"
getMBeanCount
MBeans
"Mbeans
MBeanServer"
JMXConnectorFactory
"/solrjmx"
monitoredMap
MBean
getMBeanServerConnection
JMXConnector
"mock2"
"MBean
testPutRemoveClear
MockInfoMBean
"service:jmx:rmi:///jndi/rmi://:"
MBeanServerConnection
TestJmxMonitoredMap
"solrconfig-legacy.xml"
TestLegacyMergeSchedulerPolicyConfig
testLegacy
"solrconfig-propinject-indexdefault.xml"
64.0
32.0
TestPropInject
testMergePolicyDefaults
"solrconfig-propinject.xml"
"testMergePolicyDefaults"
testPropsDefaults
"testPropsDefaults"
testProps
QuerySenderListener"
"testQuerySenderListener"
evt
"Mock
testSearcherEvents
qsl
"solrconfig-querysender.xml"
TestQuerySenderListener
newSearcherListener
testCommitAge
TestSolrDeletionPolicy1
agestr
"solrconfig-delpolicy1.xml"
"[a-zA-Z]"
testKeepOptimizedOnlyCommits
testNumCommitsConfigured
TestSolrDeletionPolicy2
testFakeDeletionPolicyClass
"solrconfig-delpolicy2.xml"
clearProperty
Req
solrRequestHandler
"solrconfig-xinclude.xml"
TestXIncludeConfig
testXInclude
testXInclude,
dogs</field>"
"cute"
"little"
>12346</field>"
"theTokens
AnalysisRequestHandlerTest
name=\"name\">big
"purr"
"12346"
name=\"name\">cute
purr</field>"
dog</field>"
>12345</field>"
kitten</field>"
"<docs><doc
name=\"text\">the
name=\"text\">cats
Text</field>"
"jumping"
'StopFilter'
Dogs"
"an
Whitetok</field>"
idResult
docsInput
'EnglishPorterFilter'
present"
queryResult
documentResult
"Jumping"
String"
Over
whitetokResult
'text'
Whitetok"
'StandardFilter'
'id'
'WhitespaceTokenizer'
name=\"text\">The
Jumped
'LowerCaseFilter'
valueResult
'StandardTokenizer'
Fox
indexResult
textResult
'whitetok'
Jack"
"Jack"
name=\"whitetok\">The
"Jumped"
Text"
DocumentAnalysisRequestHandlerTest
"Over"
"Jumping
"Expcting
"field_types
FieldAnalysisRequestHandlerTest
'whitetok'"
queryPart
"text,nametext"
"whitetok,keywordtok"
textType
whátëvêr
"field_nameds
"org.apache.solr.analysis.HTMLStripCharFilter"
'keywordtok'"
'nametext'"
whitetok
names"
"<html><body>whátëvêr</body></html>"
"nametext"
nameTextType
keywordtok
whatever
"org.apache.lucene.analysis.MappingCharFilter"
indexPart
"charfilthtmlmap"
'charfilthtmlmap'"
breakdown"
testCharFilterAnalysis
applied"
Jones"
"Eyes
"Harrison
harrison
"//result/doc[1]/int[@name='id'][.='43']"
Wars"
Business"
"Patriot
Hanks"
Mystery
"Tom
subword^0.1"
Lonley
"Cast
cruise"
"Help"
"Top
"//result/doc[2]/int[@name='id'][.='43']"
"//result/doc[2]/int[@name='id'][.='46']"
ford"
"Batman"
MoreLikeThisHandlerTest
Away"
"Philadelphia
"Nicole
"Risky
weights"
Cruise"
"name^5.0
mltreq
Green
Ford"
"//result/doc[1]/int[@name='id'][.='46']"
Story"
"Regarding
"morelike
Thunder"
Mile"
"Magical
"Star
"name,subword,foo_ti"
tom
Gun"
Henry"
Submarine"
"Forest
"Days
Kidman"
"Indiana
"//result/doc[1]/int[@name='id'][.='45']"
Games"
Report"
"morelikethis
"Far
"Sgt.
Wide
Harrison"
Shut"
Tour"
Money"
Gump"
Color
"Yellow
"Minority
count(lst)=0]"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='cat']"
"//lst[@name='corp']"
cot"
"//str[@name='exist'][.='false']"
".0"
"cattails"
testSpellCheck_03_multiWords_correctWords
"cet
"//lst[@name='coat']"
testSpellCheck_05_buildDictionary
catnip"
"//lst[@name='carm']/int[@name='frequency'][.=0]"
"//arr/str[.='cart']"
"109"
corp"
"//lst[@name='cad']/lst[@name='suggestions']/lst[@name='cod']"
"//lst[@name='coat']/lst[@name='suggestions']/lst[@name='cot']"
threshold"
"//lst[@name='carm']/lst[@name='suggestions']/lst[@name='carp']"
testSpellCheck_04_multiWords_incorrectWords
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='cod']/int[@name='frequency'][.>0]"
"sp.dictionary.threshold"
"sp.query.extendedResults"
cattails"
"//arr/str[.='solr']"
"//lst[@name='cad']/lst[@name='suggestions']/lst[@name='cat']"
testSpellCheck_01_correctWords
"//lst[@name='cet']/lst[@name='suggestions']/lst[2]"
valid"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='carp']"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='cant']/int[@name='frequency'][.>0]"
"108"
"//arr/str[.='cot']"
".2"
"//lst[@name='cert']"
"cot"
"//lst[@name='cat']"
"//lst[@name='cet']/lst[@name='suggestions']/lst[1]"
"//lst[@name='cad']/int[@name='frequency'][.=0]"
"cod"
"spell"
cod"
"coat"
"//lst[@name='corp']/lst[@name='suggestions']/lst[2]"
SpellCheckerRequestHandlerTest
cart
corn"
"107"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='cat']/int[@name='frequency'][.>0]"
"//lst[@name='cart']/int[@name='frequency'][.>0]"
check"
"cart"
cert
"//lst[@name='corp']/int[@name='frequency'][.=0]"
buildSpellCheckIndex
"//arr/str[.='cod']"
"//lst[@name='cat']/lst[@name='suggestions']/lst[@name='cat']/int[@name='frequency'][.>0]"
"0.20"
"//arr[@name='suggestions'][.='']"
"//lst[@name='coat']/lst[@name='suggestions'
"106"
carm"
"//arr/str[.='cat']"
"solr/conf/solrconfig-spellchecker.xml"
"catnip"
"corn"
"//lst[@name='corp']/lst[@name='suggestions']/lst[1]"
"//lst[@name='cert']/lst[@name='suggestions']/lst[1]"
"//str[@name='words'][.='cat']"
"cat
"//str[@name='exist'][.='true']"
testSpellCheck_02_incorrectWords
"solr/conf/schema-spellchecker.xml"
"//lst[@name='coat']/int[@name='frequency'][.=0]"
".4"
"//arr/str[.='corn']"
"//str[@name='words'][.='coat']"
car
"//str[@name='words'][.='cart']"
"//str[@name='cmdExecuted'][.='rebuild']"
"//lst[@name='carm']/lst[@name='suggestions']/lst[@name='cart']"
"cad
"//lst[@name='cart']"
"//lst[@name='coat']/lst[@name='suggestions']/lst[@name='cart']"
"sp.query.suggestionCount"
Index:"
"//lst[@name='cap']/int[@name='frequency'][.=0]"
"//lst[@name='cert']/int[@name='frequency'][.=0]"
"Confirm
"//int[@name='numDocs'][.=10]"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='carp']/int[@name='frequency'][.>0]"
"//lst[@name='cat']/int[@name='frequency'][.>0]"
"//lst[@name='coat']/lst[@name='suggestions']/lst[@name='cat']"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='cart']/int[@name='frequency'][.>0]"
"sp.query.accuracy"
"//str[@name='words'][.='corn']"
"cant"
"//lst[@name='cet']/int[@name='frequency'][.=0]"
"//str[@name='words'][.='cod']"
"//lst[@name='cert']/lst[@name='suggestions']/lst[2]"
"//lst[@name='carm']"
carp"
"//lst[@name='coat']/lst[@name='suggestions']/lst[@name='corn']"
"//arr/str[.='carp']"
cart"
"//lst[@name='cad']"
"//lst[@name='cet']"
cant"
"104"
"carp"
"//lst[@name='cap']/lst[@name='suggestions']/lst[@name='cot']/int[@name='frequency'][.>0]"
"//lst[@name='cap']"
"//result/doc[3]/int[@name='id'][.='12']"
val_s
"title:test;
"val_s
StandardRequestHandlerTest
[desc]"
[asc]"
"//result/doc[3]/int[@name='id'][.='10']"
"//result/doc[2]/int[@name='id'][.='11']"
"//result/doc[1]/int[@name='id'][.='12']"
"100|^quoted^\n"
"104|a\\\\b\n"
":EMPTY"
TestCSVLoader
"f.str_s.split"
"f.str_s.encapsulator"
testCSV
"//arr[@name='str_s']/str[3][.='c']"
"id:id"
"//str[@name='str_s'][.='a\\\\b']"
"f.str_s.map"
"//str[@name='my_s'][.='EMPTY']"
"100,\"quoted\"\n"
string']"
"102,\"a,,b\"\n"
def_charset
testCommitFalse
"//str[@name='str_s'][.='quoted
"f.str_s.keepEmpty"
\\']"
"quoted:QUOTED"
string\"']"
"//*[@numFound='8']"
"id:104"
testCSVLoad
"solr_tmp.csv"
"id,str_s\n100,\"quoted\"\n101,\n102,\"\"\n103,"
"//arr[@name='str_s']/str[2][.='b']"
\\\\
"101|a;'b';c\n"
\\,
"id,my_s"
"f.str_s.separator"
\\\n"
"id,"
"100,\"quoted
"//str[@name='my_s'][.='quoted']"
"id:102"
"f.my_s.map"
\"\"
"//arr[@name='str_s']/str[2][.='EMPTY']"
"101,\"a,b,c\"\n"
"102,end
"101,unquoted
"//str[@name='str_s'][.='']"
\"
"id:103"
"quoted:"
"//str[@name='str_s'][.='\"quoted
\\\"
"103|\n"
"//str[@name='str_s'][.='QUOTED']"
"count(//str[@name='str_s'])=0"
"id:100"
string\"\n"
string\n"
"103,\n"
"//str[@name='str_s'][.='quoted']"
"id,str_s\n"
"//str[@name='str_s'][.='end
"//str[@name='str_s'][.='unquoted
"102|a;;b\n"
"//arr[@name='str_s']/str[1][.='a']"
"//str[@name='str_s'][.='EMPTY']"
"id|str_s\n"
testCommitTrue
testIndexAndConfigAliasReplication
"/solr/replication?command=fetchindex&masterUrl="
masterClient
CheckStatus
"schema-replication2.xml"
masterQueryRsp
testStopPoll
"id:555"
masterPort
"solrconfig-master2.xml"
testBackup
slaveJetty
detected:"
BackupThread
TestReplicationHandler
name=\"status\">success</str>"
"Backup
"snapshot"
"/solr/replication?command="
testReplicateAfterStartup
slaveQueryResult
masterQueryResult
"newname"
2001"
slaveQueryRsp
"TEST_PORT"
"/solr/replication?command=enableReplication"
slaveClient
masterJetty
2000"
"/solr/replication"
backupThread
SLAVE_CONFIG
"/solr/replication?command=disableReplication"
"solrconfig-master1.xml"
testIndexAndConfigReplication
"newname
2001
checkStatus
"solrconfig-slave.xml"
testReplicateAfterWrite2Slave
testSnapPullWithMasterUrl
"solrconfig-master.xml"
"/solr/replication?command=disablepoll"
waitCnt
5.5f
name=\"name\">kitten</field>"
boost=\"2.2\">12345</field>"
boost=\"4\">bbb</field>"
boost=\"5\">bbb</field>"
name=\"ab\">a&amp;b</field>"
name=\"cat\"
boost=\"3\">aaa</field>"
bbb]"
boost=\"5.5\">"
XmlUpdateRequestHandlerTest
"solr_sd"
"solr_td"
"solr_pl"
"solr_pf"
"solr_tt"
"solr_sl"
"solr_d"
"SOLR1000"
"solr_pi"
"Apache
"solr_dt"
"solr_sf"
"solr_tf"
"solr_l"
MAX_VALID
"solr_t"
"solr_b"
"solr_sI"
LukeRequestHandlerTest
"solr_s"
"solr_ti"
"solr_tl"
"solr_sS"
"solr_i"
"2000-01-01T01:01:01Z"
doTestHistogramPowerOfTwoBucket
"solr_si"
assertHistoBucket
"solr_pdt"
"solr_tdt"
"solr_f"
"histobucket:
"solr_pd"
SystemInfoHandlerTest
testMagickGetter
"toyata"
"id,lowerfilt"
"suzuki"
DistributedSpellCheckComponentTest
"sonata"
"mclaren"
"toyota"
"ferrari"
"jaguar"
"chevrolet"
"b_t"
"ant
shark
"terms.lower"
spider
snail"
zebra"
shark"
"terms.sort"
"snake
spider"
snail
"terms.prefix"
"terms.limit"
"terms.fl"
"sn"
seal"
slug
"zebra"
"terms.upper"
DistributedTermsComponentTest
slug"
"//result/doc[4]/str[@name='id'][.='x']"
"//result/doc[1]/str[@name='id'][.='a']"
"str_s
"//result/doc[3]/str[@name='id'][.='z']"
"zzzz"
"//result/doc[1]/str[@name='id'][.='c']"
"//result/doc[1]/str[@name='id'][.='x']"
"solrconfig-elevate.xml"
"title:ipod"
"elevate.xml"
"YYYY"
"//result/doc[6]/str[@name='id'][.='c']"
"//result/doc[3]/str[@name='id'][.='x']"
<doc
?>"
text=\""
testfile
"OUT:"
"//result/doc[2]/str[@name='id'][.='y']"
"//result/doc[5]/str[@name='id'][.='b']"
QEC
"elevate"
"//result/doc[3]/str[@name='id'][.='c']"
"<query
handles
"XXXX"
"data-elevation.xml"
"//result/doc[2]/str[@name='id'][.='b']"
"//result/doc[2]/str[@name='id'][.='x']"
testEmptyQuery
"//result/doc[3]/str[@name='id'][.='b']"
writeFile
"XXXX
testElevationReloading
"</elevate>"
"//result/doc[4]/str[@name='id'][.='c']"
"xxxx"
QueryElevationComponentTest
"//result/doc[4]/str[@name='id'][.='a']"
"<elevate>"
YYYY"
id=\""
"/elevate"
"xxxxyyyy"
names0
testInitalization
names1
comps
SearchHandlerTest
lowerfilt:brown"
testReloadOnStart
"//arr[@name='suggestion'][.='lucenejava']"
"lowerfilt:lucenejavt"
testInit
"./spellchecker"
"ttle"
"spellcheck.reload"
theSuggestion
"speller
"documemtsss
cmdExec
"spellCheck
spellings
blue
"spellcheckerIndexDir
testCorrectSpelling
altSC
"lowerfilt:lazy"
"lucenejava"
"11231"
testRelativeIndexDirLocation
"alternate"
analyzers"
testExtendedResultsCount
"command
"//*/bool[@name='correctlySpelled'][.='false']"
"spellchecker3"
"spellcheck.build"
"lakkle"
"lowerfilt:lazy
lowerfilt:brown^4"
testCollate
"//*/lst[@name='suggestions']"
"spellcheck.dictionary"
"//arr[@name='suggestion'][.='title']"
"spellchecker2"
SpellCheckComponentTest
speller
spellCheck
"NullPointerException
testRebuildOnCommit
broens"
"spellchecker1"
lowerfilt:broen^4"
signalled
"//*/bool[@name='correctlySpelled'][.='true']"
"//lst[@name='true']/double[@name='mean'][.='15.0']"
"//lst[@name='false']/double[@name='min'][.='30.0']"
"//lst[@name='false']/double[@name='sum'][.='-61.0']"
"//double[@name='mean'][.='-23.333333333333332']"
"//lst[@name='true']/double[@name='min'][.='-100.0']"
"//long[@name='count'][.='8']"
"//lst[@name='true']/double[@name='sum'][.='70.0']"
active_s=true"
"stats_l"
"-10"
"//lst[@name='false']/double[@name='min'][.='40.0']"
"//lst[@name='false']/long[@name='count'][.='2']"
"//lst[@name='true']/double[@name='stddev'][.='128.16005617976296']"
"//lst[@name='false']/double[@name='mean'][.='-15.25']"
"//double[@name='min'][.='-40.0']"
"stats_tls"
"stats_tds"
"//lst[@name='false']/double[@name='sumOfSquares'][.='2601.0']"
"//long[@name='count'][.='3']"
"//double[@name='mean'][.='1.125']"
"//double[@name='sumOfSquares'][.='2100.0']"
"//lst[@name='true']/double[@name='sum'][.='30.0']"
"//lst[@name='false']/long[@name='missing'][.='0']"
active_s=false"
"//lst[@name='true']/double[@name='sumOfSquares'][.='500.0']"
"//lst[@name='false']/double[@name='sumOfSquares'][.='1600.0']"
"//double[@name='stddev'][.='87.08852228787508']"
"stats_ii"
"-20"
"//lst[@name='false']/double[@name='mean'][.='40.0']"
"stats.facet"
"stats_tf"
"//double[@name='min'][.='-100.0']"
"//double[@name='sum'][.='9.0']"
"//lst[@name='false']/long[@name='missing'][.='1']"
"stats_ti"
"//double[@name='stddev'][.='12.909944487358056']"
"//lst[@name='false']/double[@name='sum'][.='40.0']"
"//lst[@name='true']/double[@name='min'][.='10.0']"
values"
"//double[@name='max'][.='200.0']"
"//lst[@name='false']/double[@name='mean'][.='35.0']"
"//lst[@name='true']/long[@name='count'][.='4']"
"//lst[@name='false']/double[@name='sumOfSquares'][.='2500.0']"
"//lst[@name='false']/double[@name='stddev'][.='7.0710678118654755']"
doTestFacetStatisticsResult
"//lst[@name='false']/double[@name='min'][.='-40.0']"
"//lst[@name='false']/long[@name='count'][.='4']"
doTestFacetStatisticsMissingResult
"stats_is"
"//lst[@name='true']/double[@name='sumOfSquares'][.='50500.0']"
"-30"
"//lst[@name='true']/double[@name='max'][.='20.0']"
"//lst[@name='true']/double[@name='max'][.='200.0']"
doTestFieldStatisticsResult
"//lst[@name='true']/double[@name='mean'][.='17.5']"
"30"
"//lst[@name='true']/long[@name='count'][.='2']"
"stats_td"
"//lst[@name='false']/double[@name='max'][.='40.0']"
StatsComponentTest
"//double[@name='sum'][.='-100.0']"
"stats_tl"
"//double[@name='sumOfSquares'][.='53101.0']"
"stats_f"
"//long[@name='missing'][.='0']"
"//long[@name='missing'][.='1']"
"stats_i"
"//double[@name='max'][.='-10.0']"
"-40"
"//lst[@name='true']/long[@name='missing'][.='0']"
"//long[@name='count'][.='4']"
doTestMVFieldStatisticsResult
"stats_tis"
"//double[@name='stddev'][.='15.275252316519467']"
"//double[@name='sum'][.='-70.0']"
"//lst[@name='false']/double[@name='stddev'][.='23.59908190304586']"
"//lst[@name='false']/double[@name='stddev'][.='0.0']"
"//lst[@name='false']/double[@name='max'][.='10.0']"
"//lst[@name='false']/long[@name='count'][.='1']"
testStats
"//double[@name='mean'][.='-25.0']"
doTestFieldStatisticsMissingResult
"//lst[@name='false']/double[@name='sum'][.='70.0']"
"stats_d"
"stats_tfs"
"active_s"
"//lst[@name='true']/double[@name='stddev'][.='7.0710678118654755']"
"//double[@name='sumOfSquares'][.='3000.0']"
'spider',
"canon_eq"
"abb
testNoField
'snake'"
'spider'"
testRegexpWithFlags
testLowerExclusive
'snake',
"dotall"
"baa
"standardfilt"
"comments"
"Item
"termsComp"
testRegexp
"shark"
"abbb"
testSortIndex
testPastUpper
testUnlimitedRows
TermsComponentTest
"zzz_i"
"unix_lines"
testRegexpFlagParsing
"tmp
'3'"
testSortCount
"B.*"
"tc
'shark'
"spider"
"ccccc"
"baa"
testEmptyLower
testMinMaxFreq
"terms.raw"
testMultipleFields
testPrefix
'shark'"
'1'"
"bbbb"
'2'"
"abb"
"unicode_case"
"anoth"
Params:
"pos
TermVectorComponentTest
"offtv
testNoFields
"Shard:
"rb.outgoing
testOptions
"id:0"
"offsets
"localhost:3"
"localhost:2"
testDistributed
tvComp
"positions
offtv
"tvComponent"
"termVectors
"titl
"tvrh"
"tfIdf
"uniqueKeyFieldName
"localhost:0"
titl
"localhost:1"
"tvComp
"thing1"
"hl.useFastVectorHighlighter"
solrFlbEmpty
solrFbColored
<b>vector</b>
solrFbNull
"scoreOrder"
"tv_text:vector"
test']"
solrFlbNull
solrFbSO
"//lst[@name='1']/arr[@name='tv_text']/str[.='
FastVectorHighlighterTest
solrFlbSimple
solrFbEmpty
"highlighter"
"//lst[@name='highlighting']/str[@name='dummy']"
"solrconfig-highlight.xml"
HighlighterConfigTest
testTermVecMultiValuedHighlight
Miscellaneous
<em>long</em>
org.apache.solr.DisMaxRequestHandlerTest
"//arr/str[.='
"//arr/str[.='!
[mkdir]
testVariableFragsize
"Alternate
"//arr/str[.='.
\"foo_s\""
testCustomSimpleFormatterHighlight
"//lst[@name='1']/arr[@name='t_text']/str[.='a
"//arr[@name='tv_mv_text']/str[.='
text']"
<em>text</em>']"
"hl.alternateField"
t_text2"
"//lst[@name='1']/arr[@name='t_text']/str[.='"
org.apache.solr.analysis.TestRemoveDuplicatesTokenFilter
fragments."
testRegexFragmenter
Errors:
"tv_text:long"
Created
all--we
"//lst[@name='2']/arr[@name='t_text2']/str[.='more
3.519
Failures:
<B>long</B>
\"weight\""
tv_text"
"</I>"
0.797
"t_text:text
org.apache.solr.analysis.TestBufferedTokenStream
testTermOffsetsTokenStream
"hl.maxAlternateFieldLength"
"t_text2"
"//lst[@name='1']/arr[@name='t_text']/str"
"tv_mv_text:long"
text.
fmt2
examples?"
"//arr/str[.='?
"//lst[@name='1']/arr[@name='tv_mv_text']/str[.='a
unknowns
"f.t_text.hl.simple.post"
example
"l*g"
sec"
RequireFieldMatch
"hl.usePhraseHighlighter"
"lon*"
testDefaultFieldNonPrefixWildcardHighlight
"hl.requireFieldMatch"
"foo_*"
"t_text1:random
testTermVecHighlight
\"title\""
Another
testLongFragment
"f.t_text.hl.simple.pre"
"//lst[@name='1'][not(*)]"
org.apache.solr.util.TestOpenBitSet
"//lst[@name='1']/arr[@name='tv_text']/str"
count(*)=0]"
"//lst[@name='highlighting']/lst[@name='2']"
2,
"//lst[@name='1']/arr[count(str)=1]"
"not(//lst[@name='highlighting'])"
"f.t_text."
"Let
night's
0.054
"t*"
"hl.fragmenter"
Contiguous"
shortText
refers\""
substituted"
org.apache.solr.analysis.TestWordDelimiterFilter
"hl.simple.post"
"//lst[@name='1']/arr[@name='t_text']/str[.='
<I>long</I>
days
<em>text</em>
tests']"
1.021
"//arr/str[.='This
"regex
queryword"
"t_text1
line-break!
Kennedy']"
line-break']"
0.788
"Best
"hl.simple.pre"
disjoint
"tv_mv_text"
"t_text:\"text
refers"
testTermVecMultiValuedHighlight2
sentence.
7,
explode..."
org.apache.solr.HighlighterTest
testDefaultFieldPrefixWildcardHighlight
org.apache.solr.util.SolrPluginUtilsTest
highlightFieldNames
run:
"slashes/other
sentence']"
org.apache.solr.analysis.TestSynonymFilter
"//lst[@name='1']/arr[@name='tv_text']/str[.='a
slashes/other
"//lst[@name='2']/arr[@name='t_text1']/str[.='<em>random</em>
Lucene-794"
t_text2:words"
count(*)=1]"
wonder
"hl.highlightMultiTerm"
<em>example</em>
testDisMaxHighlight
org.apache.solr.search.TestDocSet
"//arr[@name='tv_text']/str[.='
5.36
"//lst[@name='highlighting']/lst[@name='1'
elapsed:
"t_text
,^/\\n\"']{20,200}"
testPhraseHighlighter
6,
"//arr/str[.='/other
testAlternateSummary
sentences']"
characters\nand
"tv_text:dir"
us']"
"Phrase
4.979
lengthly
<em>reference</em>
\"sentence\"
<em>random</em>
testMultiValueBestFragmentHighlight
Default"
fmt1
"//lst[@name='1']/arr[@name='t_text']/str[.='This
punctuation
has']"
"i
"t_text:example"
"hl.regex.pattern"
"t_text:long"
oldHighlight2
oldHighlight3
oldHighlight1
testGetHighlightFields
org.apache.solr.SampleTest
testMultiValueAnalysisHighlight
places.
"//lst[@name='1']/arr[@name='t_text2']/str[.='more
queryword
"special
night']"
fact,
testHighlightDisabled
fare
"hl.regex.slop"
1.714
sentences?
5,
night
"foo_t"
8.268
"tv_text:keyword"
<em>words</em>
"unknowns
RequireFieldMatch"
happens
"foo_sI"
0.533
field']"
Let
"//lst[@name='1']/arr[@name='t_text1']/str[.='<em>random</em>
<em>refers</em>
0.081
<em>bar</em>\']"
"junit:
"<I>"
newHighlight1
<em>foo</em>
testDefaultFieldHighlight
<em>examples</em>?']"
fragments.
[junit]
which']"
org.apache.solr.ConvertedLegacyTest
1.56
LONG_TEXT
"//lst[@name='1']/arr[@name='t_text']/str[.='this
"//lst[@name='1']/arr[@name='textgap']/str"
concatenated
testFieldMatch
"hl.fragsize"
un-optimized
isis
us
"//lst[@name='highlighting']/lst[@name='1']/arr[@name='t_text']/str[.='a
org.apache.solr.OutputWriterTest
fragmenter"
,\"']{20,200}"
testMergeContiguous
"t_text:disjoint"
case."
hasnt
"//lst[@name='highlighting']/lst[@name='1']/arr[@name='t_text']/str[.='hi']"
"//lst[@name='1']/arr[@name='textgap']/str[.=\'second
Running
all']"
testMaxChars
/home/klaas/worio/backend/trunk/build-src/solr-nightly/build/test-results
case.']"
\"text\""
testTwoFieldHighlight
"textgap"
"hl.maxAnalyzedChars"
fragments.']"
org.apache.solr.BasicFunctionalityTest
testPHPS
testNaNInf
"{\"nl\":[[\"data1\",\"hello\"],[null,42]]}"
JSONWriterTest
"data3"
testJSON
"nl"
"{'data1':float('NaN'),'data2':-float('Inf'),'data3':float('Inf')}"
"{'data1'=>(0.0/0.0),'data2'=>-(1.0/0.0),'data3'=>(1.0/0.0)}"
"a:3:{s:5:\"data1\";s:5:\"hello\";s:5:\"data2\";i:42;s:5:\"data3\";b:1;}"
"data1"
"data2"
"/int[4][@name='A'][.='1']"
(false)"
"/int[6][@name='D'][.='1']"
"/int[5][@name='E'][.='3']"
"/int[7][@name='F'][.='1']"
solrconfig
facet.sort=true"
"/int[4][@name='D'][.='1']"
"/int[5][@name='B'][.='1']"
"/int[7][@name='G'][.='5']"
"solrconfig-facet-sort.xml"
SimpleFacetsLegacySortTest
facet.sort=false"
"//lst[@name='facet_queries']/int[@name='{!ex=1}trait_s:Obnoxious'][.='2']"
limited
"/int[4][@name='B'][.='1']"
"facet.date.other"
"/int[3][@name='A'][.='1']"
"/int[@name='1976-07-22T00:00:00Z'][.='0']"
correctness)"
"/int[@name='1976-07-01T00:00:00Z'][.='0'
"/int[@name='1976-07-11T00:00:00Z'][.='0']"
"*[count(//lst[@name='facet_fields']/lst/int)=3]"
unsorted"
"/int[@name='E'][.='3']"
mincount=2"
"CC"
"/int[@name='between'][.='6']"
"//lst[@name='facet_queries']/int[@name='id:[42
"//lst[@name='facet_queries']/int[@name='id:[43
limit)"
47]'][.='3']"
"*[count(//lst[@name='trait_s']/int)=4]"
facet.prefix
"{!ex=1}trait_s:Obnoxious"
"//lst[@name='facet_fields']/lst[@name='trait_s']"
"/int[@name='1976-07-02T00:00:00Z'][.='0'
"/int[@name='1976-07-06T00:00:00Z'][.='0'
"/int[@name='1976-07-03T00:00:00Z'][.='2'
"/int[@name='1976-07-31T00:00:00Z'][.='0']"
"//lst[@name='facet_queries']/int[@name='trait_s:Obnoxious'][.='2']"
"//lst[@name='trait_s']/int[@name='Obnoxious'][.='1']"
"//lst[@name='facet_counts']/lst[@name='facet_queries']"
"+5DAYS"
".facet.mincount"
"/int[2][@name='CC'][.='2']"
facet.mincount=1&facet.missing=true
"/int[@name='1976-07-01T00:00:00Z'][.='5'
"*[count(//lst[@name='facet_fields']/lst/int)=1]"
45]'][.='4']"
"//lst[@name='trait_s']/int[not(@name)][.='1']"
"2007-07-30T07:07:07.070Z"
"1976-07-30T22:22:22.222Z"
"/int[1][@name='AAA'][.='1']"
"/int[@name='A'][.='1']"
key=bar}id:[43
hardend=false"
"{!ex=3,1}trait_s"
"/int)=7]"
"*[count(//lst[@name='trait_s']/int)=2]"
][.='3']"
"/int[@name='1976-07-23T00:00:00Z'][.='0']"
testFacetPrefixMultiValued
"//lst[@name='trait_s']/int[@name='Pig'][.='0']"
"1976-07-12T12:12:25.255Z"
"/int[@name='C'][.='2']"
"/int[3][@name='D'][.='1']"
"*[count(//lst[@name='trait_s']/int)=3]"
"/int[@name='1976-07-16T00:00:00Z'][.='0']"
"/int[@name='1976-07-19T00:00:00Z'][.='0']"
"/int)=6]"
testSimpleFacetCounts
"/int[@name='1976-07-29T00:00:00Z'][.='0']"
"1976-07-15T15:15:15.155Z"
"/int[@name='1976-07-24T00:00:00Z'][.='0']"
"/int[@name='1976-07-11T00:00:00Z'][.='4'
SimpleFacetsTest
offset"
"{!ex=2
start,
"f.trait_s.facet.missing"
"1976-07-05T00:00:00.000Z"
"/int[@name='G'][.='5']"
"/int[@name='1976-07-05T00:00:00Z'][.='2'
"/int[3][@name='B'][.='1']"
testFacetPrefixSingleValued
blank
"//lst[@name='facet_counts']/lst[@name='facet_fields']"
(test
"/int[@name='1976-07-04T00:00:00Z'][.='2'
facet.mincount=2&facet.missing=true
"/int[@name='1976-07-18T00:00:00Z'][.='0']"
paging,
paging"
"/int[1][@name='CCC'][.='3']"
][.='1']"
"1976-07-13T00:00:00.000Z"
"/int)=34]"
"/int[@name='1976-07-26T00:00:00Z'][.='0']"
notc
"/int[2][@name='BB'][.='2']"
"/int[1][@name='BB'][.='2']"
"id:[43
"1976-07-21T00:07:67.890Z"
"1976-07-13T12:12:25.255Z"
"/int[@name='between'][.='9']"
testFacetSingleValued
"1976-07-03T17:01:23.456Z"
"/int[2][@name='F'][.='1']"
"/int[@name='D'][.='1']"
fq"
"/int[5][@name='D'][.='1']"
"facet.enum.cache.minDf"
"trait_s:Obnoxious"
"/int[@name='before'
"//lst[@name='trait_s']/int[@name='Pig'][.='1']"
"999"
"/int[@name='1976-07-10T00:00:00Z'][.='0']"
"/int[1][@name='BBB'][.='3']"
"filter
term,
end,
"/int[@name='1976-07-15T00:00:00Z'][.='2'
"BB"
middle,
"/int[@name='1976-07-13T00:00:00Z'][.='1'
"1976-07-01T00:00:00.000Z"
"{!ex=3,4
"/int[@name='between'][.='11']"
doFacetPrefix
"/int[6][@name='G'][.='5']"
"/int[1][@name='E'][.='3']"
"/int[4][@name='E'][.='3']"
"//lst[@name='trait_s']/int[@name='Obnoxious'][.='2']"
"facet.zeros"
"/int[1][@name='B'][.='1']"
"/int[@name='1976-07-28T00:00:00Z'][.='0']"
"1976-07-01T00:00:00.000Z+1MONTH"
][.='2']"
"/int[@name='1976-07-17T00:00:00Z'][.='0']"
"/int[@name='1976-07-27T00:00:00Z'][.='0']"
"/int[@name='1976-07-14T00:00:00Z'][.='0']"
"/int[@name='1976-07-06T00:00:00Z'][.='0']"
"//lst[@name='trait_s']/int[@name='Tool'][.='2']"
"facet.date.hardend"
hardend=true"
"/int[@name='B'][.='1']"
"/int[@name='1976-07-25T00:00:00Z'][.='0']"
"/int)=11]"
"1907-07-12T12:12:25.255Z"
"1907-07-12T13:13:23.235Z"
key=foo}id:[42
testFacetMultiValued
"*[count("
46]"
"/int[@name='1976-07-08T00:00:00Z'][.='0']"
47]'][.='5']"
Start,
"/int[@name='1976-07-07T00:00:00Z'][.='0']"
generous
"/int[3][@name='BBB'][.='3']"
facet"
"trait_s"
":C"
"1976-07-15T00:07:67.890Z"
][.='6']"
"/int[@name='1976-07-30T00:00:00Z'][.='1'
"/int[@name='1976-07-12T00:00:00Z'][.='1'
"{!tag=1,2}id:47"
"/int[@name='F'][.='1']"
facet.zero=false&facet.missing=true
"//lst[@name='trait_s']/int[@name='Chauvinist'][.='1']"
"1976-07-05T22:22:22.222Z"
"//lst[@name='facet_queries']/int[@name='bar'][.='1']"
"*[count(//lst[@name='facet_fields']/lst/int)=0]"
naming"
unsorted
"//lst[@name='facet_queries']/int[@name='foo'][.='4']"
"/int[@name='1976-07-11T00:00:00Z'][.='1'
"/int[2][@name='D'][.='1']"
"/int[@name='after'
multi-select
45]"
"/int[@name='1976-07-09T00:00:00Z'][.='0']"
(fq)"
"/int[@name='1976-07-21T00:00:00Z'][.='1'
"facet.prefix"
"*[count(//lst[@name='facet_fields']/lst/int)=6]"
"*[count(//lst[@name='facet_fields']/lst/int)=2]"
"1976-07-03T11:02:45.678Z"
"/int[5][@name='F'][.='1']"
"java.lang.String"
testUUID
TestBinaryResponseWriter
"uuid"
rnum
doTermEnum
"'][.='1']"
"//lst[@name='many_ws']/int[@name='"
4095
TestFaceting
"*[count(//lst[@name='many_ws']/int)="
453
many_ws
"field_s"
"*[count(//lst[@name='many_ws']/int)=5000]"
testFacets
2010
commitInterval
testRegularBig
"many_ws"
4093
4092
4090
4097
4094
4098
"000"
3050
4999
"trouble"
"dumpty"
decIter
encodeTime
decodeTime
"N/A"
doPerf
"jersey"
"peace"
"humpty"
"stubborn"
decodeRate="
"nation"
"strong"
"eye"
testPerf
TestWriterPerf
"ouch"
"patriotic"
"horses"
"slow"
"speeches"
"f_ss"
"herculese"
"accidents"
encIter
encodeRate="
"f_i"
"infinity"
findErrorWithSubstring
"*_twice"
BadIndexSchemaTest
"bad-schema.xml"
"fAgain"
"ftAgain"
"ERROR:"
testSevereErrorsForDuplicateNames
testGetDestination
"text_en:simple"
NULL"
"//result/doc[1]/arr[@name='highlight']/str[.='this
"schema-copyfield-test.xml"
testGetMaxChars
testCopyFieldFunctionality
testGetSource
"CopyField
functionality"
"highlight:simple"
testCopyFieldSchemaFieldSchemaField
"highlight:functionality"
CopyFieldTest
']"
testCopyFieldSchemaFieldSchemaFieldInt
"text_en"
argument."
"text_en:functionality"
"destination"
CustomSimilarityFactory
"echo"
820454699999l
"1976-03-06T03:06:00.0Z/DAY"
"1995-12-31T23:59:59.999666Z"
"1995-12-31T23:59:59.987Z"
820454399987l
"1970-01-01T00:00:00"
DateFieldTest
assertParseMath
assertToObject
"1970-01-01T00:00:00.37"
"1995-12-31T23:59:59.990Z+5MINUTES"
820454699990l
194918400000l
"1995-12-31T23:59:59.98Z"
"1995-12-31T23:59:59.999Z+5MINUTES"
"1995-12-31T23:59:59.123Z/DAY"
"1976-03-06T03:06:00.000Z/DAY"
"1970-01-01T00:00:00.9"
"1976-03-06T03:06:00.00Z/DAY"
"1995-12-31T23:59:59.99Z+5MINUTES"
testToObject
"1995-12-31T23:59:59.987666Z"
820454399980l
"1995-12-31T00:00:00"
"1995-12-31T23:59:59.123999Z/DAY"
"1995-12-31T23:59:59Z/DAY"
"1976-03-06T03:06:00Z/DAY"
"1995-12-31T23:59:59.999765Z+5MINUTES"
"aaa_dynamic:aaa"
testSimilarityFactory
"dynamic_runtime:aaa"
"runtimefield"
":aaa"
testDynamicCopy
"aaa_i"
IndexSchemaTest
"no_such_field"
echo?"
"dynamic_aaa:aaa"
"aaa_dynamic"
testRuntimeFieldCreation
"dynamic_runtime"
testIsDynamicField
"1995-12-31T23:59:59.9998"
"1995-12-31T23:59:59.9998Z"
"1995-12-31T23:59:59.9990"
"1970-01-01T00:00:00.000"
"1995-12-31T23:59:59.0"
assertItoR
"1995-12-31T23:59:59.90"
"1995-12-31T23:59:59.990"
testIndexedToReadable
"1995-12-31T23:59:59.00"
"1995-12-31T23:59:59.9990Z"
"1970-01-01T00:00:00.900"
"org.apache.solr.schema.LegacyDateField"
"1995-12-31T23:59:59.900"
"1970-01-01T00:00:00.370"
"1995-12-31T23:59:59.000"
passthrough
NotRequiredUniqueKeyTest
testSchemaLoading
"schema-not-required-unique-key.xml"
"homed:[1
dynField
35.0
2000]"
testPointFieldType
testCartesian
"home_tier"
homeFT
boxIds
"//*[@numFound='50']"
"\"//*[@numFound='50']\""
"homed"
30,30000]"
"\"//*[@numFound='2']\""
"home_ns"
"homed:1000,10000"
79.34
"home_tier:"
"test_p"
poly
"{!func}sqedist(home,
"//str[@name='homed'][.='1000,10000']"
home,
PolyFieldTest
testSchemaBasics
dimensions"
cpfb
"//str[@name='home'][.='1,100']"
qry
"35.0,foo"
"\"//*[@numFound='3']\""
"homed:[1,1000
"home:1,100"
testSearching
testSearchDetails
hasValue
"home
"home:[10,10000
2000,35000]"
"Doesn't
testRequiredFieldsConfig
"noname
"subject
default"
"802"
"multiad
"701"
"name:nosubject"
"name:noname"
"baddef
"803"
"602"
id,
"GaRbAgeFiElD"
"603"
"name:baddef"
"703"
testRequiredFieldsSingleAdd
subject:"
subjectDefault
"nosubject
"801"
numDefaultFields
testAddMultipleDocumentsWithErrors
"702"
subject="
"601"
name,
"schema-required-fields.xml"
documents,
"id:530
RequiredFieldsTest
"//result[@numFound=3]"
"name:multiad"
subject"
TestBinaryField
"schema-binaryfield.xml"
"example"
UUIDFieldTest
"d574fb6a-5f79-4974-b01a-fcd598a19ef5"
'NEW'
uuidfield
"STOP"
randSize
DocSetPerf
bds
"intersectSize"
"intersect"
oper
hds
bds1
bsSize
"intersectAndSize"
hds1
idx1
FooQParser
FunctionQuery"
LiteralValueSource"
"_val_:'foo'"
testFunctionQParser
"'foo'"
FunctionQParserTest
bottom"
"weight,
"pow((weight,2)
jw)
testBad
2)
bday"
"pow(weight,2))
desc,bday
"sum(product(float(r_f),sum(float(d_f),float(t_f),const(1.0))),float(a_f))"
bday
"strdist(str(foo_s),literal(junk),
top,bday
"pow(weight,,2)
"sum(product(r_f,sum(d_f,t_f,1)),a_f)
desc,"
"pow(float(weight),const(2.0))"
"weight
QueryParsingTest
\"junk\",
"strdist(foo_s,
"pow()
"pow(weight,
dist=org.apache.lucene.search.spell.JaroWinklerDistance)"
desc,
getDocSlice
doFilterTest
smallSetType
getRandomSet
getRandomDocSet
loadfactor
doTestIteratorEqual
sz2
dummyMultiReader
dummyIndexReader
getIntDocSet
testRandomDocSets
getBitDocSet
smallSetCuttoff
nSets
TestDocSet
getHashDocSet
getRandomSets
doMany
maxSeg
maxSetSize
nReaders
ib
nSeg
doSingle
minSetSize
op"
foo_i
foo_b"
allr
"GB"
oner
(-star)"
TestExtendedDismaxParser
"trait_ss
"50"
foo_dt
"Order
(-id:42)"
"text_sw"
"foo_i:100"
foo_i"
"48"
"trait_ss"
"qf
"text_sw
testFocusQueryParser
"edismax
apple
"49"
foo_d
foo_f
"foo_i:-100"
foo_l
"+Order
"op"
"star
big"
twor
potential"
defaultSearchField"
50]"
gigabyte
maxKey="
nThreads=
hitRatio="
2L
gets="
numGets
TestFastLRUCache
102L
perfTestBoth
101L
cachePerfTest
scNew
"TheValue"
doPerfTest
puts
maxKey
useCache
testOldestItems
1.1
",maxSize="
beforeDelete
sr5
sr3
sr6
sr4
sqr
r5
TestIndexSearcher
baseRefCount
r6
getStringVal
"string1"
afterDelete
"7574"
sval1
"string6"
"//doc[./float[@name='v_f']='1.5'
f=v_t}Hello
"{!prefix
"\"]"
"{!boost
v=\"hel\"}"
"//*[@name='"
rep"
quotes"
f='v_t'
"2002-08-26T01:01:01Z"
b=sum(v_f,1)}id:[5
1234567891234567890L
./float[@name='score']='2.25']"
f="
boost"
literal"
"qqq"
"v_ti"
"sqrt(v_f)^100
"my.v"
"{!q.op=OR
f=v_t}he"
f=v_t}Hello"
"{!field
999.0
"q2"
b=$q2}"
f=v_s}internal\"quote"
f=v_t
"wow
f=v_t}HELLO"
f=v_f}1.5"
operator"
log(sum(v_f,1))^50"
v=hel
"//doc[./float[@name='v_f']='3.14159'
":[\""
v=hel}"
f=v_ti}5"
defType=query
v=\"internal\\\"quote\"}"
"{!v=$q2}"
TestQueryTypes
"{!func}v_f"
"_query_:\"{!query
"v_tdts"
"v_tf"
v=$q1}\""
_val_:\""
testQueryTypes
"v_tls"
f=v_s
{!
v=$qqq}"
"myf"
literal
"{!q.op=AND
sarr
v=$q1
"v_tds"
dismax
f=v_t}hel"
"v_tfs"
"//*[@name='id'][.='999.0']"
v=$my.v}"
"{!lit"
"{!df=v_t}hel*"
qf=v_t}hello"
"'}"
"q1"
dude"
subst
l='"
subst"
"v_td"
f=v_s}{!lit"
"+id:999
./float[@name='score']='4.14159']"
DUDE"
u='"
"v_tl"
"{!lucene}v_t:hel*"
"2000-05-10T01:01:01Z"
v=\"internal\\u0022quote\"}"
"{!foo
v='internal\"quote'}"
v='internal\\\"quote'}"
f=v_t}hello"
"hel"
df=v_t}Hello
"{!raw
whitespace"
func"
"v_tdt"
f=$myf
"internal\"quote"
"{!dismax}hello"
"v_tis"
"-qlkciyopsbgzyvkylsjhchghjrdf"
name:G
filters"
"-name:A"
q"
+id:[*
"-text:stopworda
filter,
-name:W"
neg"
W"
-name:G"
"+id:1"
"-(name:G
"name:G"
"//result[@numFound='7']"
"-name:C"
-id:1"
"//result[@numFound='3']"
"-(name:W
"//result[@numFound='15']"
"-text:stopworda"
-name:W
-name:E
-name:G
TestQueryUtils
"name:W"
-name:E"
stopword"
"compound
"name:E"
id:[*
"//result[@numFound='5']"
nothing,
"//result[@numFound='13']"
name:W)"
"E
"F
"negative
"-name:E"
neg2
"-name:G"
-name:C"
"dude"
"-name:G
"-name:W
"//result[@numFound='16']"
"-name:W"
tq2
"neg
name:C)"
testNegativeQueries
"name:A"
":[*
norm_fields
"{!frange}"
"foo_l"
"2000-01-01T00:00:00.002Z"
"foo_tdt"
sameDocs
"2000-01-01T00:00:00.000Z"
incu=false"
2e-16
u=2}id"
all_fields
"foo_td"
"foo_pl"
"1999-12-31T23:59:59.999Z"
"foo_pd"
"foo_tl"
incl=false"
500000000000000000L
u=102}sum(id,100)"
DocProcessor
incl=false
incl=true
dates
testRandomRangeQueries
"2000-01-01T00:00:00.001Z"
"foo_dt"
"foo_pi"
upperMissing
l=0
TestRangeQuery
lowerMissing
1e-16
0.3333333333333333
"{!frange"
u=2}product(id,2)"
testRangeQueries
"1999-12-31T23:59:59.998Z"
l=100
2000000000
frange_fields
incu=true"
XtestRangePerformance
"foo1_s"
frange
cacheQuery
doSetGen
"foomany_s"
doListGen
"foo4_s"
19999
"foomany_s:["
XtestSetGenerationPerformance
fractionCovered
foo1_s
qf=t10_100_ws
cacheFilt
"foo8_s"
"t10_100_ws"
pf=t10_100_ws
49999
"foo2_s"
XtestFilteringPerformance
t10_100_ws
foo8_s
ps=20}"
"}foomany_s"
foo2_s
foo4_s
foomany_s
TestSearchPerf
parser2
createIndex2
qiter
iterCnt
trackMaxScores
mydocs
maxval
sdocs
MyDoc
commitCountdown
nullRep
sortIdx
smallDoc
scoreInOrder
collectedDocs
emptyDoc
myCollector
trackScores
randSet
commitCount
mydoc
nvlFloatValue
nvlFloatValueArg
NvlValueSourceParser
"nvl"
nvl
"nvlFloatValue"
"//result/doc[4]/int[@name='id'][.='3']"
w_td
asc,
"//result/doc[1]/int[@name='id'][.='4']"
"//result/doc[1]/int[@name='id'][.='2']"
"//result/doc[4]/int[@name='id'][.='4']"
"//result/doc[2]/int[@name='id'][.='3']"
"//result/doc[2]/int[@name='id'][.='4']"
"//result/doc[2]/int[@name='id'][.='1']"
"//result/doc[3]/int[@name='id'][.='2']"
"//result/doc[3]/int[@name='id'][.='3']"
SortByFunctionTest
"//result/doc[4]/int[@name='id'][.='1']"
"sum(z_td,
"//result/doc[2]/int[@name='id'][.='2']"
"sum(x_td,
"//result/doc[3]/int[@name='id'][.='4']"
"f_t:ipod"
y_td)
"deg(.5)"
superman"
"pow(\0,\0)"
"/external_"
"{!func}ms(2009-08-31T12:10:10.125Z,b_tdt)"
"abs(\0)"
54321
ngram,
singleTest
"tanh(.5)"
"batman
"//float[@name='score']='6.0'"
jw)"
"0=1"
nargs
"add(2,3)"
"sqrt(\0)"
"id:120"
"2009-08-31T12:10:10.124Z"
"scale(\0,-10,1000)"
"atan2(.25,.5)"
'foit',
*]'},8)"
"a_tdt"
"asin(.5)"
v=$vv})"
"sqrt(abs(\0))"
"atan(.5)"
"rad(45)"
"sum(query($v1,5),query($v1,7))"
"sqrt(sum(29,\0))"
"10001"
"//float[@name='score']='0.875'"
"div(1,\0)"
"external_foo_extf"
0.9230769f
"//doc[./float[@name='"
"{!func}ms(2009-08-31T12:10:10.124Z,a_tdt)"
"product(\0,1)"
parseableQuery
543210
"//float[@name='score']='0.8833333'"
v=\0})"
"sinh(.5)"
"pow(2,0.5)"
"sum(\0,\0,5)"
"hypot(3,4)"
28.846153f
"{!func}ord(id)"
TestFunctionQuery
testExternalField
testStrDistance
"{!func}rad(x_td)"
"text:batman"
"scale(log(\0),-1000,1000)"
"pow(\0,0.5)"
"{!func}ms(a_tdt,b_tdt)"
"//float[@name='score']='90.0'"
"120"
v='\0:[*
"sum(\0,1)"
./float[@name='score']='"
v=\0},7.1),query({!func
"log(\0)"
"query($vv)"
"sum(query({!func
edit)"
doTestFuncs
'\0'
"{!func}sub(div(sum(0.0,product(1,query($qq))),1),0)"
3125
"ceil(2.3)"
"{!func}deg(y_td)"
"{!func}\0"
"sin(.5)"
2)"
dofunc
"cosh(.5)"
"cos(.5)"
"map(\0,0,0,500)"
"x_s"
v=\0}))"
doTestDegreeRads
"scale(\0,-1,1)"
"batman"
"ln(3)"
answers
"//float[@name='score']='-1.0'"
"//float[@name='score']='0.75'"
"//float[@name='score']>'1.0'"
"2009-08-31T12:10:10.123Z"
1.414213f
"text:superman"
testFunctions
"{!func}strdist(x_s,
"nvl(sum(0,\0),1)"
"{!func}top(ord(id))"
"{!func}query($qq)"
"acos(.5)"
"div(1,1)"
"_val_:\""
"log(100)"
"1.414213"
"54321=543210\n0=-999\n25=250"
"exp(1)"
"nvl(\0,1)"
u=10}query($qq)"
"sqrt(9)"
"foo_extf"
"sum(\0,\0)"
"cbrt(8)"
"tan(.5)"
"{!func}ms(b_tdt,a_tdt)"
"\0"
.01f
"{!func}ms(2009-08-31T12:10:10.125Z/SECOND,2009-08-31T12:10:10.124Z/SECOND)"
"rint(2.3)"
"//float[@name='score']='45.0'"
toDegrees
"sub(\0,1)"
funcTemplate
"//float[@name='score']<'1.0'"
"b_tdt"
"\0:[*
"map(\0,-4,5,500)"
"mul(2,3)"
"query({!func
"pi()"
"query({!lucene
"foil"
"{!func}rord(id)"
"e()"
"-1.414213"
"']='"
"product(\0,-2,-4)"
l=1
"floor(2.3)"
"{!func}top(rord(id))"
"{!func}ms(2009-08-31T12:10:10.125Z,2009-08-31T12:10:10.124Z)"
makeExternalFile
vector(x_td,
"//*[@numFound='7']"
DistanceFunctionTest
80.9289094
7.9
"{!func}dist(1,
"{!func}ghhsin("
"//float[@name='score']='1.0471976'"
"-2.4"
"{!func}hsin(1,
geohash(32,
81.9289094
"{!func}recip(ghhsin("
gh_s,
"point_hash"
-78.0"
"\",)"
79.9289094
point_hash,
2.4
"5.5"
32.7693246
78.9289094
"//result/doc[2]/float[@name='id'][.='7.0']"
"7.9"
z_td,
"2.3"
testHaversine
testVector
x_td,
"//result/doc[1]/float[@name='id'][.='6.0']"
"id,point_hash,score"
w_td,
"5.5,10.9"
"45.0"
"32.6,
"{!func}sqedist(x_td,
"//float[@name='score']='122.309006'"
y_td,
y_td),
"gh_s"
"32.5,
5.5
0)"
-79.0"
"\"),
"1.0,0.0"
-79))"
"xyz:solr"
POST"
OK
Precondition
past"
testCacheVetoException
NotModified
CHARSET
"csv"
CacheHeaderTest
"\"1231323423\",
testCacheVetoHandler
CONTENTS
\"1211211\",
Pragma
"\"xyz1223\""
Cache-Control
checkVetoHeaders
HeadMethod
responseBody
getResponseBodyAsString
"Response
"/select?wt=xml&version=2.2&echoParams=explicit&q=*:*"
"Kittens!!!
nonexistang
"</field></doc></add>"
"/update?"
testInsertThenSelect
\u20AC"
getIt
name=\"echoParams\">explicit</str>"
testSimpleRequest
"/path
DirectSolrConnectionTest
thingy!!"
cmds
"/select?wt=xml&q=id:42"
name=\"subject\">"
We
"solrconfig-nocache.xml"
header.
NoCacheHeaderTest
"this%20is%20simple"
testStandardParseParamsAndFillStreams
"\u0026"
"http://www.apache.org/dist/lucene/solr/"
SolrRequestParserTest
"%C3%BC"
"AMANAPLANPANAMA"
simple"
teststr
"this+is+simple"
"val="
"%E2%82%AC"
testUrlParamParsing
testStreamBody
"text/xxx"
body1
body3
body2
"\u20AC"
"qwertasdfgzxcvb"
"contentType:
"application/x-www-form-urlencoded;"
connection."
testStreamURL
"Application/x-www-form-urlencoded"
testFieldType
FileBasedSpellCheckerTest
"spellings.txt"
"teststop"
"solar"
"Solar"
"fob"
Horse
IndexBasedSpellCheckerTest
"Born
"Caroline"
testSpelling
"Sweet
"jumpin
flash"
"Londons
"bug
altIndexDir
"Sargent
testAlternateDistance
ALT_DOCS
"alternateIdx"
"flesh"
"green
testExtendedResults
Burning"
Run"
bud"
Name"
"red
"flesh
"bug"
DOCS
"sd
Lonely
testAlternateLocation
"Thunder
bun"
"flash"
Caroline"
Road"
testUnicode
"text_field:我购xyz买了道具和服装。"
"field_with_digits123:value_with_digits123"
"text_field:我购买了道具和服装。
testMultipleClauses
isOffsetCorrect
"field_with_underscore:value_with_underscore"
field2:bar"
SpellingQueryConverterTest
"foo:bar^5.0"
converter
"field:foo"
"买text_field:我购买了道具和服装。
"text_购field:我购买了道具和服装。"
"text_field:我购买了道具和服装。"
"field-with-hyphens:value-with-hyphens"
testSpecialChars
"id:500"
waitForCommit
AutoCommitTest
"but
"500"
"id:A1"
"id:A15"
"A15"
"A14"
trigger
testMaxTime
"550"
"id:530"
triggered
"id:A14"
none"
"deleted,
30000
CommitListener
passed"
towait
XXXtestMaxDocs
"solrconfig-duh-optimize.xml"
"subject_"
"id_"
DirectUpdateHandlerOptimizeTest
assertNumSegments
testOptimize
addSimpleDoc
duh2
testDeleteRollback
testRequireUniqueKey
rbkCmd
testDeleteCommit
testAddRollback
testUncommit
ids"
"\"ZZZ\"
"//result/doc[1]/str[@name='id'][.='ZZZ']"
"id:ZZZ"
"//result/doc[2]/str[@name='id'][.='B']"
"id:A
DirectUpdateHandlerTest
\"B\"
deleteSimpleDoc
id:B"
"\"B\"
"ZZZ"
"//result/doc[1]/str[@name='id'][.='A']"
"id:A"
testAddCommit
"\"A\"
testMultiField
DocumentBuilderTest
testNullField
"2.2,3.3"
testBuildDocument
"home_1"
anyway?"
about,
"iter"
includeDoc
indexing?"
"iter="
TestIndexingPerformance
"includeDoc="
that?"
testIndexingPerf
"solrconfig_perf.xml"
"includeDoc"
iterS
"radical!"
white"
"3a"
"dedupe"
"testThread2-"
"random2"
"ali
Dude
"t_field"
galore"
"bishop
"MMMMM"
SignatureUpdateProcessorFactoryTest
3l
babi"
black"
threads2
"testThread-"
girl!"
babi'"
"5a"
"baryy
man!"
"2a"
"random1"
"1a"
testDupeDetection
"Goodbye
"3b"
4l
"{name={n8=88,n9=99}}"
UpdateRequestProcessorFactoryTest
"solrconfig-transformers.xml"
testConfiguration
ArraysUtilsTest
leftChars
rightChars
BitSetPerf
"clone"
"open"
"cardinality"
"union"
oset
"nextSetBit"
"bit"
target1
OpenBitSet"
"iterator"
<iter>
osets
target2
"icount"
randomSets
<numSets>
<bitSetSize>
<impl>"
<numBitsSet>
<testName>
newset
"BitSetTest
+BOB"
WW
"2001-01-01T00:00:00.000"
"2000-07-01T00:00:00.000"
"-1SECOND+1SECOND"
"/MONTH+35DAYS/MONTH"
"2001-08-04T12:08:56.235"
"-1MINUTE+1MINUTE"
"2000-07-04T12:08:57.235"
"2000-07-04T13:00:00.000"
"+1DAY-1DAY"
"+1MONTH"
"-1YEAR+1DAY"
round:"
"+1MONTH-1YEAR"
"-1HOUR+1HOUR"
testParseStatelessness
"-1MINUTE"
"2000-07-04T12:08:56.236"
"+SECOND"
"2000-07-04T12:09:56.235"
"2001-07-01T00:00:00.000"
"-1YEAR+1MONTH"
"2008-02-29T17:09:59.999"
"2001-06-04T12:08:56.235"
"+1MONTH-1MONTH"
"/YEAR"
"+1HOUR-1YEAR"
"-1YEAR+1YEAR"
"/HOUR"
"-1YEAR+1SECOND/DAY"
"-1MONTH"
"2001-07-05T12:08:56.235"
"-2MILLI/"
HH
"-1YEAR"
math:"
"+1MILLISECOND-1YEAR"
"-1DAY+1DAY"
"-1YEAR+1DAY/SECOND"
"2000-08-04T12:08:56.235"
"-1MONTH+1MONTH"
yyyyy
"+1MILLISECOND"
"2001-07-04T00:00:00.000"
"-1YEAR+1HOUR/HOUR"
"-1HOUR"
"2002-07-04T12:08:56.235"
"-1DAY"
ww
"+1MINUTE"
"-1YEAR+1HOUR"
"+2SECONDS
getErrorOffset
DateMathParserTest
assertAdd
"-5DAYS+20MINUTES"
assertRound
SSS
"/SECOND"
"+1SECOND-1YEAR"
"+7YEARS"
"+1MINUTE-1MINUTE"
"-1YEAR+1SECOND"
testAddZero
"+1MILLISECOND-1MILLISECOND"
"2001-07-04T12:08:56.236"
"-1YEAR+1MILLISECOND"
"2001-07-04T12:07:56.235"
"/BOB"
"2001-07-04T12:08:55.235"
"2000-07-04T12:08:56.235"
"2001-07-03T12:08:56.235"
"+1YEAR-1YEAR"
"2001-07-04T11:08:56.235"
"?SECONDS"
"+1HOUR-1HOUR"
"2001-07-04T12:09:56.235"
"2000-07-05T12:08:56.000"
"2001-07-04T12:08:00.000"
"2001-07-04T12:08:57.235"
"2001-07-04T12:08:56.000"
"+3MILLIS/MINUTE"
"/MONTH"
"+1SECOND"
"2000-08-04T12:08:56.000"
"2001-07-04T12:08:56.235"
"-1MILLISECOND"
"2000-07-04T00:00:00.000"
"+25MONTH"
"/4"
"+1YEAR"
DD
"-1MILLISECOND+1MILLISECOND"
"+1HOUR"
"+1SECOND-1SECOND"
testParseMathExceptions
"2000-07-05T12:08:56.235"
"/MINUTE"
"2001-07-04T12:08:56.234"
"-1SECOND"
"-1YEAR+1MINUTE/DAY"
testCalendarUnitsConsistency
trash
"2001-07-04T13:08:56.235"
"2000-07-04T13:08:56.235"
"2006-02-01T00:00:00.000"
"2006-01-31T17:10:00.000"
assertMath
rounding:
badCommands
"+1MINUTE-1YEAR"
"+1DAY-1YEAR"
"-1YEAR+1MONTH/SECOND"
"2001-07-04T12:00:00.000"
"-1YEAR+1MINUTE"
"-1YEAR+1MILLISECOND/MONTH"
"2006-02-28T17:09:59.999"
"2006-01-31T17:09:59.999"
"hoss:XXXXXXXX"
cow?\""
-----------------------------------------------------------------------------------------------------------------------"
go\"
"foo\\?"
testParseFieldBoosts
"spacey
spacey
"3<-25%
\"now!\""
"\"you
testDisjunctionMaxQueryParser
testMinShouldMatchCalculator
-bar"
YYYYY\"
"3235"
"foo+"
hoss:the"
stripOp
"+foo\\:bar"
\"bar\""
--bar
fieldThree^-0.4
testDocListConversion
"3236"
10<-3"
"hoss:XXXXXXXX
"\"how
e1"
"42\""
words)"
sind^1.5"
--bar"
"text^2.0"
"fieldTwo"
title_stemmed
"foo\\!
++"
subject^0.5"
name^1.2
"XXXXXXXX"
"50%"
"-25%"
"+foo:bar"
"foo!
DMQ:"
"foo+
"99"
"0%"
"sind"
"-5"
"subject:\"simple
testPartialEscape
"25%"
"3<0"
-------------------------------------------------------------------------------------------------------------------------bar
"sind:\"simple
(stop
"test:YYYYY"
TermQuery:
"subject^0.5
wasn't
"val_t"
"-100%"
SolrPluginUtilsTest
testStripUnbalancedQuotes
"foo\\:bar"
"fieldThree"
"fieldOne"
calcMSM
"subject:XXXXXXXX"
"title^2.0
"3<25%"
"hoss:\"XXXXXX
fieldTwo
"foo?"
"list
testStripIllegalOperators
countItems
"fieldOne^2.3
fieldThree^-0.4"
test:YYYYY"
boolean:"
fieldOne^2.3
PhraseQuery:
"hoss"
green
e2"
\\!
"3234"
testwordchars
"hash"
"char"
"chars"
"result="
stopwordschars
testwords
SortDouble
Converter
getFloatSpecial
fspecial
rng
lspecial
getLongSpecial
dspecial
getDoubleSpecial
input!=output"
Float2Float
out2
Int2Int
SortFloat
converters
TestNumberUtils
.1
arrstr
"),"
SortInt
conv
65536
SortLong
Base100S
Base10kS
getSpecial
testConverters
"\\r\\n
realMap
foo\\
testNamedLists
"\\r\\n"
\\t\\f\\b"
bar\\
testNumberUtils
":foo:"
TestUtils
1.234
"\t\f\b"
"\\t\\f\\b"
":bar:"
"/h/s,/h/\\,s,"
sortable
"\\:foo\\::\\:bar\\:"
"/h/,s"
unsupported..."
testSplitEscaping
"/h/s"
"\\r\\n:\\t\\f\\b"
"CoreDescriptor
iox
unresponsive"
waitForSolr
Servlet404
isRunning
setDebugEnabled
dispatchFilter
Connectors"
addFilter
"Jetty
3456
NoLog
FilterHolder
conns
getConnectors
addServlet
"NOLOG["
"/*"
"/select?q={!raw+f=junit_test_query}ping"
setInitParameter
"Jetty/Solr
SESSIONS
REQUEST
"DEBUG"
"java.util.logging.config.file"
"/conf/schema.xml"
loggingPath
loggingConfig
checked"
Selector</h1>"
hierarchy
"</th>"
bgcolor='#CCCCFF'>"
fine
yet
roots
border=\"0\"
"</td>\n"
getLoggerNames
logManager
value='cancel'
"</td></tr>\n"
exist.)</td>"
Level
name='"
alt=\"Solr\"></a>"
"</body></html>\n"
"Selection
LogLevelSelection
align=left>Logger/Category
getLogManager
"<a
"intermediate
"</table>\n"
width=\"142\"
align=left>Effective</th>\n"
levels.</p>\n"
"JDK
name<br>"
FINEST
"far
Admin:
FINE
"<table
shown
value='set'
"<title>Solr
"submit"
"class='button'>\n"
WARNING
cancelled"
ROOT
type='submit'
src=\"solr_small.png\"
name='submit'
align=\"right\"
href=\".\"><img
"<link
logger/categories
"</tr>\n"
type=\"text/css\"
align=left>"
align=center>"
type='radio'
Log
bgcolor='#AAAAAA'"
Note
LEVELS
buildWrappers
"<br><br>\n"
"unset"
LogManager
"<input
setting.
"<p>Below
"<html><head>\n"
height=\"78\"
iParams
isLoggable
"<form
"<h1>JDK
Selector</title>\n"
right.
cellpadding='2'>"
rel=\"stylesheet\"
"</tr><tr
href=\"solr-admin.css\"
"</form>\n"
nearest
"<th
method='POST'>\n"
level,
"<td
bgcolor='#AAAAAA'>"
cellspacing='2'
loggerNames
"<tr
iWrappers
"Unset
getLevel
ancestor
OFF
level."
"<br>\n"
getEffectiveLevel
"><td>"
synthesized.
effective
sendRedirect
"</head><body>\n"
LogWrapper
colspan=9>Level</th>"
"<tr"
"(Dark
unset
"org.apache.solr.SolrCore"
solrResp
follow
pathPrefix
"org.apache.solr.CoreContainer"
"/admin"
solr/home
through..."
<abortOnConfigurationError>false</abortOnConfigurationError>\n"
createInitializer
wrong.\n"
handleAdminRequest
"user.dir="
change:
"<root/>"
FilterConfig
retrieved
adminRequestParser
abortErrorMessage
"path-prefix"
getContextPath
"Severe
FilterChain
configuration.\n"
errors,
property"
"SolrDispatchFilter.init()"
setPathPrefix
getPathPrefix
corename
ServletResponse
detailed
Solr.
getServletPath
"-------------------------------------------------------------"
getPathInfo
respWriter
doFilter
alternate
code:
"SolrDispatchFilter.init()
handleSelect
isMultipartContent
uploadLimitKB
RAW
SolrRequestParser
MULTIPART
setSizeMax
enableRemoteStreams
uex
disabled."
globalConfig
Streaming
isFormField
FileItem
content!
HttpRequestContentStream
ServletFileUpload
SimpleRequestParser
setHandleSelect
"requestDispatcher/requestParsers/@multipartUploadLimitInKB"
FileItemContentStream
STANDARD
DiskFileItemFactory
fileupload
"requestDispatcher/requestParsers/@enableRemoteStreaming"
"multipart"
"Remote
"Content-Length"
parseRequest
1048
"requestDispatcher/@handleSelect"
sendErr
"SolrServlet.init()
"SolrServlet.init()"
SolrServlet
solrconfig.xml\n\n"
requestReader
name=\"/update\"
xmlResponseWriter
/update
legacyUpdateHandler
SolrUpdateServlet
<requestHandler
solrj,
class=\"solr.XmlUpdateRequestHandler\"
Add:
"SolrUpdateServlet.init()
@Deprecated
servlet.\n"
rather
setDateHeader
headerEtags
getHeaders
"Sat,
checkETagValidators
headerList
sendPreconditionFailed
calcEtag
sendNotModified
isMatchingEtag
SC_PRECONDITION_FAILED
01
EtagCacheVal
calcLastModified
unmodifiedSince
currentIndexVersion
encodeBase64
ifNoneMatchList
ifMatchList
01:00:00
indexVersionCache
etagCoreCache
modifiedSince
etagCache
getDateHeader
checkLastModValidators
SC_NOT_MODIFIED
setHeader
